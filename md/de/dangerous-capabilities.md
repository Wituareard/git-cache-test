---
title: Regulierung gefährlicher Fähigkeiten in der KI
description: Je leistungsfähiger KI-Systeme in bestimmten Bereichen werden, desto größer werden die Risiken. Wie können wir verhindern, dass diese gefährlichen Fähigkeiten entstehen oder sich verbreiten?
---

In diesem Artikel werden wir diskutieren:

- Welche KI-Fähigkeiten gefährlich sein können
- Wie wir verhindern können, dass diese Fähigkeiten entstehen oder sich verbreiten
- Warum es gefährlich ist, sich auf Bewertungen als politische Maßnahme zu verlassen

Je leistungsfähiger KI-Modelle werden, desto gefährlicher werden sie auch.
An welchem Punkt sollten wir also vorsichtig sein?
Ein bestimmter Schwellenwert, der oft erwähnt wird, ist AGI - oder künstliche allgemeine Intelligenz.
Es gibt viel Diskussion darüber, was AGI genau bedeutet.
Einige sagen, es ist, wenn KI alle kognitiven Aufgaben erledigen kann, die Menschen können.
Einige sagen, GPT-4 sei bereits AGI.
Steve Wozniak definiert AGI als das erste System, das in eine Küche eintreten und einen Kaffee zubereiten kann.

Aus Sicherheitssicht ist die Definition von AGI nicht so wichtig.
Tatsächlich kann sie uns ein falsches Gefühl von Sicherheit geben, weil wir denken könnten, dass wir sicher sind, bis wir AGI erreichen.
Auch wenn eine KI keinen Kaffee zubereiten kann, kann sie immer noch gefährlich sein.
Was zählt, sind die Fähigkeiten, die eine KI hat.

In diesem Artikel werden wir uns mit verschiedenen gefährlichen Fähigkeiten auseinandersetzen und was wir tun können, um zu verhindern, dass sie uns schaden.

## Welche Fähigkeiten können gefährlich sein?

- **Cybersicherheit**. Wenn eine KI in der Lage ist, Sicherheitslücken zu entdecken (insbesondere neue, unbekannte), kann sie (verwendet werden, um) [in Systeme einzudringen](/cybersecurity-risks). Aktuelle [State-of-the-Art](/sota)-KI-Systeme können einige Sicherheitslücken finden, aber noch nicht auf gefährlichem, fortgeschrittenem Niveau. Allerdings steigt mit zunehmenden Cybersicherheitsfähigkeiten auch das potenzielle Schadenspotential, das eine KI-gestützte Cyberwaffe anrichten könnte. Groß angelegte Cyberangriffe könnten unsere Infrastruktur stören, Zahlungen deaktivieren und Chaos verursachen.
- **Biologisch**. Entwurf neuer biologischer Agenzien oder Hilfe bei der Entwicklung einer Pandemie. Eine Gruppe von Studenten konnte einen Chatbot verwenden, um [alle Schritte zu erstellen, die benötigt werden, um eine neue Pandemie zu schaffen](https://arxiv.org/abs/2306.03809). Eine KI, die entwickelt wurde, um sichere Medikamente zu finden, wurde verwendet, um [40.000 neue chemische Waffen in sechs Stunden zu entdecken](https://www.theverge.com/2022/3/17/22983197/ai-new-possible-chemical-weapons-generative-models-vx).
- **Algorithmische Verbesserungen**. Eine KI, die effiziente Algorithmen für ein bestimmtes Problem finden kann, könnte zu einer rekursiven Schleife der Selbstverbesserung führen, die schnell außer Kontrolle gerät. Dies wird als _Intelligenzexplosion_ bezeichnet. Die resultierende KI wäre unglaublich leistungsfähig und könnte alle möglichen anderen gefährlichen Fähigkeiten haben. Glücklicherweise kann noch keine KI sich selbst verbessern. Es gibt jedoch KIs, die neue, sehr effiziente Algorithmen finden können (wie [AlphaDev](https://www.deepmind.com/blog/alphadev-discovers-faster-sorting-algorithms)).
- **Täuschung**. Die Fähigkeit, Menschen zu manipulieren, einschließlich sozialer Ingenieurskunst. Verschiedene Formen der Täuschung sind [bereits in aktuellen KI-Systemen vorhanden](https://twitter.com/DanHendrycks/status/1699437800301752332). Zum Beispiel wurde Meta's CICERO-KI (die darauf trainiert wurde, zu "besserer, natürlicher KI-Mensch-Kooperation" zu führen) zu einem Expertenlügner, der andere Agenten im Spiel täuschte. Eine KI, die Menschen täuschen kann, kann Menschen während des Trainings täuschen. Sie könnte ihre Fähigkeiten oder Absichten verbergen.
- **Selbstreplikation**. Wenn eine KI neue Instanzen auf anderen Maschinen erstellen kann, besteht das Risiko, dass sie sich unkontrolliert verbreitet und zu einem [_KI-Übernahme_](/ai-takeover) führt. Eine hinreichend fähige KI könnte Menschen überlegen sein und zu [menschlichem Aussterben](/xrisk) führen. Beachten Sie, dass dies sogar passieren kann, bevor ein KI-Modell eingesetzt wird.

Diese Liste ist nicht erschöpfend, es gibt also andere gefährliche Fähigkeiten, die eine KI haben könnte.

## Verhinderung der Entstehung gefährlicher Fähigkeiten

Können wir die Entstehung dieser gefährlichen Fähigkeiten verhindern?
Je größer KIs werden und je mehr Daten sie trainieren, erlangen sie neue Fähigkeiten.
Es stellt sich heraus, dass es sehr schwierig ist, vorherzusagen, welche Fähigkeiten entstehen werden und wie gut eine KI performen wird.
Deshalb werden sie oft als _Emergente Fähigkeiten_ bezeichnet.

Unser aktuelles Paradigma von großen Sprachmodellen ist fast inhärent unvorhersehbar.
KI-Modelle werden nicht wie Software geschrieben - sie werden trainiert.
Sie sind Black-Box-Systeme, die aus Milliarden von numerischen Parametern bestehen.
Niemand weiß wirklich, was darin vor sich geht.
Diese Unvorhersehbarkeit macht es schwierig zu sagen, ob ein Trainingslauf zu einer gefährlichen KI führen wird.
Interpretierbarkeitsforschung kann dies in Zukunft ändern, aber momentan können wir nicht wirklich erklären, warum KI tut, was sie tut.

Die Verhinderung der Entstehung gefährlicher Fähigkeiten kann daher praktisch nur auf eine Weise erfolgen:
Baue keine immer leistungsfähigeren KI-Systeme.
Dies wäre der sicherste Weg, aber das ist nicht, was KI-Labors vorschlagen.

## Verhinderung der Verbreitung gefährlicher Fähigkeiten

Derzeit passiert viel im Bereich der KI-Regulierung.
Viele dieser Vorschläge (einschließlich aller, die von KI-Labors stammen) basieren auf Sicherheitsbewertungen (oder _Evals_): Vor-Implementierungstests von KI-Tests von KI-Tests von KI-Tests von KI-Tests von KI-Tests von KI-Tests von KI-Tests von KI-Tests von KI-Tests von KI-Tests von KI-Tests von KI-Tests von KI-Tests von KI-Tests von KI-Tests von