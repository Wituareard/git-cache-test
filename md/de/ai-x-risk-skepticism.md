

---
title: Entkräftung skeptischer Argumente zu existenziellen Risiken durch künstliche Intelligenz
description: Warum existenzielle Risiken durch künstliche Intelligenz real sind und ernsthafte Aufmerksamkeit verdienen
---
_Diese Seite ist eine Zusammenfassung des Artikels [AI Risk Skepticism](https://arxiv.org/ftp/arxiv/papers/2303/2303.03885.pdf) von Ambartsoumean & Yampolskiy._

Für andere häufige Einwände sollten Sie [AISafety.info: Einwände und Antworten](https://aisafety.info/questions/9TDI/Objections-and-responses) und [unsere Einführung in x-Risiken](/xrisk) konsultieren.

## Wir haben noch viel Zeit, uns vorzubereiten {#well-have-a-long-time-to-prepare}

- Skeptiker behaupten, der Fortschritt in der künstlichen Intelligenz verlaufe nicht so schnell wie von einigen vorhergesagt, und die Entwicklung einer allgemeinen künstlichen Intelligenz (AGI) sei noch weit entfernt. Sie verweisen auf vergangene Fehlprognosen und die Grenzen aktueller KI-Systeme.
- Der Fortschritt in der künstlichen Intelligenz war jedoch tatsächlich sehr schnell, mit exponentiell wachsenden Fähigkeiten in vielen Teilgebieten. Auch wenn genaue Vorhersagen schwierig sind, macht der anhaltende Fortschritt leistungsfähige KI-Systeme irgendwann unvermeidlich. Selbst wenn dies in der Ferne liegt, benötigt die KI-Sicherheitsforschung ausreichend Zeit, um sich auf diese Herausforderungen vorzubereiten.

## KI kann keine menschenähnlichen Fähigkeiten haben {#ai-cannot-have-human-like-capabilities}

- Skeptiker argumentieren, KI fehle es an Eigenschaften, die mit menschlicher Intelligenz assoziiert werden, wie Kreativität, allgemeine Argumentationsfähigkeit, Emotionen und Bewusstsein. Sie behaupten, Computer könnten nur eng begrenzte Aufgaben optimieren.
- KI-Systeme zeigen jedoch bereits einige menschenähnliche Fähigkeiten wie Kreativität und allgemeine Spielkompetenz. Es gibt keinen grundlegenden Grund, warum KI nicht weiterhin in allen Dimensionen der Intelligenz fortschreiten könnte. KI benötigt kein Bewusstsein oder Emotionen, um Risiken darzustellen.

## KI kann keine Ziele oder Autonomie haben {#ai-cannot-have-goals-or-autonomy}

- Skeptiker sagen, KI-Systeme würden nur die Ziele optimieren, die wir ihnen geben, und könnten nicht unabhängig handeln oder eigene Ziele haben. Autonomie und unvorhersehbares, selbstgesteuertes Verhalten sei ein Mythos.
- Komplexe KI-Systeme können jedoch potenziell emergente Autonomie und Ziele haben, insbesondere im Hinblick auf Selbstschutz, wie von der Theorie der KI-Antriebe vorhergesagt. Mangelnde Autonomie macht KI nicht sicher, wenn sie von Menschen missbraucht wird.

## KI wird keine unkontrollierte Macht haben {#ai-will-not-have-uncontrolled-power}

- Skeptiker argumentieren, KI-Systeme würden begrenzte Werkzeuge unter menschlicher Kontrolle sein. Sie sehen keinen Weg, wie KI unbegrenzte Intelligenz und Macht erlangen könnte, um die Kontrolle zu übernehmen.
- Es benötigt nur ein unkontrolliertes KI-System, um potenziell Schaden anzurichten. Die Fähigkeiten von KI werden wahrscheinlich die menschliche Kontrolle irgendwann weit überschreiten. Die Unterschätzung der Macht des exponentiellen technologischen Fortschritts ist kurzsichtig.

## KI wird mit menschlichen Werten übereinstimmen {#ai-will-be-aligned-with-human-values}

- Skeptiker erwarten, dass nützliche Werte natürlich entstehen, wenn KI intelligenter wird. Sie vergleichen es mit freundlichen Haustieren und menschlichem moralischem Fortschritt.
- Es gibt jedoch keine Garantie für eine solche Wertübereinstimmung ohne gezielte Anstrengungen. Die Schaffung von KI, die mit komplexen, nuancierten menschlichen Werten übereinstimmt, steht vor erheblichen technischen Herausforderungen, die umfangreiche Forschung erfordern.

## Regulierung wird KI-Risiken verhindern {#regulation-will-prevent-ai-risks}

- Skeptiker sagen, regulatorische Aufsicht und ethische Richtlinien würden schädliche KI-Anwendungen einschränken, so dass wir uns keine Sorgen machen müssten.
- Regulierungspolitik bleibt jedoch oft hinter technologischen Entwicklungen zurück, insbesondere bei exponentiellen Fortschritten. Selbstregulierung in einem wettbewerbsorientierten Umfeld ist ebenfalls unzureichend. Technische KI-Sicherheitsforschung ist nach wie vor von entscheidender Bedeutung.

## Schlussfolgerung {#conclusion}

Die skeptischen Argumente weisen im Allgemeinen fehlerhafte Argumentation auf, unterschätzen das exponentielle Tempo und die Unvorhersehbarkeit des KI-Fortschritts und zeigen mangelndes Verständnis für die Schwierigkeiten der Wertübereinstimmung. Angesichts der beteiligten Interessen macht es Sinn, einen vorsichtigen, proaktiven Ansatz für KI-Sicherheit zu wählen. Obwohl die Zukunftsaussichten unklar bleiben, scheint es unklug, existenzielle Risiken durch KI rundweg abzutun. Eine differenziertere, technische Analyse und Debatte sind erforderlich.