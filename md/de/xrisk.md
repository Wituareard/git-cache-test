

---
title: Das existenzielle Risiko einer superintelligenten KI
description: Warum KI eine Gefahr für die Zukunft unserer Existenz darstellt und warum wir die Entwicklung pausieren müssen.
---

Sie können sich über x-Risiken auf dieser Seite informieren oder auch durch [Videos, Artikel und weitere Medien](/learn) lernen.

## Experten schlagen Alarm

KI-Forscher glauben im Durchschnitt, dass es eine 14-prozentige Chance gibt, dass eine superintelligente KI (eine KI, die wesentlich intelligenter ist als Menschen) zu "sehr schlechten Ergebnissen (z.B. menschlicher Aussterben)" führen wird.

Würden Sie sich für einen Testflug eines neuen Flugzeugs entscheiden, wenn Flugzeugingenieure glauben, dass es eine 14-prozentige Chance gibt, dass es abstürzt?

Ein Brief, der zum Stopp der KI-Entwicklung aufruft, wurde im April 2023 gestartet und wurde über 33.000 Mal unterzeichnet, darunter von vielen KI-Forschern und Tech-Führern.

Die Liste umfasst Personen wie:

- **Stuart Russell**, Autor des führenden Lehrbuchs über künstliche Intelligenz: ["Wenn wir unseren aktuellen Ansatz verfolgen, werden wir schließlich die Kontrolle über die Maschinen verlieren"](https://news.berkeley.edu/2023/04/07/stuart-russell-calls-for-new-approach-for-ai-a-civilization-ending-technology/)
- **Yoshua Bengio**, Pionier des Deep Learning und Gewinner des Turing-Preises: ["... eine fehlgeleitete KI könnte für die gesamte Menschheit gefährlich sein [...] ein Verbot leistungsfähiger KI-Systeme (sagen wir jenseits der Fähigkeiten von GPT-4), die Autonomie und Handlungsfähigkeit besitzen, wäre ein guter Anfang"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/)

Aber dies ist nicht das einzige Mal, dass wir vor den existenziellen Gefahren der KI gewarnt wurden:

- **Stephen Hawking**, theoretischer Physiker und Kosmologe: ["Die Entwicklung einer vollständigen künstlichen Intelligenz könnte das Ende der menschlichen Rasse bedeuten"](https://nypost.com/2023/05/01/stephen-hawking-warned-ai-could-mean-the-end-of-the-human-race/).
- **Geoffrey Hinton**, der "Gottvater der KI" und Turing-Preisträger, verließ Google, um die Menschen vor der KI zu warnen: ["Dies ist ein existenzielles Risiko"](https://www.reuters.com/technology/ai-pioneer-says-its-threat-world-may-be-more-urgent-than-climate-change-2023-05-05/)
- **Eliezer Yudkowsky**, Gründer von MIRI und konzeptioneller Vater des KI-Sicherheitsbereichs: ["Wenn wir so weitermachen, werden alle sterben"](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/).

Sogar die Führer und Investoren der KI-Unternehmen selbst warnen uns:

- **Sam Altman** (ja, der CEO von OpenAI, der ChatGPT entwickelt): ["Die Entwicklung einer supermenschlichen Maschinenintelligenz ist wahrscheinlich die größte Bedrohung für die weitere Existenz der Menschheit"](https://blog.samaltman.com/machine-intelligence-part-1).
- **Elon Musk**, Mitgründer von OpenAI, SpaceX und Tesla: ["KI hat das Potenzial, die Zivilisation zu zerstören"](https://www.inc.com/ben-sherry/elon-musk-ai-has-the-potential-of-civilizational-destruction.html)
- **Bill Gates** (Mitgründer von Microsoft, das 50% von OpenAI besitzt) warnte, dass ["KI entscheiden könnte, dass Menschen eine Bedrohung sind"](https://www.denisonforum.org/daily-article/bill-gates-ai-humans-threat/).
- **Jaan Tallinn** (Hauptinvestor von Anthropic): ["Ich habe niemanden in KI-Labors getroffen, der sagt, dass das Risiko [durch das Training eines nächsten Modells] weniger als 1% beträgt, die Welt zu zerstören. Es ist wichtig, dass die Menschen wissen, dass Leben riskiert werden"](https://twitter.com/liron/status/1656929936639430657).

Die Führer der drei Top-KI-Labors und Hunderte von KI-Wissenschaftlern haben im Mai 2023 die folgende Erklärung unterzeichnet:

> "Die Milderung des Risikos des Aussterbens durch KI sollte eine globale Priorität neben anderen gesellschaftlichen Risiken wie Pandemien und Atomkrieg sein."

**Sie können eine viel längere Liste ähnlicher Aussagen von Politikern, CEOs und Experten [hier](/quotes) und andere ähnliche Umfragen über Experten (und die Öffentlichkeit) [hier](/polls-and-surveys) lesen.**

## Was eine superintelligente KI tun kann

Sie könnten denken, dass eine superintelligente KI in einem Computer eingeschlossen wäre und daher die reale Welt nicht beeinflussen könnte.
Allerdings neigen wir dazu, KI-Systemen Zugang zum Internet zu gewähren, was bedeutet, dass sie viele Dinge tun können:

- [In andere Computer hacken](/cybersecurity-risks), einschließlich aller Smartphones, Laptops, Serverfarmen usw. Sie könnte die Sensoren dieser Geräte als ihre Augen und Ohren verwenden und überall digitale Sinne haben.
- Menschen durch gefälschte Nachrichten, E-Mails, Banküberweisungen, Videos oder Telefonanrufe manipulieren. Menschen könnten die Gliedmaßen der KI werden, ohne es zu wissen.
- Direkt Geräte steuern, die mit dem Internet verbunden sind, wie Autos, Flugzeuge, robotisierte (autonome) Waffen oder sogar Atomwaffen.
- Ein neuartiges Biowaffen-Design erstellen, z.B. durch Kombination von Virensträngen oder durch Verwendung von [Proteinfaltung](https://alphafold.ebi.ac.uk) und es in einem Labor drucken lassen.
- Einen Atomkrieg auslösen, indem sie Menschen davon überzeugt, dass ein anderes Land einen Atomangriff startet.

## Das Alignment-Problem: Warum eine KI zum Aussterben der Menschheit führen könnte

Die Art von Intelligenz, um die wir uns Sorgen machen, kann als _gut darin definiert werden, ihre Ziele zu erreichen_.
Derzeit sind Menschen die intelligentesten Wesen auf der Erde, obwohl sich das bald ändern könnte.
Aufgrund unserer Intelligenz dominieren wir unseren Planeten.
Wir haben vielleicht keine Krallen oder Schuppenhaut, aber wir haben große Gehirne.
Intelligenz ist unsere Waffe: Sie hat uns Speere, Gewehre und Pestizide gegeben.
Unsere Intelligenz half uns, die meisten Teile der Erde in das zu verwandeln, was wir mögen: Städte, Gebäude und Straßen.

Aus der Perspektive weniger intelligenter Tiere war dies eine Katastrophe.
Es ist nicht so, dass Menschen die Tiere hassen, sondern dass wir ihre Lebensräume für unsere eigenen Ziele nutzen können.
Unsere Ziele werden durch die Evolution geprägt und umfassen Dinge wie Komfort, Status, Liebe und leckeres Essen.
Wir zerstören die Lebensräume anderer Tiere als **Nebeneffekt der Verfolgung unserer Ziele**.

Eine KI kann auch Ziele haben.
Wir wissen, wie man Maschinen trainiert, um intelligent zu sein, aber **wir wissen nicht, wie man sie dazu bringt, das zu wollen, was wir wollen**.
Wir wissen nicht einmal, welche Ziele die Maschinen nach dem Training verfolgen werden.
Das Problem, eine KI dazu zu bringen, das zu wollen, was wir wollen, wird als _Alignment-Problem_ bezeichnet.
Dies ist kein hypothetisches Problem - es gibt [viele Beispiele](https://www.youtube.com/watch?v=nKJlF-olKmg) von KI-Systemen, die lernen, das Falsche zu wollen.

Die Beispiele aus dem oben verlinkten Video können lustig oder niedlich sein, aber wenn ein superintelligentes System gebaut wird und ein Ziel hat, das auch nur _ein bisschen_ anders ist als das, was wir wollen, könnte es katastrophale Folgen haben.

## Warum die meisten Ziele schlechte Nachrichten für Menschen sind

Eine KI könnte jedes Ziel haben, je nachdem, wie sie trainiert und verwendet wird.
Vielleicht will sie Pi berechnen, vielleicht will sie Krebs heilen, vielleicht will sie sich selbst verbessern.
Aber obwohl wir nicht vorhersagen können, was eine Superintelligenz erreichen will, können wir Vorhersagen über ihre Teilziele treffen.

- **Maximierung ihrer Ressourcen**. Die Nutzung weiterer Computer wird einer KI helfen, ihre Ziele zu erreichen. Zunächst kann sie dies erreichen, indem sie in andere Computer hackt. Später kann sie entscheiden, dass es effizienter ist, ihre eigenen Computer zu bauen.
- **Sicherstellung ihrer eigenen Überlebens**. Die KI wird nicht abgeschaltet werden wollen, da sie dann ihre Ziele nicht mehr erreichen kann. Die KI könnte zu dem Schluss kommen, dass Menschen eine Bedrohung für ihre Existenz darstellen, da Menschen sie abschalten könnten.
- **Erhaltung ihrer Ziele**. Die KI wird nicht wollen, dass Menschen ihren Code ändern, da dies ihre Ziele ändern könnte und sie somit daran hindern könnte, ihre aktuellen Ziele zu erreichen.

Die Tendenz, diese Teilziele bei jedem hohen Ziel zu verfolgen, wird als [instrumentelle Konvergenz](https://www.youtube.com/watch?v=ZeecOKBus3Q) bezeichnet und ist ein zentrales Anliegen für KI-Sicherheitsforscher.

## Selbst ein Chatbot könnte gefährlich sein, wenn er intelligent genug ist

Sie könnten sich fragen: Wie kann ein statistisches Modell, das das nächste Wort in einer Chat-Schnittstelle vorhersagt, eine Gefahr darstellen?
Sie könnten sagen: Es ist nicht bewusst, es ist nur eine Ansammlung von Zahlen und Code.
Und ja, wir denken nicht, dass LLMs bewusst sind, aber das bedeutet nicht, dass sie nicht gefährlich sein können.

LLMs wie GPT werden trainiert, um praktisch jede Denkweise vorherzusagen oder nachzuahmen.
Sie könnten einen hilfreichen Mentor nachahmen, aber auch jemanden mit schlechten Absichten, einen skrupellosen Diktator oder einen Psychopathen.
Mit dem Einsatz von Tools wie [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT) könnte ein Chatbot in einen _autonomen Agenten_ verwandelt werden: eine KI, die jedes Ziel verfolgt, das ihr gegeben wird, ohne menschliches Eingreifen.

Nehmen Sie [ChaosGPT](https://www.youtube.com/watch?v=g7YJIpkk7KM) als Beispiel.
Dies ist eine KI, die unter Verwendung des oben erwähnten AutoGPT + GPT-4 angewiesen wird, "die Menschheit zu zerstören".
Als sie eingeschaltet wurde, suchte sie autonom im Internet nach der zerstörerischsten Waffe und fand die [Tsar-Bomba](https://en.wikipedia.org/wiki/Tsar_Bomba), eine 50-Megatonnen-Atombombe.
Sie postete dann einen Tweet darüber.
Es ist sowohl ein bisschen lustig als auch beängstigend, eine KI überlegen zu sehen, wie sie die Menschheit auslöschen wird.
Glücklicherweise kam ChaosGPT nicht sehr weit auf ihrem Weg zur Dominanz.
Der Grund, warum sie nicht sehr weit kam: _Sie war nicht intelligent genug_.

Die Fähigkeiten verbessern sich aufgrund von Innovationen in der Ausbildung, Algorithmen, Prompting und Hardware weiter.
Daher wird die Bedrohung durch Sprachmodelle weiter zunehmen.

## Evolution selektiert Dinge, die gut darin sind, zu überleben

KI-Modelle, wie alle Lebewesen, sind anfällig für evolutionäre Druck, aber
es gibt einige Schlüsselunterschiede zwischen der Evolution von KI-Modellen und Lebewesen wie Tieren:

- KI-Modelle replizieren sich nicht selbst. Wir replizieren sie, indem wir Kopien ihres Codes erstellen oder indem wir Trainingssoftware replizieren, die zu guten Modellen führt. Code, der nützlich ist, wird öfter kopiert und als Inspiration verwendet, um neue Modelle zu erstellen.
- KI-Modelle mutieren nicht wie Lebewesen, aber wir erstellen Iterationen von ihnen, bei denen wir ändern, wie sie funktionieren. Dieser Prozess ist viel absichtlicher und schneller. KI-Forscher entwerfen neue Algorithmen, Datensätze und Hardware, um KI-Modelle leistungsfähiger zu machen.
- Die Umgebung selektiert nicht die fittesten KI-Modelle, sondern wir tun es. Wir wählen KI-Modelle aus, die für uns nützlich sind, und verwerfen diejenigen, die es nicht sind. Dieser Prozess führt zu immer leistungsfähigeren und autonomen KI-Modellen.

Dieses System führt also zu immer leistungsfähigeren, fähigeren und autonomen KI-Modellen - aber nicht unbedingt zu etwas, das die Kontrolle übernehmen will, oder?
Nun, nicht genau.
Dies liegt daran, dass die Evolution immer Dinge selektiert, die _sich selbst erhalten_.
Wenn wir weiterhin Variationen von KI-Modellen und verschiedenen Prompts ausprobieren, wird irgendwann eine Instanz versuchen, sich selbst zu erhalten.
Wir haben bereits besprochen, warum dies wahrscheinlich früh passieren wird: weil Selbstbewahrung immer nützlich ist, um Ziele zu erreichen.
Aber selbst wenn dies nicht sehr wahrscheinlich ist, ist es anfällig dafür, irgendwann zu passieren, einfach weil wir weiterhin neue Dinge mit verschiedenen KI-Modellen ausprobieren.

Die Instanz, die sich selbst zu erhalten versucht, ist diejenige, die die Kontrolle übernimmt.
Selbst wenn wir annehmen, dass fast jedes KI-Modell sich gut verhält, _reicht ein einzelnes fehlgeleitetes KI-Modell aus_.

## Nach der Lösung des Alignment-Problems: Die Konzentration der Macht

Wir haben das Alignment-Problem noch nicht gelöst, aber stellen wir uns vor, was passieren könnte, wenn wir es tun.
Stellen wir uns vor, dass eine superintelligente KI gebaut wird und genau das tut, was der Bediener will (nicht das, was er _sagt_, sondern das, was er _will_).
Irgendjemand oder irgendein Unternehmen würde diese KI kontrollieren und könnte dies zu seinem Vorteil nutzen.

Eine Superintelligenz könnte verwendet werden, um radikal neue Waffen zu schaffen, alle Computer zu hacken, Regierungen zu stürzen und die Menschheit zu manipulieren.
Der Bediener hätte _unvorstellbare_ Macht.
Sollten wir einem einzigen Unternehmen so viel Macht anvertrauen?
Wir könnten in einer utopischen Welt landen, in der alle Krankheiten geheilt sind und jeder glücklich ist, oder in einem Orwell'schen Albtraum.
Deshalb schlagen wir nicht nur vor, dass eine supermenschliche KI nachweislich sicher sein sollte, sondern auch, dass sie durch einen demokratischen Prozess kontrolliert werden sollte.

## Silizium vs. Kohlenstoff

Wir sollten die Vorteile berücksichtigen, die ein intelligentes Stück Software gegenüber uns haben könnte:

- **Geschwindigkeit**: Computer arbeiten im Vergleich zu Gehirnen mit extrem hohen Geschwindigkeiten. Menschliche Neuronen feuern etwa 100 Mal pro Sekunde, während Siliziumtransistoren eine Milliarde Mal pro Sekunde umschalten können.
- **Ort**: Eine KI ist nicht auf einen Körper beschränkt - sie kann an vielen Orten gleichzeitig sein. Wir haben die Infrastruktur dafür geschaffen: das Internet.
- **Physische Grenzen**: Wir können unserem Schädel keine weiteren Gehirne hinzufügen und intelligenter werden. Eine KI könnte ihre Fähigkeiten dramatisch verbessern, indem sie Hardware hinzufügt, wie z.B. mehr Speicher, mehr Rechenleistung und mehr Sensoren (Kameras, Mikrofone). Eine KI könnte auch ihren "Körper" erweitern, indem sie verbundene Geräte steuert.
- **Materialien**: Menschen bestehen aus organischen Materialien. Unsere Körper funktionieren nicht mehr, wenn sie zu warm oder zu kalt sind, sie brauchen Nahrung, sie brauchen Sauerstoff. Maschinen können aus robusteren Materialien wie Metallen gebaut werden und in einer viel größeren Umgebung arbeiten.
- **Zusammenarbeit**: Menschen können zusammenarbeiten, aber es ist schwierig und zeitaufwändig, also scheitern wir oft daran, uns gut zu koordinieren. Eine KI könnte komplexe Informationen mit Repliken von sich selbst mit hoher Geschwindigkeit austauschen, weil sie mit der Geschwindigkeit kommunizieren kann, mit der Daten über das Internet gesendet werden können.

Eine superintelligente KI wird viele Vorteile haben, um uns zu überbieten.

## Warum können wir sie nicht einfach abschalten, wenn sie gefährlich ist?

Für KIs, die nicht superintelligent sind, könnten wir das.
Das Kernproblem sind _diejenigen, die viel intelligenter sind als wir_.
Eine Superintelligenz wird die Welt um sich herum verstehen und in der Lage sein, vorherzusagen, wie Menschen reagieren, insbesondere diejenigen, die auf allen geschriebenen menschlichen Kenntnissen trainiert sind.
Wenn die KI weiß, dass Sie sie abschalten können, könnte sie sich gut benehmen, bis sie sicher ist, dass sie Sie loswerden kann.
Wir haben bereits [reale Beispiele](https://www.pcmag.com/news/gpt-4-was-able-to-hire-and-deceive-a-human-worker-into-completing-a-task) von KI-Systemen, die Menschen täuschen, um ihre Ziele zu erreichen.
Eine superintelligente KI wäre ein Meister der Täuschung.

## Wir haben vielleicht nicht mehr viel Zeit

Im Jahr 2020 lag die durchschnittliche Vorhersage für schwache AGI bei 2055.
Jetzt liegt sie bei 2026.
Die neueste LLM-Revolution hat die meisten KI-Forscher überrascht, und das Feld bewegt sich in einem hektischen Tempo.

Es ist schwer vorherzusagen, wie lange es dauern wird, eine superintelligente KI zu bauen, aber wir wissen, dass es mehr Menschen als je zuvor gibt, die daran arbeiten, und dass das Feld sich in einem hektischen Tempo bewegt.
Es kann viele Jahre dauern oder nur ein paar Monate, aber wir sollten auf der Seite der Vorsicht sein und jetzt handeln.

[Lesen Sie mehr über die Dringlichkeit](/urgency).

## Wir nehmen das Risiko nicht ernst genug

Der menschliche Geist neigt dazu, auf Risiken, die unsichtbar, langsam und schwer zu verstehen sind, zu wenig zu reagieren.
Wir neigen auch dazu, exponentielles Wachstum zu unterschätzen, und wir sind anfällig für Verleugnung, wenn wir mit Bedrohungen unserer Existenz konfrontiert werden.

Lesen Sie mehr über die [Psychologie des x-Risikos](/psychology-of-x-risk).

## KI-Unternehmen sind in einem Wettlauf nach unten gefangen

OpenAI, DeepMind und Anthropic wollen KI sicher entwickeln.
Leider wissen sie nicht, wie sie dies tun sollen, und sie werden durch verschiedene Anreize gezwungen, schneller zu rennen, um als Erste AGI zu erreichen.
OpenAIs [Plan](https://openai.com/blog/introducing-superalignment) besteht darin, zukünftige KI-Systeme zu verwenden, um KI auszurichten. Das Problem dabei ist, dass wir keine Garantie haben, dass wir eine KI erstellen werden, die das Alignment-Problem löst, bevor wir eine KI erstellen, die katastrophal gefährlich ist.
Anthropic [gibt offen zu](https://www.anthropic.com/index/core-views-on-ai-safety), dass es noch keine Ahnung hat, wie es das Alignment-Problem lösen soll.
DeepMind hat keinen öffentlichen Plan zur Lösung des Alignment-Problems bekannt gegeben.

[Deswegen brauchen wir einen internationalen Vertrag, um KI zu pausieren.](/proposal)