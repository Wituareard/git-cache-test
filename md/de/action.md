

---
title: Handeln Sie
description: Möglichkeiten, bei der Pause der AGI-Entwicklung zu helfen.
---

Die Gruppe der Menschen, die sich der Risiken von künstlicher Intelligenz bewusst sind, ist noch klein.

Sie gehören jetzt dazu.

**Daher zählen Ihre Aktionen mehr, als Sie denken.**

Hier sind einige Beispiele für das, was Sie tun können.

## Andere informieren

- **Teilen** Sie Informationen über die Risiken von künstlicher Intelligenz in sozialen Medien. Diese Website ist ein guter Ausgangspunkt.  <!-- Sobald wir ein Intro-Video haben, sollte das unsere erste Wahl sein. Wenn wir weit davon entfernt sind, könnten wir hier einige offizielle E-Flyer teilen? https://drive.google.com/drive/u/1/folders/1c6D_i8U95FUpfrl-eR-oRNoHUf3zghOc -->
- **Erstellen** Sie [Artikel](/learn#articles), [Videos](/learn#videos) oder [Memes](https://twitter.com/AISafetyMemes).
- **Sprechen** Sie mit Menschen in Ihrem Leben darüber. Beantworten Sie ihre Fragen und bringen Sie sie dazu, zu handeln. Verwenden Sie unsere [Gegenargumente](/counterarguments), um überzeugender zu sein.
- [**Infostände**](/tabling) und [**Flyerverteilung**](/flyering) sind großartige Möglichkeiten, viele Menschen in kurzer Zeit zu erreichen.

## Politiker informieren

- [**Senden Sie eine E-Mail an einen Politiker**](/email-builder): E-Mails sind sehr effektiv. Versuchen Sie, Ihre Regierung dazu zu bringen, auf eine Pause hinzuarbeiten und sich auf den [Gipfel](/summit) vorzubereiten. Hier sind einige [**Lobby-Tipps**](/lobby-tips).
- **Protestieren**: Nehmen Sie an [einem der Proteste](/protests) teil oder [organisieren Sie selbst einen](/organizing-a-protest).
- **Unterschreiben Sie Petitionen**: [Internationaler AI-Vertrag](https://aitreaty.org), [Verbot von Superintelligenz](https://chng.it/Djjfj2Gmpk), [Forderung nach verantwortungsvoller KI](https://www.change.org/p/artificial-intelligence-time-is-running-out-for-responsible-ai-development-91f0a02c-130a-46e1-9e55-70d6b274f4df) oder eine der **nationalen Petitionen**: [UK](https://petition.parliament.uk/petitions/639956), [AUS](https://www.aph.gov.au/e-petitions/petition/EN5163), [NL](https://aipetitie.nl)

## Der Bewegung beitreten

- Treten Sie einer [lokalen Gemeinschaft](/communities) oder einem [Event](/events) bei.
- Treten Sie dem PauseAI-[Discord](https://discord.gg/2XXWXvErfA) bei, wo die meisten Zusammenarbeitsaktivitäten stattfinden.
- Wenn Sie sich freiwillig engagieren möchten, füllen Sie bitte [dieses Formular](https://airtable.com/embed/appWPTGqZmUcs3NWu/pagoxRuCai4OYJEHt/form) aus.
- [**Spenden**](/donate) Sie an PauseAI oder kaufen Sie Merchandise in unserem [Shop](https://pauseai-shop.fourthwall.com/).
- **Folgen** Sie unseren [sozialen Medien-Kanälen](https://linktr.ee/pauseai) und bleiben Sie auf dem Laufenden.

## Wenn Sie...

### Wenn Sie Politiker oder Regierungsmitarbeiter sind

- **Bereiten Sie sich auf den nächsten [KI-Sicherheitsgipfel](/summit) vor**. Bilden Sie Koalitionen mit anderen Ländern. Arbeiten Sie auf einen globalen Vertrag hin.
- **Laden Sie (oder laden Sie vor) KI-Lab-Leiter** zu parlamentarischen/parlamentarischen Anhörungen ein, um ihre Vorhersagen und Zeitpläne für KI-Katastrophen zu geben.
- **Einrichten Sie einen Ausschuss**, um die [Risiken von KI](/risks) zu untersuchen.
- **Machen Sie KI-Sicherheit zu einer Priorität** in der Plattform Ihrer Partei, der Politik Ihrer Regierung oder stellen Sie sicher, dass es auf der Agenda steht.

### Wenn Sie internationales Recht kennen

- **Helfen Sie bei der Erstellung von Richtlinien**. [Beispiele für Entwürfe](https://www.campaignforaisafety.org/celebrating-the-winners-law-student-moratorium-treaty-competition/). ([Einige](https://futureoflife.org/wp-content/uploads/2023/04/FLI_Policymaking_In_The_Pause.pdf) [Rahmenbedingungen](https://www.openphilanthropy.org/research/12-tentative-ideas-for-us-ai-policy/))
- **Machen Sie Stellungnahmen zu Regierungsanfragen für Kommentare** zu KI-Richtlinien ([Beispiel](https://ntia.gov/issues/artificial-intelligence/request-for-comments)).

### Wenn Sie im Bereich KI arbeiten

- **Arbeiten Sie nicht an Superintelligenz**. Wenn Sie eine coole Idee haben, wie wir KI-Systeme 10x schneller machen können, bitte bauen Sie es nicht / verbreiten Sie es nicht / sprechen Sie nicht darüber. Wir müssen die KI-Entwicklung verlangsamen, nicht beschleunigen.
- **Sprechen Sie mit Ihrem Management und Ihren Kollegen** über die Risiken. Bringen Sie sie dazu, eine institutionelle Position dazu einzunehmen.
- **Halten Sie ein Seminar** über KI-Sicherheit an Ihrem Arbeitsplatz. Überprüfen Sie [diese Vorträge und Videos](https://www.youtube.com/playlist?list=PLI46NoubGtIJa0JVCBR-9CayxCOmU0EJt) für Inspiration.
- **Unterschreiben** Sie die [Erklärung zu KI-Risiken](https://www.safe.ai/statement-on-ai-risk).