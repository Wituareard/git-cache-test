

---
title: Die schwierige Psychologie des existenziellen Risikos
description: Über das Ende der Welt nachzudenken ist schwer.
---

Die meisten Menschen reagieren anfangs auf das Thema des existenziellen Risikos durch künstliche Intelligenz (KI) mit einer Mischung aus Spott, Ablehnung und Unglauben. Die Angst setzt oft erst nach längerer Auseinandersetzung mit dem Thema ein.

Die Psychologie des existenziellen Risikos ist ein Thema, das im Vergleich zu den technischen Aspekten der KI-Sicherheit selten diskutiert wird. Es könnte jedoch ebenso wichtig sein, da wir, wenn wir die Menschen nicht dazu bringen können, das Thema ernst zu nehmen und entsprechend zu handeln, nichts dagegen unternehmen können.

Es ist schwer, das Thema **anzusprechen**, schwer zu **glauben**, schwer zu **verstehen** und schwer, **darauf zu reagieren**. Ein besseres Verständnis dafür, warum diese Dinge so schwierig sind, kann uns helfen, überzeugender, effektiver und empathischer zu sein.

## Schwierig anzusprechen {#difficult-to-bring-up}

Das existenzielle Risiko ist ein schwieriges Thema, insbesondere für Politiker. Die Menschen könnten denken, dass man verrückt ist, und man könnte sich nicht wohl dabei fühlen, über dieses technisch komplexe Thema zu sprechen.

### Angst vor Spott {#fear-of-being-ridiculed}

Die erste Reaktion auf existenzielles Risiko ist oft, es einfach abzutun. Wir haben auch gesehen, dass dies im Weißen Haus passiert ist, als das existenzielle Risiko zum ersten Mal angesprochen wurde. Dies macht es wiederum schwieriger, das Thema erneut anzusprechen, da andere befürchten, verspottet zu werden.

Profis können befürchten, dass ihr Ruf geschädigt wird, wenn sie ihre Bedenken teilen.

> "Es war fast gefährlich, aus beruflicher Sicht zuzugeben, dass man besorgt war", sagte Jeff Clune.

Das Eintreten für vernünftige politische Maßnahmen (wie eine Pause) kann als "extremistisch" oder "alarmistisch" angesehen werden, was die Glaubwürdigkeit oder den Ruf senken kann.

### Angst, als Rassist, Sektenmitglied oder Verschwörungstheoretiker bezeichnet zu werden {#fear-of-being-called-racistcultistconspiracy-theorist}

In den letzten Monaten sind verschiedene Verschwörungstheorien aufgetaucht. Einige Personen haben behauptet, dass alle KI-Sicherheitsforscher Rassisten seien und dass KI-Sicherheit eine Sekte sei. Einige haben behauptet, dass KI-"Doomer" Teil einer Verschwörung von Big-Tech-Unternehmen seien, um KI zu "hype". Diese lächerlichen Anschuldigungen können besorgte Menschen davon abhalten, ihre Bedenken zu teilen.

Bevor man jedoch wütend auf die Menschen wird, die diese Anschuldigungen erheben, sollte man bedenken, dass sie das Ergebnis von Angst und Ablehnung sein können (siehe unten). Die Anerkennung der Gefahren von KI ist beängstigend, und es kann einfacher sein, den Boten abzulehnen, als die Botschaft zu internalisieren.

### Ein komplexes Thema, über das zu diskutieren ist {#a-complex-topic-to-argue-about}

Die Menschen sprechen gerne über Dinge, die sie kennen. Die technische Schwierigkeit der KI-Sicherheit macht es zu einem einschüchternden Thema für die meisten Menschen. Es dauert Zeit und Mühe, die Argumente zu verstehen. Als Politiker möchte man nicht dabei erwischt werden, etwas Falsches zu sagen, also könnte man das Thema ganz vermeiden.

## Schwierig zu glauben {#difficult-to-believe}

Selbst wenn es eine Diskussion über existenzielles Risiko gibt, ist es schwierig, die Menschen davon zu überzeugen, dass es ein reales Problem ist. Es gibt verschiedene Gründe, warum die meisten Menschen die Idee sofort ablehnen.

### Normalitätsbias {#normalcy-bias}

Wir alle kennen die Bilder von Katastrophen in Filmen, oder? Menschen, die schreien und in Panik geraten. Es stellt sich heraus, dass das Gegenteil oft wahr ist: etwa 80% der Menschen zeigen Symptome von Normalitätsbias während Katastrophen: Sie suchen nicht nach Schutz während eines Tornados, ignorieren Regierungswarnungen und schütteln weiterhin Hände in den frühen COVID-Tagen. Der Normalitätsbias beschreibt unsere Tendenz, die Möglichkeit einer Katastrophe zu unterschätzen und zu glauben, dass das Leben weiterhin normal verläuft, selbst angesichts erheblicher Bedrohungen oder Krisen.

> Die Menschen zögern, nach Meinungen zu fragen, weil sie hören wollen, dass alles in Ordnung ist. Sie werden weiter fragen und zögern, bis sie die Antwort erhalten, die sie hören wollen.

Ein weiteres Beispiel dafür ist die Challenger-Raumfähren-Katastrophe im Jahr 1986. Roger Boisjoly war ein Ingenieur, der vorhersagte, dass sie explodieren würde, aber keiner seiner Vorgesetzten wollte glauben, dass es möglich war:

> Wir wussten alle, dass die Dichtungen versagen würden, wenn die Raumfähre explodieren würde. Ich kämpfte wie verrückt, um den Start zu stoppen. Ich bin so zerrissen, dass ich kaum darüber sprechen kann, selbst jetzt.

Eine Erklärung dafür, warum unser Gehirn sich weigert zu glauben, dass Gefahr droht, ist die kognitive Dissonanz.

### Kognitive Dissonanz {#cognitive-dissonance}

Wenn wir mit neuen Informationen konfrontiert werden, versucht unser Gehirn, sie mit dem zu vereinbaren, was wir bereits wissen. Ideen, die bereits mit unseren bestehenden Überzeugungen übereinstimmen, werden leicht zu unserem Weltbild hinzugefügt. Ideen, die zu unterschiedlich von dem sind, was wir bereits glauben, verursachen kognitive Dissonanz - wir fühlen uns unwohl und versuchen, die Ideen abzulehnen oder alternative Erklärungen für das zu finden, was wir hören.

Viele Überzeugungen, die die meisten Menschen haben, werden durch die Idee des existenziellen Risikos in Frage gestellt:

- Die Technologie ist da, um uns zu dienen und kann leicht kontrolliert werden.
- Es gibt intelligente Menschen, die dafür sorgen, dass alles in Ordnung ist.
- Ich werde wahrscheinlich alt werden, und meine Kinder auch.

Viele dieser Gedanken werden durch die Idee in Frage gestellt, dass KI ein existenzielles Risiko darstellt. Unser Gehirn sucht nach alternativen Erklärungen dafür, warum wir hören, dass Wissenschaftler uns warnen:

- Sie werden von Big-Tech-Unternehmen bezahlt.
- Sie sind Teil einer Verschwörung oder Sekte.
- Sie wollen nur Aufmerksamkeit oder Macht.

Die Internalisierung, dass Wissenschaftler uns warnen, weil sie glauben, dass wir in Gefahr sind, steht im Widerspruch zu unseren bestehenden Überzeugungen und verursacht zu viel kognitive Dissonanz.

### Das Ende der Welt ist noch nie passiert {#the-end-of-the-world-has-never-happened}

Sehen ist glauben (siehe Verfügbarkeitsheuristik). Das ist ein Problem für das Aussterberisiko, weil wir es nie sehen werden, bevor es zu spät ist.

Andererseits haben wir tonnenweise Beweise für das Gegenteil. Das Ende der Zeiten wurde von vielen Menschen vorhergesagt, und jeder Einzelne von ihnen hat sich bisher geirrt.

Wenn die Menschen also von existenziellem Risiko hören, denken sie, es sei nur eine weitere dieser Weltuntergangs-Sekten-Vorhersagen. Versuchen Sie, diese Sichtweise zu verstehen und seien Sie nicht zu streng mit Menschen, die so denken. Sie haben wahrscheinlich nicht die gleichen Informationen wie Sie.

### Wir möchten glauben, dass wir besonders sind {#we-like-to-think-that-we-are-special}

Sowohl auf kollektiver als auch auf individueller Ebene möchten wir glauben, dass wir besonders sind.

Auf kollektiver Ebene möchten wir die Menschen als etwas sehr anderes als Tiere sehen - Darwins Idee, dass wir von Affen abstammen, war für die meisten fast undenkbar. Die meisten Religionen haben Geschichten über den Himmel oder die Wiedergeburt, in denen die Menschen (oder zumindest die Gläubigen) auf irgendeine Weise ewig leben werden. Die Idee, dass die Menschheit eines Tages nicht mehr existieren könnte, ist sehr verstörend und schwer zu internalisieren. Wir möchten glauben, dass wir eine Art "Plot-Panzer" haben - dass wir die Hauptfiguren in einer Geschichte sind und dass die Geschichte ein glückliches Ende haben wird. Die Menschen können es rational in Betracht ziehen, aber sie werden es nicht fühlen.

Ein Video von Robert Miles mit dem Titel "Es gibt keine Regel, die sagt, dass wir es schaffen werden" erklärt dies sehr gut.

Auf individueller Ebene sind wir stolz auf unsere einzigartigen intellektuellen Fähigkeiten. Viele wollten nie glauben, dass eine KI eines Tages in der Lage sein könnte, Kunst zu schaffen, Bücher zu schreiben oder uns sogar beim Schach zu schlagen. Der Gedanke, dass unsere eigene Intelligenz nur ein Produkt der Evolution ist und dass sie von einer Maschine repliziert werden kann, ist etwas, das viele Menschen schwer akzeptieren können. Dies macht es schwierig zu akzeptieren, dass eine KI intelligenter sein könnte als wir.

### Fiktion hat uns darauf konditioniert, ein glückliches Ende zu erwarten {#fiction-has-conditioned-us-to-expect-a-happy-ending}

Die meisten Dinge, die wir über existenzielles Risiko wissen, stammen aus der Fiktion. Dies hilft wahrscheinlich nicht, weil fiktive Geschichten nicht geschrieben werden, um realistisch zu sein: Sie werden geschrieben, um unterhaltsam zu sein.

In der Fiktion gibt es oft einen Helden, Konflikt, Hoffnung und schließlich ein glückliches Ende. Wir sind darauf konditioniert, einen Kampf zu erwarten, den wir gewinnen können. In Science-Fiction werden KIs oft sehr anthropomorph dargestellt - als böse, als menschlich, als ihre Ziele ändernd. All dies stimmt nicht mit dem überein, worüber KI-Sicherheitsexperten besorgt sind.

Und in den meisten Geschichten gewinnt der Held. Die KI macht einen dummen Fehler, und der Held findet einen Weg, die Sache, die viel intelligenter sein soll, zu überlisten. Der Held ist durch eine Art "Plot-Panzer" geschützt. In realistischeren KI-Untergangsszenarien gibt es keinen Helden, keinen Plot-Panzer, keinen Kampf, kein menschliches Ausmanövrieren einer Superintelligenz und kein glückliches Ende.

### Fortschritt war immer (meist) gut {#progress-has-always-been-mostly-good}

Viele der Technologien, die in unsere Gesellschaft eingeführt wurden, waren größtenteils vorteilhaft für die Menschheit. Wir haben Krankheiten geheilt, unsere Lebenserwartung erhöht und unser Leben komfortabler gemacht. Und jedes Mal, wenn wir dies getan haben, gab es Menschen, die sich diesen Innovationen widersetzten und vor den Gefahren warnten. Die Ludditen zerstörten die Maschinen, die ihre Arbeitsplätze nahmen, und die Menschen hatten Angst vor den ersten Zügen und Autos. Diese Menschen haben sich immer geirrt.

### Wir möchten nicht an unseren Tod denken {#we-dont-like-to-think-about-our-death}

Der menschliche Geist mag keine schlechten Nachrichten, und er hat verschiedene Bewältigungsmechanismen, um damit umzugehen. Die wichtigsten, wenn es um existenzielles Risiko geht, sind Verleugnung und Kompartimentalisierung. Wenn es um unseren eigenen Tod geht, sind wir sehr anfällig für Verleugnung. Bücher wurden über die Verleugnung des Todes geschrieben.

Diese Bewältigungsmechanismen schützen uns vor dem Schmerz, akzeptieren zu müssen, dass die Welt nicht so ist, wie wir dachten. Sie können uns jedoch auch daran hindern, auf eine Bedrohung angemessen zu reagieren.

Wenn Sie bemerken, dass jemand diese Bewältigungsmechanismen verwendet, versuchen Sie, empathisch zu sein. Sie tun es nicht absichtlich, und sie sind nicht dumm. Es ist eine natürliche Reaktion auf schlechte Nachrichten, und wir alle tun es in gewissem Maße.

### Es ist schwer, zuzugeben, dass die eigene Arbeit gefährlich ist {#admitting-your-work-is-dangerous-is-hard}

Für diejenigen, die an KI-Fähigkeiten gearbeitet haben, ist es noch schwieriger, die Gefahren zu akzeptieren.

Nehmen Sie Yoshua Bengio zum Beispiel. Yoshua Bengio hat einen brillanten Geist und ist einer der Pioniere auf dem Gebiet der KI. KI-Sicherheitsexperten warnen seit Jahren vor den potenziellen Gefahren der KI, aber es dauerte lange, bis er ihre Warnungen ernst nahm. In einem Interview gab er die folgende Erklärung:

> "Warum habe ich nicht früher darüber nachgedacht? Warum hat Geoffrey Hinton nicht früher darüber nachgedacht? [...] Ich glaube, es gibt einen psychologischen Effekt, der immer noch bei vielen Menschen vorherrscht. [...] Es ist sehr schwer, aus beruflicher Sicht zuzugeben, dass das, woran man seit Jahrzehnten arbeitet, tatsächlich sehr gefährlich für die Menschheit sein könnte. [...] Ich denke, ich wollte nicht zu viel darüber nachdenken, und das ist wahrscheinlich auch bei anderen der Fall."

Es sollte niemanden überraschen, dass einige der heftigsten KI-Risiko-Leugner KI-Forscher selbst sind.

### Leicht zu entkräften als Verschwörung oder Sekte {#easy-to-dismiss-as-conspiracy-or-cult}

Im letzten Jahr wurde der größte Teil der Bevölkerung mit dem Konzept des existenziellen Risikos durch KI bekannt gemacht. Wenn die Menschen davon hören, suchen sie nach einer Erklärung. Die richtige Erklärung ist, dass KI tatsächlich gefährlich ist, aber dies zu glauben ist schwierig und beängstigend: Es wird zu viel kognitive Dissonanz führen. Die Menschen werden also fast direkt nach einer anderen Erklärung suchen.

Es gibt zwei alternative Erklärungen, die viel einfacher zu glauben sind:

1. **Es ist alles eine große Verschwörung**. KI-Unternehmen hypen KI, um mehr Geld zu bekommen, und KI-Sicherheitsleute sind Teil dieser Hype-Maschine. Diese Erzählung stimmt mit verschiedenen Beobachtungen überein: Unternehmen lügen oft, viele KI-Sicherheitsforscher werden von KI-Unternehmen beschäftigt, und es gibt eine Reihe von Milliardären, die KI-Sicherheitsforschung finanzieren. Wir können jedoch auch darauf hinweisen, warum diese Verschwörungsgeschichte einfach nicht wahr ist. Viele der Alarmisten sind Wissenschaftler, die nichts zu gewinnen haben. Die Unternehmen könnten auf irgendeine Weise profitieren, aber bis vor kurzem (Mai 2023) haben sie sich fast vollständig still über KI-Risiken gehalten. Dies macht Sinn, da Unternehmen meist nicht davon profitieren, wenn die Menschen Angst vor ihrem Produkt oder ihrer Dienstleistung haben. Wir haben vor Microsoft und OpenAI protestiert, teilweise weil wir wollten, dass sie die Risiken anerkennen.
2. **Es ist eine Sekte**. Die Gruppe, die an KI-Sicherheit glaubt, ist nur eine Gruppe von verrückten religiösen Extremisten, die an das Ende der Welt glauben. Dies scheint auch zu passen, da die Menschen in der KI-Sicherheitsgemeinschaft oft sehr leidenschaftlich über das Thema sind und allerlei Insider-Jargon verwenden. Es fällt jedoch auseinander, wenn man darauf hinweist, dass die Menschen, die vor KI-Risiken warnen, keine einzige Organisation sind. Es ist eine große, vielfältige Gruppe von Menschen, es gibt keinen einzigen Anführer, es gibt keine Rituale, und es gibt keinen Dogmatismus.

Was diese Erklärungen so überzeugend macht, ist nicht nur, dass sie einfach zu verstehen sind oder dass sie alle Beobachtungen perfekt erklären - der Hauptgrund ist, dass sie tröstlich sind. Zu glauben, dass die Menschen vor KI warnen, weil es eine reale Bedrohung gibt, ist beängstigend und schwierig zu akzeptieren.

## Schwierig zu verstehen {#difficult-to-understand}

Die Argumente für das existenzielle Risiko durch KI sind oft sehr technisch, und wir neigen dazu, KI-Systeme zu anthropomorphisieren.

### KI-Alignment ist überraschend schwierig {#ai-alignment-is-surprisingly-hard}

Die Menschen könnten intuitiv denken, dass sie das KI-Alignment-Problem lösen könnten. Warum nicht einfach einen Stopp-Knopf hinzufügen? Warum nicht die KI wie ein Kind aufziehen? Warum nicht Asimovs drei Gesetze? Im Gegensatz zu den meisten technischen Problemen haben die Menschen eine Meinung darüber, wie man das KI-Alignment-Problem lösen kann, und unterschätzen die Schwierigkeit des Problems. Das Verständnis der tatsächlichen Schwierigkeit erfordert viel Zeit und Mühe.

### Wir anthropomorphisieren {#we-anthropomorphize}

Wir sehen Gesichter in Wolken, und wir sehen menschliche Qualitäten in KI-Systemen. Millionen von Jahren der Evolution haben uns zu hochsozialen Wesen gemacht, aber diese Instinkte sind nicht immer hilfreich. Wir neigen dazu, KIs als menschliche Ziele und Motivationen zu sehen, Emotionen zu fühlen und ein moralisches Empfinden zu haben. Wir erwarten von einer sehr intelligenten KI, dass sie auch sehr weise und freundlich ist. Dies ist einer der Gründe, warum die Menschen intuitiv denken, dass KI-Alignment einfach ist, und warum die Orthogonalitäts-These so kontraintuitiv sein kann.

### KI-Sicherheit verwendet komplexe Sprache {#ai-safety-uses-complex-language}

Das Feld der KI-Sicherheit besteht größtenteils aus einer kleinen Gruppe von (intelligenten) Menschen, die ihren eigenen Jargon entwickelt haben. Das Lesen von LessWrong-Beiträgen kann sich wie das Lesen einer fremden Sprache anfühlen. Viele Beiträge gehen davon aus, dass der Leser bereits mit mathematischen Konzepten, verschiedenen technischen Konzepten und dem Jargon des Fachgebiets vertraut ist.

## Schwierig, darauf zu reagieren {#difficult-to-act-on}

Selbst wenn die Menschen die Argumente verstehen, ist es schwierig, darauf zu reagieren. Der Einfluss ist zu groß, wir haben Bewältigungsmechanismen, die die Risiken herunterspielen, und wenn wir die Schwere der Situation fühlen, können wir uns machtlos fühlen.

### Mangel an angeborener Angstreaktion {#lack-of-innate-fear-response}

Unsere Gehirne haben sich entwickelt, um Dinge zu fürchten, die gefährlich sind. Wir fürchten instinktiv Höhen, große Tiere mit scharfen Zähnen, plötzliche laute Geräusche und Dinge, die sich in einer S-Form bewegen. Eine superintelligente KI trifft jedoch nicht auf unsere ursprünglichen Ängste. Darüber hinaus haben wir eine starke Angst vor sozialer Ablehnung oder dem Verlust des sozialen Status, was bedeutet, dass die Menschen Angst haben, über KI-Risiken zu sprechen.

### Umfangsinsensibilität {#scope-insensitivity}

> "Ein einzelner Tod ist eine Tragödie; ein Million Tote sind eine Statistik." - Joseph Stalin

Umfangsinsensibilität ist die menschliche Tendenz, die Auswirkungen großer Zahlen zu unterschätzen. Wir kümmern uns nicht 10-mal so viel um 1000 Tote wie um 100 Tote. Existenzielles Risiko bedeutet den Tod aller 8 Milliarden Menschen auf der Erde (ohne ihre Nachkommen).

Selbst wenn es nur eine 1-prozentige Chance gibt, dass dies passiert, ist es immer noch ein sehr großes Problem. Rational sollten wir diese 1-prozentige Chance von 8 Milliarden Toten genauso wichtig nehmen wie den sicheren Tod von 80 Millionen Menschen.

Wenn jemand denkt, dass das Ende der Welt nicht so schlimm ist (Sie wären überrascht, wie oft dies passiert), können Sie versuchen, die Dinge persönlicher zu machen. Die Menschheit ist nicht nur ein abstraktes Konzept, sondern Ihre Freunde, Ihre Familie und Sie selbst. Alle Menschen, die Ihnen wichtig sind, werden sterben.

### Unser Verhalten wird durch unsere Umgebung und primitive Geister geprägt {#our-behavior-is-shaped-by-our-environment-and-primitive-minds}

Unsere Handlungen werden durch das bestimmt, was als normal, gut und vernünftig angesehen wird. Egal, wie sehr wir in einer Situation handeln möchten, die danach verlangt, wenn diese Handlungen ungewöhnlich sind, fürchten wir uns oft bewusst oder unbewusst davor, von der Gesellschaft ausgeschlossen zu werden. Und was normal ist, wird uns durch unsere enge soziale Umgebung und Online-Feeds eingeimpft. Menschen, die einfach Dinge tun und über Dinge sprechen, die nichts mit dem zu tun haben, was uns wirklich wichtig ist, werden das, was in unseren Köpfen ist, überschreiben und uns motivieren, täglich andere Dinge zu tun.

Existentielle Risiken verdienen viel mehr unserer Zeit, Energie und Aufmerksamkeit. Unsere Reaktionen darauf sollten eher wie lebensbedrohliche Situationen sein, die uns mit Adrenalin füllen. Aber wegen der abstrakten Natur der Probleme und unserer unangepassten Geister gehen die meisten Menschen, die davon erfahren, einfach weiter mit ihrem Tag, als hätten sie nichts gelernt.

### Bewältigungsmechanismen (Verhinderung von Handlungen) {#coping-mechanisms-preventing-action}

Die gleichen Bewältigungsmechanismen, die die Menschen daran hindern, an existenzielles Risiko zu glauben, hindern sie auch daran, darauf zu reagieren. Wenn Sie in Verleugnung oder Kompartimentalisierung sind, werden Sie nicht das Bedürfnis fühlen, etwas dagegen zu unternehmen.

### Stress und Angst {#stress-and-anxiety}

Während ich dies schreibe, fühle ich mich gestresst und ängstlich. Es ist nicht nur, weil ich das Ende der Welt fürchte, sondern auch, weil ich das Gefühl habe, dass ich etwas dagegen unternehmen muss. Es gibt viel Druck zu handeln, und es kann überwältigend sein. Dieser Stress kann ein guter Motivator sein, aber er kann auch lähmend sein.

### Hoffnungslosigkeit und Machtlosigkeit {#hopelessness-and-powerlessness}

Wenn die Menschen das Thema ernst nehmen und die volle Schwere der Situation begreifen, können sie sich hoffnungslos und machtlos fühlen. Es kann sich wie eine Krebsdiagnose anfühlen: Sie werden früher sterben, als Sie wollten, und es gibt nichts, was Sie dagegen tun können. Das Problem ist zu groß, um es anzugehen, und Sie sind zu klein. Die meisten Menschen sind keine KI-Sicherheitsexperten oder erfahrene Lobbyisten, also wie können sie etwas dagegen unternehmen?

## Aber Sie können helfen! {#but-you-can-help}

Es gibt viele Dinge, die Sie tun können. Ein Brief zu schreiben, an einer Demonstration teilzunehmen, Geld zu spenden oder einer Gemeinschaft beizutreten, ist nicht so schwer! Und diese Aktionen haben eine reale Auswirkung. Selbst wenn wir dem Ende der Welt gegenüberstehen, kann es immer noch Hoffnung und sehr lohnende Arbeit geben. Treten Sie PauseAI bei und werden Sie Teil unserer Bewegung.