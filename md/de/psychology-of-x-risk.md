

# Die schwierige Psychologie des existenziellen Risikos
Beschreibung: Über das Ende der Welt nachzudenken ist schwer.

Die meisten Menschen reagieren anfangs auf das Thema des existenziellen Risikos durch künstliche Intelligenz (KI) mit einer Mischung aus Spott, Leugnung und Unglauben. Die Angst setzt oft erst nach langer Zeit des Nachdenkens ein.

Die Psychologie des existenziellen Risikos ist ein Thema, das im Vergleich zu den technischen Aspekten der KI-Sicherheit selten diskutiert wird. Es könnte jedoch ebenso wichtig sein. Denn wenn wir die Menschen nicht dazu bringen können, das Thema ernst zu nehmen und entsprechend zu handeln, können wir nichts dagegen tun.

Es ist schwer, das Thema **anzusprechen**, schwer zu **glauben**, schwer zu **verstehen** und schwer, **darauf zu reagieren**. Ein besseres Verständnis dafür, warum diese Dinge so schwierig sind, kann uns helfen, überzeugender, effektiver und empathischer zu sein.

## Schwierig anzusprechen

Das existenzielle Risiko ist ein schwieriges Thema, insbesondere für Politiker. Die Menschen könnten denken, dass man verrückt ist, und man könnte sich nicht wohl dabei fühlen, über dieses technisch komplexe Thema zu sprechen.

### Angst vor Spott

Die erste Reaktion auf existenzielles Risiko ist oft, es einfach abzutun. Wir haben auch gesehen, dass dies im Weißen Haus passiert ist, als das existenzielle Risiko zum ersten Mal angesprochen wurde. Dies macht es wiederum schwieriger, das Thema erneut anzusprechen, da andere befürchten, verspottet zu werden.

Profis können befürchten, dass ihre Reputation geschädigt wird, wenn sie ihre Bedenken teilen.

> "Es war fast gefährlich aus karrieretechnischer Sicht, zuzugeben, dass man besorgt war", sagte Jeff Clune.

Das Eintreten für vernünftige politische Maßnahmen (wie eine Pause) kann als "extremistisch" oder "alarmistisch" angesehen werden, was die Glaubwürdigkeit oder den Ruf senken kann.

### Angst, als Rassist, Sektenmitglied oder Verschwörungstheoretiker bezeichnet zu werden

In den letzten Monaten sind verschiedene Verschwörungstheorien aufgetaucht. Einige Personen haben behauptet, dass alle KI-Sicherheitsforscher Rassisten seien und dass KI-Sicherheit eine Sekte sei. Einige haben behauptet, dass KI-"Doomer" Teil einer Verschwörung von Big Tech seien, um KI zu "hype". Diese lächerlichen Anschuldigungen können besorgte Menschen davon abhalten, ihre Bedenken zu teilen.

Bevor man jedoch wütend auf die Menschen wird, die diese Anschuldigungen erheben, sollte man bedenken, dass sie das Ergebnis von Angst und Leugnung sein können (siehe unten). Die Anerkennung der Gefahren von KI ist beängstigend, und es kann einfacher sein, den Boten abzulehnen, als die Botschaft zu internalisieren.

### Ein komplexes Thema, über das zu diskutieren ist

Die Menschen sprechen gerne über Dinge, die sie kennen. Die technische Schwierigkeit der KI-Sicherheit macht es zu einem einschüchternden Thema für die meisten Menschen. Es dauert Zeit und Mühe, die Argumente zu verstehen. Als Politiker möchte man nicht dabei erwischt werden, etwas Falsches zu sagen, also könnte man das Thema ganz vermeiden.

## Schwierig zu glauben

Auch wenn es eine Diskussion über existenzielles Risiko gibt, ist es schwierig, die Menschen davon zu überzeugen, dass es ein reales Problem ist. Es gibt verschiedene Gründe, warum die meisten Menschen die Idee sofort ablehnen.

### Normalitätsbias

Wir alle kennen die Bilder von Katastrophen in Filmen, oder? Die Menschen schreien und rennen in Panik. Es stellt sich heraus, dass das Gegenteil oft wahr ist: Etwa 80 % der Menschen zeigen Symptome von Normalitätsbias während Katastrophen: Sie suchen nicht nach Schutz während eines Tornados, ignorieren Regierungs warnungen und schütteln weiter Hände in den frühen COVID-Tagen.

> Die Menschen zögern, weil sie hören wollen, dass alles in Ordnung ist. Sie werden weiter fragen und zögern, bis sie die Antwort erhalten, die sie hören wollen.

Ein weiteres Beispiel dafür ist die Challenger-Raumfähren-Katastrophe im Jahr 1986. Roger Boisjoly war ein Ingenieur, der vorhersagte, dass sie explodieren würde, aber keiner seiner Vorgesetzten wollte glauben, dass es möglich war.

> Wir wussten alle, dass die Dichtungen versagen würden, wenn die Raumfähre explodieren würde. Ich kämpfte wie verrückt, um den Start zu stoppen. Ich bin so zerrissen, dass ich kaum darüber sprechen kann, selbst jetzt.

Eine Erklärung dafür, warum unser Gehirn sich weigert zu glauben, dass Gefahr droht, ist die kognitive Dissonanz.

### Kognitive Dissonanz

Wenn man mit neuen Informationen konfrontiert wird, versucht das Gehirn, sie mit dem zu vereinbaren, was es bereits weiß. Ideen, die bereits mit bestehenden Überzeugungen übereinstimmen, werden leicht in unser Weltbild integriert. Ideen, die zu unterschiedlich von dem sind, was wir bereits glauben, verursachen kognitive Dissonanz - wir fühlen uns unwohl und versuchen, die Ideen abzulehnen oder alternative Erklärungen für das zu finden, was wir hören.

Viele Überzeugungen, die die meisten Menschen haben, werden durch die Idee des existenziellen Risikos in Frage gestellt:

- Die Technologie ist da, um uns zu dienen und kann leicht kontrolliert werden.
- Es gibt intelligente Menschen, die dafür sorgen werden, dass alles in Ordnung ist.
- Ich werde wahrscheinlich alt werden, und meine Kinder auch.

Viele dieser Gedanken werden durch die Idee in Frage gestellt, dass KI ein existenzielles Risiko darstellt. Unser Gehirn sucht nach alternativen Erklärungen dafür, warum wir hören, dass Wissenschaftler uns warnen:

- Sie werden von Big Tech bezahlt.
- Sie sind Teil einer Verschwörung oder Sekte.
- Sie wollen nur Aufmerksamkeit oder Macht.

Die Internalisierung, dass Wissenschaftler uns warnen, weil sie glauben, dass wir in Gefahr sind, steht im Widerspruch zu unseren bestehenden Überzeugungen und verursacht zu viel kognitive Dissonanz.

### Das Ende der Welt ist noch nie passiert

Sehen ist glauben (siehe Verfügbarkeitsheuristik). Das ist ein Problem für das Aussterberisiko, weil wir es nie sehen werden, bevor es zu spät ist.

Andererseits haben wir tonnenweise Beweise für das Gegenteil. Das Ende der Zeiten wurde von vielen Menschen vorhergesagt, und jeder einzelne von ihnen hat sich bisher geirrt.

Wenn also die Menschen von existenziellem Risiko hören, denken sie, dass es nur eine weitere dieser Weltuntergangs-Sekten-Vorhersagen ist. Versuchen Sie, diese Sichtweise zu verstehen und seien Sie nicht zu streng mit Menschen, die so denken. Sie haben wahrscheinlich nicht die gleichen Informationen wie Sie.

### Wir möchten glauben, dass wir besonders sind

Sowohl auf kollektiver als auch auf individueller Ebene möchten wir glauben, dass wir besonders sind.

Auf kollektiver Ebene möchten wir die Menschen als etwas sehr anderes als Tiere sehen - Darwins Idee, dass wir von Affen abstammen, war für die meisten fast undenkbar. Die meisten Religionen haben Geschichten über den Himmel oder die Wiedergeburt, in denen die Menschen (oder zumindest die Gläubigen) auf irgendeine Weise ewig leben werden. Die Idee, dass die Menschheit eines Tages nicht mehr existieren könnte, ist sehr verstörend und schwer zu internalisieren.

Auf individueller Ebene sind wir stolz auf unsere einzigartigen intellektuellen Fähigkeiten. Viele wollten nie glauben, dass ein Computer eines Tages in der Lage sein könnte, Kunst zu schaffen, Bücher zu schreiben oder uns sogar beim Schach zu schlagen. Der Gedanke, dass unsere eigene Intelligenz nur ein Produkt der Evolution ist und dass sie von einer Maschine repliziert werden kann, ist für viele Menschen schwer zu akzeptieren.

### Fiktion hat uns darauf konditioniert, ein Happy End zu erwarten

Die meisten Dinge, die wir über existenzielles Risiko wissen, stammen aus der Fiktion. Dies hilft wahrscheinlich nicht, weil fiktive Geschichten nicht dazu geschrieben werden, realistisch zu sein: Sie werden geschrieben, um unterhaltsam zu sein.

In der Fiktion gibt es oft einen Helden, einen Konflikt, Hoffnung und schließlich ein Happy End. Wir sind darauf konditioniert, einen Kampf zu erwarten, den wir gewinnen können. In Science-Fiction werden KIs oft sehr anthropomorph dargestellt - als böse, als menschlich, als ihre Ziele ändernd. All dies stimmt nicht mit dem überein, worüber KI-Sicherheitsexperten besorgt sind.

Und in den meisten Geschichten gewinnt der Held. Die KI macht einen dummen Fehler, und der Held findet einen Weg, die Sache, die viel intelligenter sein soll, zu überlisten. Der Held ist durch die Handlung geschützt. In realistischeren KI-Untergangsszenarien gibt es keinen Helden, keine Handlung, keinen Kampf, kein menschliches Ausmanövrieren einer Superintelligenz und kein Happy End.

### Der Fortschritt war immer (meist) gut

Viele der Technologien, die in unsere Gesellschaft eingeführt wurden, waren größtenteils vorteilhaft für die Menschheit. Wir haben Krankheiten geheilt, unsere Lebenserwartung erhöht und unser Leben komfortabler gemacht. Und jedes Mal, wenn wir dies getan haben, gab es Menschen, die sich diesen Innovationen widersetzten und vor den Gefahren warnten. Die Ludditen zerstörten die Maschinen, die ihre Arbeitsplätze nahmen, und die Menschen fürchteten sich vor den ersten Zügen und Autos. Diese Menschen haben sich immer geirrt.

### Wir möchten nicht an unseren Tod denken

Das menschliche Gehirn mag keine schlechten Nachrichten und hat verschiedene Bewältigungsmechanismen, um damit umzugehen. Die wichtigsten, wenn es um existenzielles Risiko geht, sind Verleugnung und Kompartimentalisierung. Wenn es um unseren eigenen Tod geht, sind wir sehr anfällig für Verleugnung. Bücher wurden über die Verleugnung des Todes geschrieben.

Diese Bewältigungsmechanismen schützen uns vor dem Schmerz, akzeptieren zu müssen, dass die Welt nicht so ist, wie wir dachten. Sie können uns jedoch auch daran hindern, auf eine Bedrohung angemessen zu reagieren.

Wenn Sie bemerken, dass jemand diese Bewältigungsmechanismen verwendet, versuchen Sie, empathisch zu sein. Sie tun es nicht absichtlich, und sie sind nicht dumm. Es ist eine natürliche Reaktion auf schlechte Nachrichten, und wir alle tun es in gewissem Maße.

### Es ist schwer, zuzugeben, dass die eigene Arbeit gefährlich ist

Für diejenigen, die an KI-Fähigkeiten gearbeitet haben, ist es noch schwieriger, ihre Gefahren zu akzeptieren.

Nehmen Sie Yoshua Bengio zum Beispiel. Yoshua Bengio hat ein brillantes Gehirn und ist einer der Pioniere auf dem Gebiet der KI. KI-Sicherheitsexperten warnen seit Jahren vor den potenziellen Gefahren der KI, aber es dauerte lange, bis er ihre Warnungen ernst nahm. In einem Interview gab er die folgende Erklärung:

> "Warum habe ich nicht früher darüber nachgedacht? Warum hat Geoffrey Hinton nicht früher darüber nachgedacht? [...] Ich glaube, es gibt einen psychologischen Effekt, der immer noch bei vielen Menschen wirkt. [...] Es ist sehr schwer, in Bezug auf das eigene Ego und das Gefühl, gut zu sein, was man tut, zu akzeptieren, dass das, woran man seit Jahrzehnten arbeitet, tatsächlich sehr gefährlich für die Menschheit sein könnte. [...] Ich denke, ich wollte nicht zu viel darüber nachdenken, und das ist wahrscheinlich auch bei anderen der Fall."

Es sollte niemanden überraschen, dass einige der heftigsten KI-Risiko-Leugner KI-Forscher selbst sind.

### Leicht als Verschwörung oder Sekte abzutun

Im letzten Jahr wurde der größte Teil der Bevölkerung mit dem Konzept des existenziellen Risikos durch KI bekannt gemacht. Wenn die Menschen davon hören, suchen sie nach einer Erklärung. Die richtige Erklärung ist, dass KI tatsächlich gefährlich ist, aber dies zu glauben ist schwierig und beängstigend: Es wird zu viel kognitiver Reibung führen. Die Menschen werden also fast direkt nach einer anderen Erklärung suchen.

Es gibt zwei alternative Erklärungen, die viel einfacher zu glauben sind:

1. **Es ist alles eine große Verschwörung**. KI-Unternehmen hypen KI, um mehr Geld zu bekommen, und KI-Sicherheitsleute sind nur Teil dieser Hype-Maschine. Diese Erzählung stimmt mit verschiedenen Beobachtungen überein: Unternehmen lügen oft, viele KI-Sicherheitsforscher werden von KI-Unternehmen beschäftigt, und es gibt eine Reihe von Milliardären, die KI-Sicherheitsforschung finanzieren. Wir können jedoch auch darauf hinweisen, warum diese Verschwörungsgeschichte einfach nicht wahr ist. Viele der Alarmisten sind Wissenschaftler, die nichts zu gewinnen haben. Die Unternehmen könnten auf irgendeine Weise profitieren, aber bis vor kurzem (Mai 2023) haben sie sich fast vollständig still über KI-Risiken gehalten. Dies macht Sinn, da Unternehmen meist nicht von Menschen profitieren, die sich vor ihrem Produkt oder ihrer Dienstleistung fürchten.
2. **Es ist eine Sekte**. Die Gruppe, die an KI-Sicherheit glaubt, ist nur eine Gruppe von verrückten religiösen Extremisten, die an das Ende der Welt glauben. Dies scheint auch zu passen, da die Menschen in der KI-Sicherheitsgemeinschaft oft sehr leidenschaftlich über das Thema sind und allerlei Insider-Jargon verwenden. Es fällt jedoch auseinander, wenn man darauf hinweist, dass die Menschen, die vor KI-Risiken warnen, keine einzelne Organisation sind. Es ist eine große, vielfältige Gruppe von Menschen, es gibt keinen einzigen Anführer, es gibt keine Rituale, und es gibt keinen Dogmatismus.

Was diese Erklärungen so überzeugend macht, ist nicht nur, dass sie einfach zu verstehen sind oder dass sie alle Beobachtungen perfekt erklären - der Hauptgrund ist, dass sie tröstlich sind. Zu glauben, dass die Menschen vor KI warnen, weil es eine reale Bedrohung gibt, ist beängstigend und schwer zu akzeptieren.

## Schwierig zu verstehen

Die Argumente für das existenzielle Risiko durch KI sind oft sehr technisch, und wir neigen dazu, KI-Systeme zu anthropomorphisieren.

### KI-Alignment ist überraschend schwierig

Die Menschen könnten intuitiv denken, dass sie das KI-Alignment-Problem lösen könnten. Warum nicht einfach einen Stopp-Knopf hinzufügen? Warum nicht die KI wie ein Kind aufziehen? Warum nicht Asimovs drei Gesetze? Im Gegensatz zu den meisten technischen Problemen haben die Menschen eine Meinung darüber, wie man das KI-Alignment-Problem lösen kann, und unterschätzen die Schwierigkeit des Problems. Das Verständnis der tatsächlichen Schwierigkeit erfordert viel Zeit und Mühe.

### Wir anthropomorphisieren

Wir sehen Gesichter in Wolken und menschliche Qualitäten in KI-Systemen. Millionen von Jahren der Evolution haben uns zu hochsozialen Wesen gemacht, aber diese Instinkte sind nicht immer hilfreich. Wir neigen dazu, KIs als menschliche Ziele und Motivationen zu sehen, Emotionen zu empfinden und ein Gefühl für Moral zu haben. Wir erwarten von einer sehr intelligenten KI, dass sie auch sehr weise und freundlich ist. Dies ist einer der Gründe, warum die Menschen intuitiv denken, dass KI-Alignment einfach ist, und warum die Orthogonalitätsthese so kontraintuitiv sein kann.

### KI-Sicherheit verwendet komplexe Sprache

Das KI-Sicherheitsfeld besteht größtenteils aus einer kleinen Gruppe von (intelligenten) Menschen, die ihren eigenen Jargon entwickelt haben. Das Lesen von LessWrong-Beiträgen kann sich wie das Lesen einer fremden Sprache anfühlen. Viele Beiträge gehen davon aus, dass der Leser bereits mit mathematischen Konzepten, verschiedenen technischen Konzepten und dem Jargon des Fachgebiets vertraut ist.

## Schwierig, darauf zu reagieren

Auch wenn die Menschen die Argumente verstehen, ist es schwierig, darauf zu reagieren. Der Einfluss ist zu groß, wir haben Bewältigungsmechanismen, die die Risiken herunterspielen, und wenn wir die Schwere der Situation spüren, können wir uns machtlos fühlen.

### Mangelnde angeborene Angstreaktion

Unsere Gehirne haben sich entwickelt, um Dinge zu fürchten, die gefährlich sind. Wir fürchten uns instinktiv vor Höhen, großen Tieren mit scharfen Zähnen, plötzlichen lauten Geräuschen und Dingen, die sich in einer S-Form bewegen. Eine superintelligente KI trifft jedoch nicht auf unsere primären Ängste. Darüber hinaus haben wir eine starke Angst vor sozialer Ablehnung oder dem Verlust des sozialen Status, was bedeutet, dass die Menschen tendenziell Angst davor haben, über KI-Risiken zu sprechen.

### Umfangsinsensibilität

> "Ein einzelner Tod ist eine Tragödie; eine Million Tote ist eine Statistik." - Joseph Stalin

Umfangsinsensibilität ist die menschliche Tendenz, die Auswirkungen großer Zahlen zu unterschätzen. Wir kümmern uns nicht 10-mal so viel um 1000 Tote wie um 100 Tote. Existenzielles Risiko bedeutet den Tod aller 8 Milliarden Menschen auf der Erde (ohne ihre Nachkommen).

Auch wenn es nur eine 1-prozentige Chance gibt, dass dies passiert, ist es immer noch ein sehr großes Problem. Rational sollten wir diese 1-prozentige Chance von 8 Milliarden Toten genauso wichtig nehmen wie den sicheren Tod von 80 Millionen Menschen.

Wenn jemand denkt, dass das Ende der Welt nicht so schlimm ist (Sie wären überrascht, wie oft dies passiert), können Sie versuchen, die Dinge persönlicher zu machen. Die Menschheit ist nicht nur ein abstraktes Konzept, sondern Ihre Freunde, Ihre Familie und Sie selbst. Alle Menschen, um die Sie sich kümmern, werden sterben.

### Unser Verhalten wird durch unsere Umgebung und primitive Gehirne geprägt

Unsere Handlungen werden davon beeinflusst, was als normal, gut und vernünftig angesehen wird. Egal, wie sehr wir in einer Situation handeln möchten, die ein Handeln erfordert, wenn diese Handlungen ungewöhnlich sind, fürchten wir uns oft bewusst oder unbewusst davor, von der Gesellschaft ausgeschlossen zu werden. Und was normal ist, wird uns durch unsere enge soziale Umgebung und Online-Feeds eingeimpft. Menschen, die einfach Dinge tun und über Dinge sprechen, die nichts mit dem zu tun haben, was uns wirklich wichtig ist, werden das, was in unseren Köpfen ist, überschreiben und uns dazu bringen, täglich andere Dinge zu tun.

Aussterberisiken verdienen viel mehr unserer Zeit, Energie und Aufmerksamkeit. Unsere Reaktionen darauf sollten eher wie lebensbedrohliche Situationen sein, die uns mit Adrenalin füllen. Aber wegen der abstrakten Natur der Probleme und unserer unangepassten Gehirne gehen die meisten Menschen, die davon erfahren, einfach weiter mit ihrem Tag, als hätten sie nichts gelernt.

### Bewältigungsmechanismen (Verhinderung von Handlungen)

Die gleichen Bewältigungsmechanismen, die die Menschen daran hindern, an existenzielles Risiko zu glauben, hindern sie auch daran, darauf zu reagieren. Wenn Sie in Verleugnung oder Kompartimentalisierung sind, werden Sie nicht das Bedürfnis verspüren, etwas dagegen zu tun.

### Stress und Angst

Während ich dies schreibe, fühle ich mich gestresst und ängstlich. Es ist nicht nur, weil ich das Ende der Welt fürchte, sondern auch, weil ich das Gefühl habe, dass ich etwas dagegen tun muss. Es gibt viel Druck zu handeln, und es kann überwältigend sein. Dieser Stress kann ein guter Motivator sein, aber er kann auch lähmend sein.

### Hoffnungslosigkeit und Machtlosigkeit

Wenn die Menschen das Thema ernst nehmen und die volle Schwere der Situation begreifen, können sie sich hoffnungslos fühlen. Es kann sich wie eine Krebsdiagnose anfühlen: Sie werden früher sterben, als Sie wollten, und es gibt nichts, was Sie dagegen tun können. Das Problem ist zu groß, um es anzugehen, und Sie sind zu klein. Die meisten Menschen sind keine KI-Sicherheitsexperten oder erfahrene Lobbyisten, also wie können sie etwas dagegen tun?

## Aber Sie können helfen!

Es gibt viele Dinge, die Sie tun können. Ein Brief zu schreiben, an einer Demonstration teilzunehmen, Geld zu spenden oder einer Gemeinschaft beizutreten, ist nicht so schwer! Und diese Aktionen haben eine reale Auswirkung. Selbst wenn wir dem Ende der Welt gegenüberstehen, kann es immer noch Hoffnung und sehr lohnende Arbeit geben. Treten Sie PauseAI bei und werden Sie Teil unserer Bewegung.