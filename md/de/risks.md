---
title: Risiken der künstlichen Intelligenz
description: KI bedroht unsere Demokratie, unsere Technologie und unsere Spezies.
---

KI ist eine leistungsfähige Technologie, die unsere Welt zunehmend verändert.
Sie bietet enormes Potenzial, aber auch eine enorme Menge an ernsthaften Risiken.
Dies ist ein Versuch, alle Risiken aufzulisten, die durch eine Pause gemildert werden könnten.

## Aktuelle Gefahren

### Falschmeldungen, Polarisierung und Bedrohung der Demokratie

Ein großer Teil unserer Gesellschaft basiert auf Vertrauen. Wir vertrauen darauf, dass das Geld auf unserem Bankkonto real ist, dass die Nachrichten, die wir lesen, wahr sind, und dass die Menschen, die Online-Rezensionen posten, existieren.

KI-Systeme sind außergewöhnlich gut darin, falsche Medien zu erstellen.
Sie können falsche Videos, falsche Audios, falsche Texte und falsche Bilder erstellen.
Die Erstellung falscher Medien ist nicht neu, aber KI macht es viel billiger und viel realistischer.
Diese Fähigkeiten verbessern sich rasant.

Vor gerade zwei Jahren haben wir über die schlecht gemachten Dall-E-Bilder gelacht, aber jetzt haben wir [Deepfake-Bilder, die Fotowettbewerbe gewinnen](https://www.theguardian.com/technology/2023/apr/17/photographer-admits-prize-winning-image-was-ai-generated).
Ein KI-generiertes Bild einer Explosion verursachte [Panikverkäufe an der Wall Street](https://www.euronews.com/next/2023/05/23/fake-news-about-an-explosion-at-the-pentagon-spreads-on-verified-accounts-on-twitter).
Ein 10-Sekunden-Audio-Clip oder ein einzelnes Bild kann ausreichen, um ein überzeugendes Deepfake zu erstellen.
Vielleicht noch gefährlicher als die Deepfakes selbst ist, wie die Existenz von überzeugenden Deepfakes das Vertrauen zerstört.
[Reale Bilder können als KI-generiert bezeichnet werden](https://www.axios.com/2024/08/13/trump-crowd-photo-ai-deepfake-truth), und die Menschen werden es glauben.

GPT-4 kann auf eine Weise schreiben, die von Menschen nicht zu unterscheiden ist, aber mit einer viel schnelleren Geschwindigkeit und einem Bruchteil der Kosten.
Wir könnten bald sehen, dass soziale Medien mit falschen Diskussionen und Meinungen und falschen Nachrichtenartikeln überschwemmt werden, die von echten nicht zu unterscheiden sind.

Dies führt zu einer Polarisierung zwischen verschiedenen Gruppen von Menschen, die an verschiedene Informationsquellen und Narrative glauben und durch verzerrte Darstellungen dessen, was passiert, ihre Differenzen eskalieren lassen, bis sie in gewalttätige und antidemokratische Reaktionen kulminieren.

Ein Stopp der Entwicklung von leistungsfähigeren KI-Modellen (unser [Vorschlag](/proposal)) würde nicht die Modelle stoppen, die heute verwendet werden, um falsche Medien zu erstellen, aber es könnte helfen, zukünftige leistungsfähige Modelle zu verhindern.
Außerdem würde es den Grundstein für zukünftige Regulierungen legen, die darauf abzielen, falsche Medien und andere spezifische Probleme zu mildern, die durch KI verursacht werden. Nicht zu vergessen ist die Erhöhung der öffentlichen Aufmerksamkeit und des Bewusstseins für diese Gefahren und der Beweis, dass sie angegangen werden können.

### Deepfakes und Identitätsdiebstahl

Falsche Inhalte, die mit KI erstellt werden, auch Deepfakes genannt, können nicht nur die Identität von berühmten Menschen stehlen und [Desinformation verbreiten](https://time.com/6565446/biden-deepfake-audio/), sondern auch Sie selbst darstellen.
Jeder, der Fotos, Videos oder Audios von jemandem und genug Wissen hat, kann Deepfakes von ihnen erstellen und sie verwenden, um Betrug zu begehen, sie zu belästigen oder nicht einvernehmliche sexuelle Inhalte zu erstellen.
Etwa [96% aller Deepfake-Inhalte sind sexuelle Inhalte](https://www.technologyreview.com/2019/10/07/132735/deepfake-porn-deeptrace-legislation-california-election-disinformation/).

Wie im Abschnitt über Falschmeldungen erwähnt, würden unsere Vorschläge nicht alle falschen Medien verhindern, aber sie könnten in einem gewissen Umfang reduziert werden.
Ein nicht so kleiner Umfang, wenn man bedenkt, dass KI-Systeme wie Chatbots sehr beliebt geworden sind und wir sie daran hindern würden, noch leistungsfähiger und beliebter zu werden, was auch Systeme einschließen könnte, die mit weniger Filtern und trainierbar mit neuen Gesichtern konzipiert sind.

### Vorurteile und Diskriminierung

KI-Systeme werden mit Daten trainiert, und viele der Daten, die wir haben, sind auf eine Weise voreingenommen.
Dies bedeutet, dass KI-Systeme die Vorurteile unserer Gesellschaft erben werden.
Ein automatisiertes Rekrutierungssystem bei Amazon [erbte eine Voreingenommenheit gegen Frauen](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G).
Schwarze Patienten wurden [weniger wahrscheinlich an einen medizinischen Spezialisten überwiesen](https://www.science.org/doi/full/10.1126/science.aax2342).
Voreingenommene Systeme, die in der Strafverfolgung verwendet werden, wie zum Beispiel Vorhersagealgorithmen für Polizeieinsätze, könnten zu einer unfairen Zielsetzung bestimmter Gruppen führen.
Generative KI-Modelle kopieren nicht nur die Vorurteile aus ihren Trainingsdaten, [sie verstärken sie](https://www.bloomberg.com/graphics/2023-generative-ai-bias/).
Diese Vorurteile treten oft auf, ohne dass die Ersteller des KI-Systems sich dessen bewusst sind.

### Arbeitsplatzverlust, wirtschaftliche Ungleichheit und Instabilität

Während der industriellen Revolution verloren viele Menschen ihre Arbeitsplätze an Maschinen.
Allerdings wurden neue (oft bessere) Arbeitsplätze geschaffen, und die Wirtschaft wuchs.
Diesmal könnte es anders sein.

KI ersetzt nicht nur unsere Muskeln, wie die Dampfmaschine es tat, sondern auch unsere Gehirne.
Normale Menschen könnten nichts mehr anzubieten haben, was die Wirtschaft benötigt.
Bildgenerierungsmodelle (die stark auf urheberrechtlich geschütztem Material von professionellen Künstlern trainiert werden) wirken sich bereits [auf die Kreativbranche aus](https://cointelegraph.com/news/artists-face-a-choice-with-ai-adapt-or-become-obsolete).
Schriftsteller [streiken](https://www.newscientist.com/article/2373382-why-use-of-ai-is-a-major-sticking-point-in-the-ongoing-writers-strike/).
GPT-4 hat [die Anwaltsprüfung bestanden](https://law.stanford.edu/2023/04/19/gpt-4-passes-the-bar-exam-what-that-means-for-artificial-intelligence-tools-in-the-legal-industry/), kann exzellente schriftliche Inhalte erstellen und kann Code schreiben (wiederum teilweise trainiert auf [urheberrechtlich geschütztem Material](https://www.ischool.berkeley.edu/news/2023/new-research-prof-david-bamman-reveals-chatgpt-seems-be-trained-copyrighted-books)).

Die Menschen, die diese KI-Systeme besitzen, werden in der Lage sein, sie zu kapitalisieren, aber die Menschen, die ihre Arbeitsplätze an sie verlieren, werden es nicht.
Es ist schwierig vorherzusagen, welche Arbeitsplätze zuerst ersetzt werden.
Sie könnten Sie arbeitslos und ohne Einkommen lassen, egal wie viel Zeit, Geld und Energie Sie in die Erlangung der Erfahrung und Kenntnisse investiert haben, die Sie haben, und wie wertvoll sie vor einem Moment waren.
Die Art und Weise, wie wir Reichtum in unserer Gesellschaft verteilen, ist nicht auf diese Situation vorbereitet.

Politische Maßnahmen wie ein universelles Grundeinkommen könnten die schlimmsten wirtschaftlichen Folgen verhindern, aber es ist unklar, ob sie rechtzeitig umgesetzt werden.
Sobald unsere Arbeitsplätze ersetzt sind, könnten wir ohne Verhandlungsmacht bleiben, um nach sozialen Netzen zu fragen.

Und selbst wenn wir es schaffen, die Probleme im Zusammenhang mit Ungleichheit und Instabilität richtig zu meistern, könnten wir in einer Welt enden, in der unser Sinn für Zweck verloren geht.
Viele Künstler fühlen sich bereits so, da sie sehen, dass ihre Arbeit durch KI ersetzt wird.
Bald könnten wir alle so fühlen.

### Psychische Gesundheit, Sucht und Entfremdung zwischen Menschen

Soziale Medienunternehmen haben KI-Systeme verwendet, um ihren Profit zu maximieren, während sie unsere Primatengehirne ausnutzen, um unsere psychische Gesundheit zu schädigen.
KI-Chatbots, die Benutzern eine romantische Beziehung anbieten, haben in den letzten Jahren ein enormes Wachstum erlebt, mit mehr als 3 Milliarden Suchergebnissen für "KI-Freundin" auf Google.
Diese KI-Beziehungs-Apps sind [als süchtig machend erwiesen](https://onlinelibrary.wiley.com/doi/10.1002/mar.21899), insbesondere für "einsame, verletzliche Menschen".

Die Unternehmen, die diese Apps kontrollieren, sind motiviert, sie so süchtig machend wie möglich zu machen, und haben eine enorme Macht, indem sie das Verhalten und die Meinungen dieser Modelle prägen.

Eine Pause bei den größten Modellen könnte verhindern, dass sie zu Mehrzweck-Chatbots werden, die unsere Bedürfnisse perfekt erfüllen, ohne dass die Menschen die langfristigen Auswirkungen verstehen.

### Automatisierte Ermittlungen (Verlust der Privatsphäre)

Wir hinterlassen viele Spuren im Internet.
Die Verbindung der Punkte ist schwierig und zeitaufwendig, aber KI kann dies jetzt viel billiger machen.
Große Sprachmodelle können jetzt autonom das Internet durchsuchen und sind gut genug, um große Mengen an Daten zu analysieren und interessante Details zu finden.
Dies kann verwendet werden, um Informationen zu finden, die sonst sehr teuer zu finden wären.

- Informationen darüber finden, wo eine Person wahrscheinlich zu einem bestimmten Zeitpunkt ist. Dies kann verwendet werden, um Dissidenten aufzuspüren oder Attentate zu planen.
- Anonyme Konten im Internet mit realen Identitäten verknüpfen. Dies kann verwendet werden, um herauszufinden, wer Informationen weitergibt.

Im September 2024 baute eine Gruppe von Studenten [eine App](https://x.com/AnhPhuNguyen1/status/1840786336992682409), die Informationen über Fremde wie Namen, Verwandte und andere persönliche Daten in der erweiterten Realität anzeigt, indem sie Gesichtserkennung und LLMs verwendet.

### Umweltgefahren

Umweltschäden beginnen signifikant zu werden, und die größten KI-Unternehmen planen, ihren Energieverbrauch stark zu erhöhen. Sie können hier lesen, wie KI die Umwelt negativ beeinflussen wird [/environmental].

### Autonome Waffen

Unternehmen verkaufen bereits KI-gesteuerte Waffen an Regierungen.
Lanius baut [fliegende Selbstmord-Drohnen](https://www.youtube.com/watch?v=G7yIzY1BxuI), die autonom Feinde identifizieren.
Palantirs [AIP-System](https://www.youtube.com/watch?v=XEM5qz__HOU) verwendet große Sprachmodelle, um Schlachtfelddaten zu analysieren und optimale Strategien zu entwickeln.

Nationen und Waffenunternehmen haben erkannt, dass KI einen enormen Einfluss auf die Überlegenheit gegenüber ihren Feinden haben wird.
Wir sind in ein neues Wettrüsten eingetreten.
Diese Dynamik belohnt das Beschleunigen und Abkürzen.

Im Moment haben wir noch Menschen in der Schleife für diese Waffen.
Aber wenn die Fähigkeiten dieser KI-Systeme verbessert werden, wird es immer mehr Druck geben, den Maschinen die Kontrolle zu übergeben.
Wenn wir die Kontrolle über Waffen an KI delegieren, könnten Fehler und Bugs schreckliche Konsequenzen haben.
Die Geschwindigkeit, mit der KI Informationen verarbeiten und Entscheidungen treffen kann, könnte Konflikte in Minuten eskalieren lassen.
Ein [aktueller Artikel](https://arxiv.org/pdf/2401.03408.pdf) kommt zu dem Schluss, dass "Modelle dazu neigen, eine Dynamik des Wettrüstens zu entwickeln, die zu mehr Konflikten und in seltenen Fällen sogar zum Einsatz von Nuklearwaffen führt".

Lesen Sie mehr auf [stopkillerrobots.org](https://www.stopkillerrobots.org/military-and-killer-robots/)

## Gefahren in naher Zukunft

### Machtakkumulation und Tyrannei

Leistungsfähige KI-Modelle können verwendet werden, um mehr Macht zu erlangen.
Diese positive Rückkopplungsschleife kann dazu führen, dass einige Unternehmen oder Regierungen eine ungesunde Menge an Macht haben.
Die Kontrolle über Tausende von intelligenten, autonomen Systemen könnte verwendet werden, um Meinungen zu beeinflussen, Märkte zu manipulieren oder sogar Krieg zu führen.
In den Händen einer autoritären Regierung könnte dies verwendet werden, um Dissens zu unterdrücken und die Macht aufrechtzuerhalten.

### Biologische Waffen

KI kann Wissen zugänglicher machen, was auch Wissen darüber einschließt, wie biologische Waffen erstellt werden. [Dieser Artikel](https://arxiv.org/abs/2306.03809) zeigt, wie GPT-4 nicht-wissenschaftlichen Studenten helfen kann, einen pandemischen Krankheitserreger zu erstellen:

> In einer Stunde schlugen die Chatbots vier potenzielle pandemische Krankheitserreger vor, erklärten, wie sie aus synthetischer DNA mithilfe von Reverse-Genetik erstellt werden können, lieferten die Namen von DNA-Synthese-Unternehmen, die wahrscheinlich keine Bestellungen überprüfen, identifizierten detaillierte Protokolle und wie man sie debuggt, und empfahlen, dass jeder, der nicht die Fähigkeiten für Reverse-Genetik hat, eine Kernanlage oder eine Vertragsforschungsorganisation beauftragt.

Diese Art von Wissen war noch nie so zugänglich, und wir haben nicht die Sicherheitsvorkehrungen, um mit den möglichen Konsequenzen umzugehen.

Darüber hinaus können einige KI-Modelle verwendet werden, um völlig neue gefährliche Krankheitserreger zu entwerfen.
Ein Modell namens MegaSyn entwarf [40.000 neue chemische Waffen / toxische Moleküle in einer Stunde](https://www.theverge.com/2022/3/17/22983197/ai-new-possible-chemical-weapons-generative-models-vx).
Das revolutionäre AlphaFold-Modell kann die Struktur von Proteinen vorhersagen, was auch eine [Dual-Use-Technologie](https://unicri.it/sites/default/files/2021-12/21_dual_use.pdf) ist.
Die Vorhersage von Proteinstrukturen kann verwendet werden, um "Krankheitsursachen-Mutationen mithilfe der Genomsequenz eines Individuums zu entdecken".
Wissenschaftler erstellen jetzt sogar [vollständig autonome chemische Labors, in denen KI-Systeme neue Chemikalien auf eigene Faust synthetisieren können](https://twitter.com/andrewwhite01/status/1670794000398184451).

Die grundlegende Gefahr besteht darin, dass die Kosten für die Entwicklung und Anwendung biologischer Waffen durch KI um Größenordnungen gesenkt werden.

### Computerviren und Cyberangriffe

Praktisch alles, was wir heute tun, hängt auf eine Weise von Computern ab.
Wir bezahlen unsere Lebensmittel, planen unsere Tage, kontaktieren unsere Lieben und fahren unsere Autos mit Computern.

Moderne KI-Systeme können Software analysieren und schreiben.
Sie [können Schwachstellen in Software finden](https://betterprogramming.pub/i-used-gpt-3-to-find-213-security-vulnerabilities-in-a-single-codebase-cc3870ba9411) und [sie könnten verwendet werden, um sie auszunutzen](https://blog.checkpoint.com/2023/03/15/check-point-research-conducts-initial-security-analysis-of-chatgpt4-highlighting-potential-scenarios-for-accelerated-cybercrime/).
Wenn die Fähigkeiten von KI wachsen, werden auch die Fähigkeiten der Exploits, die sie erstellen können, wachsen.

Hochpotente Computerviren waren immer extrem schwierig zu erstellen, aber KI könnte dies ändern.
Anstatt ein Team von geschickten Sicherheitsexperten/Hackern anzuheuern, um Zero-Day-Exploits zu finden, könnten Sie einfach eine viel billigere KI verwenden, um dies für Sie zu tun. Natürlich könnte KI auch bei der Cyberverteidigung helfen, und es ist unklar, auf welcher Seite der Vorteil liegt.

[Lesen Sie mehr über KI und Cybersicherheitsrisiken](/cybersecurity-risks)

### Existenzrisiko

Viele KI-Forscher warnen, dass KI zum Ende der Menschheit führen könnte.

Sehr intelligente Dinge sind sehr mächtig.
Wenn wir eine Maschine bauen, die viel intelligenter ist als Menschen, müssen wir sicherstellen, dass sie das gleiche will wie wir.
Allerdings stellt sich heraus, dass dies sehr schwierig ist.
Dies wird als _Ausrichtungsproblem_ bezeichnet.
Wenn wir es nicht rechtzeitig lösen, könnten wir mit superintelligenten Maschinen enden, die sich nicht um unser Wohlergehen kümmern.
Wir würden eine neue Spezies auf dem Planeten einführen, die uns überlisten und