

---
title: Warum wir AI-Sicherheitsgipfel benötigen
description: Warum wir den AI-Sicherheitsgipfel benötigen und was er erreichen sollte.
---

Künstliche Intelligenz (KI) birgt zahlreiche Risiken für die Menschheit, einschließlich des Risikos des Aussterbens.
Die Fortschritte in der KI-Forschung beschleunigen sich in einem atemberaubenden Tempo, und wir sind nicht auf die Konsequenzen vorbereitet.
KI-Unternehmen sind in einem Wettlauf um die Vorherrschaft, bei dem Sicherheit nicht die höchste Priorität hat.
Wir benötigen Regierungen, die eingreifen und verhindern, dass KI ein superhumanes Level erreicht, bevor wir wissen, wie wir sie sicher machen können.
Diese Pause muss auf internationaler Ebene stattfinden, da Länder in einem ähnlichen Wettlauf wie die Unternehmen stecken.
Internationale Abkommen bedeuten Verträge, und dafür müssen Länder persönlich zusammenkommen und verhandeln.
**Der einzige Weg, eine wahre Pause zu erreichen, ist durch einen Gipfel.**

Es gab einige Beispiele für internationale Gipfel und daraus resultierende Verträge, die erfolgreich waren, um Risiken zu reduzieren:

- **Montreal-Protokoll** (1987): Das Montreal-Protokoll ist ein internationales Umweltabkommen, das den Schutz der Ozonschicht durch die schrittweise Abschaffung der Produktion und des Verbrauchs von ozonschädigenden Substanzen zum Ziel hat. Es war sehr erfolgreich bei der Reduzierung des Einsatzes von Substanzen wie Fluorchlorkohlenwasserstoffen (FCKW) und hat zum allmählichen Wiederherstellen der Ozonschicht beigetragen.
- **Stockholmer Übereinkommen über persistente organische Schadstoffe** (2001): Das Stockholmer Übereinkommen ist ein internationales Abkommen, das darauf abzielt, die menschliche Gesundheit und die Umwelt vor persistenten organischen Schadstoffen (POP) zu schützen. Diese sind giftige Chemikalien, die in der Umwelt persistieren, in lebenden Organismen bioakkumulieren und schwerwiegende negative Auswirkungen auf die menschliche Gesundheit und Ökosysteme haben können. Wissenschaftler haben Bedenken hinsichtlich der schädlichen Auswirkungen von POP geäußert, einschließlich ihrer Fähigkeit, über lange Strecken durch Luft- und Wasserströmungen zu reisen. Das Übereinkommen führte zu einem Verbot oder strengen Beschränkungen der Produktion und des Einsatzes mehrerer POP, einschließlich polychlorierter Biphenyle (PCB), Dichlordiphenyltrichlorethan (DDT) und Dioxine.

## KI-Sicherheitsgipfel {#ai-safety-summits}

### UK-KI-Sicherheitsgipfel 2023 {#2023-uk-ai-safety-summit}

Das Hauptziel von PauseAI war es, eine Regierung davon zu überzeugen, einen solchen Gipfel zu organisieren.
Nur fünf Wochen nach dem ersten PauseAI-Protest gab die britische Regierung bekannt, dass sie einen KI-Sicherheitsgipfel ausrichten würde, der am 1. und 2. November 2023 stattfand.
Der Gipfel war relativ klein (nur 100 Personen waren eingeladen) und fand in Bletchley Park statt.
Obwohl er nicht zu einem bindenden Vertrag führte, führte er zu der "Bletchley-Erklärung", die von allen 28 teilnehmenden Ländern unterzeichnet wurde.
In dieser Erklärung anerkannten die Länder die Risiken von KI (einschließlich "Probleme der Kontrolle im Zusammenhang mit der Ausrichtung auf menschliche Absichten").
Dieser Gipfel führte auch zu zwei Folgegipfeln, die für 2024 in Seoul und Paris angekündigt wurden.

### Südkorea-KI-Sicherheitsgipfel 2024 (21. und 22. Mai) {#2024-south-korea-ai-safety-summit-may-21st-22nd}

Monatelang war unklar, welchen Umfang dieser Gipfel in Seoul haben würde.
Alles, was wir wussten, war, dass es ein "virtuelles Minigipfeltreffen" sein würde.
Eine eher unambitionierte Art, mit den hoch alarmierenden Aufrufen zur Regulierung umzugehen.
Im April 2024 wurde der zweite KI-Sicherheitsgipfel offiziell von der britischen Regierung angekündigt.
Wir organisierten am 13. Mai einen Protest, um unsere Minister davon zu überzeugen, am Gipfel teilzunehmen (einige hatten nicht einmal vor, teilzunehmen) und Verhandlungen über einen Vertrag zur Pause aufzunehmen.

Der Gipfel führte zu folgenden Ergebnissen:

1. 16 Unternehmen (die meisten prominenten KI-Unternehmen) unterzeichneten die "Frontier AI Safety Commitments", was bedeutet, dass diese Unternehmen RSPs veröffentlichen werden. Bisherige freiwillige Verpflichtungen wurden ignoriert.
2. Eine neue Erklärung wurde von 27 Ländern unterzeichnet.

## KI-Sicherheitskonferenz in San Francisco im November 2024 {#2024-november-san-francisco-ai-safety-conference}

Im September überraschten uns das AISI und die US-Regierung mit der Ankündigung eines neuen Gipfels (oder "Konferenz") in San Francisco.
Am 20. und 21. November findet das erste internationale Treffen von KI-Sicherheitsinstituten in San Francisco statt, organisiert von der US-Regierung.
Am 21. und 22. November veranstaltet das britische AISI eine "Konferenz" in San Francisco.
Eines der Ziele dieser Konferenz ist es, verschiedene KI-Sicherheitsinstitute dazu zu bringen, ihre Forschungsergebnisse und Ideen zu teilen.

Man könnte argumentieren, dass einige sicherheitsbewusste hochrangige Beamte von den Entscheidungen Frankreichs enttäuscht waren und beschlossen, dass ein wahrer Sicherheitsgipfel bald benötigt wurde.
Es ist unglaublich unglücklich, dass das neue chinesische KI-Sicherheitsinstitut nicht eingeladen wurde, teilzunehmen.

## ~~2024~~ 2025 Frankreich-KI-~~Sicherheits~~-Aktionsgipfel {#2024-2025-france-ai-safety-action-summit}

Während des Bletchley-Gipfels 2023 entschied sich Frankreich, den nächsten großen Gipfel im November 2024 auszurichten.
Frankreich verschob ihn auf Februar 2025.
Er wurde auch in "KI-Aktionsgipfel" umbenannt und der wichtige Fokus auf "Sicherheit" wurde fallen gelassen.
Wir wurden informiert, dass Sicherheit nur einer von fünf Schwerpunkten des Gipfels sein wird.
Er wird von der KI-Skeptikerin Anne Bouverot geleitet, die "Alarmdiskurs" ablehnt und KI mit Taschenrechnern vergleicht und Bedenken hinsichtlich der KI-Sicherheit mit Y2K-Bedenken vergleicht und sich sicher ist, dass "KI uns nicht ersetzen, sondern uns helfen wird".
Es scheint immer unwahrscheinlicher, dass dieser Gipfel zu den internationalen Regulierungen führen wird, die wir fordern.