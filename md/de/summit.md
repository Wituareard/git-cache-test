

---
title: Warum wir AI-Sicherheitsgipfel benötigen
description: Warum wir den AI-Sicherheitsgipfel benötigen und was er erreichen sollte.
---

Künstliche Intelligenz (KI) birgt zahlreiche [Risiken](/risks) für die Menschheit, einschließlich des [Risikos des Aussterbens](/xrisk).
Die Fortschritte in der KI-Forschung beschleunigen sich in einem [atemberaubenden Tempo](/urgency), und wir sind nicht auf die Konsequenzen vorbereitet.
KI-Unternehmen sind in einem Wettlauf um die Vorherrschaft gefangen, bei dem Sicherheit nicht die höchste Priorität hat.
Wir benötigen Regierungen, die eingreifen und verhindern, dass KI ein superhumanes Level erreicht, bevor wir wissen, wie wir sie sicher machen können.
Diese [Pause](/proposal) muss auf internationaler Ebene stattfinden, da Länder in einem ähnlichen Wettlauf wie die Unternehmen gefangen sind.
Internationale Abkommen bedeuten _Verträge_, und dafür müssen Länder persönlich zusammenkommen und verhandeln.
**Der einzige Weg, eine wahre Pause zu erreichen, ist durch einen Gipfel.**

Es gab einige Beispiele für internationale Gipfel und daraus resultierende Verträge, die erfolgreich waren, um Risiken zu reduzieren:

- **Montreal-Protokoll** (1987): Das Montreal-Protokoll ist ein internationales Umweltabkommen, das den Schutz der Ozonschicht durch die schrittweise Abschaffung der Produktion und des Verbrauchs von ozonschädigenden Substanzen zum Ziel hat. Es war sehr erfolgreich bei der Reduzierung des Einsatzes von Substanzen wie Fluorchlorkohlenwasserstoffen (FCKW) und hat zum allmählichen Wiederherstellen der Ozonschicht beigetragen.
- **Stockholmer Übereinkommen über persistente organische Schadstoffe** (2001): Das Stockholmer Übereinkommen ist ein internationales Abkommen, das darauf abzielt, die menschliche Gesundheit und die Umwelt vor persistenten organischen Schadstoffen (POPs) zu schützen. Diese sind giftige Chemikalien, die in der Umwelt persistieren, in lebenden Organismen bioakkumulieren und schwerwiegende negative Auswirkungen auf die menschliche Gesundheit und Ökosysteme haben können. Wissenschaftler haben Bedenken hinsichtlich der schädlichen Auswirkungen von POPs geäußert, einschließlich ihrer Fähigkeit, über lange Strecken durch Luft- und Wasserströmungen zu reisen. Das Übereinkommen führte zu einem Verbot oder strengen Beschränkungen der Produktion und des Einsatzes mehrerer POPs, einschließlich polychlorierter Biphenyle (PCB), Dichlordiphenyltrichlorethan (DDT) und Dioxine.

## KI-Sicherheitsgipfel

### UK KI-Sicherheitsgipfel 2023

Das Hauptziel von PauseAI war es, eine Regierung davon zu überzeugen, einen solchen Gipfel zu organisieren.
Nur 5 Wochen nach dem ersten PauseAI-Protest kündigte die britische Regierung an, dass sie einen KI-Sicherheitsgipfel ausrichten würde, der am 1. und 2. November 2023 stattfand.
Der Gipfel war relativ klein (nur 100 Personen waren eingeladen) und fand in Bletchley Park statt.
Obwohl er nicht zu einem bindenden Vertrag führte, führte er zu der ["Bletchley-Erklärung"](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023), die von allen 28 teilnehmenden Ländern unterzeichnet wurde.
In dieser Erklärung anerkannten die Länder die KI-Risiken (einschließlich "Probleme der Kontrolle im Zusammenhang mit der Ausrichtung auf menschliche Absichten").
Dieser Gipfel führte auch zu zwei Folgegipfeln, die für 2024 in Seoul und Paris angekündigt wurden.

### Südkorea KI-Sicherheitsgipfel 2024 (21. und 22. Mai)

Monatelang war unklar, was der Umfang dieses Gipfels in Seoul sein würde.
Alles, was wir wussten, war, dass es ein ["virtuelles Minigipfeltreffen"](https://www.bracknellnews.co.uk/news/national/23898764.ai-safety-institute-will-make-uk-global-hub-rishi-sunak-says/) sein würde.
Eine eher unambitionierte Art, mit den hoch alarmierenden Aufrufen zur Regulierung umzugehen.
Im April 2024 wurde der zweite KI-Sicherheitsgipfel von der britischen Regierung [offiziell angekündigt](https://www.gov.uk/government/news/uk-and-republic-of-korea-to-build-on-legacy-of-bletchley-park).
Wir [organisierten einen Protest am 13. Mai](/2024-may), um unsere Minister davon zu überzeugen, am Gipfel teilzunehmen (einige hatten [nicht einmal vor, teilzunehmen](https://www.reuters.com/technology/second-global-ai-safety-summit-faces-tough-questions-lower-turnout-2024-04-29/) und Verhandlungen über einen Vertrag zur Pause aufzunehmen.

Der Gipfel führte zu folgenden Ergebnissen:

1. 16 Unternehmen (die meisten prominenten KI-Unternehmen) unterzeichneten die ["Frontier AI Safety Commitments"](https://www.gov.uk/government/news/historic-first-as-companies-spanning-north-america-asia-europe-and-middle-east-agree-safety-commitments-on-development-of-ai?utm_source=substack&utm_medium=email), was bedeutet, dass diese Unternehmen RSPs veröffentlichen werden. Frühere freiwillige Verpflichtungen [wurden ignoriert](https://www.politico.eu/article/rishi-sunak-ai-testing-tech-ai-safety-institute/).
2. Eine [neue Erklärung](https://www.gov.uk/government/publications/seoul-ministerial-statement-for-advancing-ai-safety-innovation-and-inclusivity-ai-seoul-summit-2024/seoul-ministerial-statement-for-advancing-ai-safety-innovation-and-inclusivity-ai-seoul-summit-2024) wurde von 27 Ländern unterzeichnet.

## KI-Sicherheitskonferenz in San Francisco im November 2024

Im September überraschten uns das AISI und die US-Regierung mit der Ankündigung einer neuen Konferenz in San Francisco.
Am 20. und 21. November findet das erste internationale Treffen von [KI-Sicherheitsinstituten](https://www.commerce.gov/news/press-releases/2024/09/us-secretary-commerce-raimondo-and-us-secretary-state-blinken-announce) in San Francisco statt, das von der US-Regierung organisiert wird.
Am 21. und 22. November veranstaltet das britische AISI eine ["Konferenz"](https://www.aisi.gov.uk/work/conference-on-frontier-ai-safety-frameworks) in San Francisco.
Eines der Ziele dieser Konferenz ist es, verschiedene KI-Sicherheitsinstitute dazu zu bringen, ihre Forschungsergebnisse und Ideen zu teilen.

Man könnte argumentieren, dass einige sicherheitsbewusste hochrangige Beamte von den Entscheidungen Frankreichs enttäuscht waren und beschlossen, dass ein wahrer Sicherheitsgipfel bald benötigt wurde.
Es ist unglaublich unglücklich, dass [Chinas neues KI-Sicherheitsinstitut](https://x.com/yi_zeng/status/1831133250946838740) nicht eingeladen wurde, teilzunehmen.

## ~~2024~~ 2025 Frankreich KI-~~Sicherheits~~ Aktionsgipfel

Während des Gipfels in Bletchley 2023 entschied sich Frankreich, den nächsten großen Gipfel im November 2024 auszurichten.
Frankreich verschob ihn auf Februar 2025.
Er wurde auch in "KI-_Aktions_gipfel" umbenannt und der wichtige Fokus auf "Sicherheit" wurde fallen gelassen.
Wir wurden informiert, dass Sicherheit nur einer von fünf Schwerpunkten des Gipfels sein wird.
Er wird von der KI-Skeptikerin Anne Bouverot geleitet, die [abweisend](https://legrandcontinent-eu.translate.goog/es/2023/12/08/la-ia-no-nos-sustituira-una-conversacion-con-anne-bouverot-yann-le-cun-y-alexandre-viros/?_x_tr_sl=es&_x_tr_tl=en&_x_tr_hl=en&_x_tr_pto=sc) gegenüber "Alarmdiskursen" ist und KI mit Taschenrechnern vergleicht und Bedenken hinsichtlich der KI-Sicherheit mit Y2K-Bedenken vergleicht, da sie sicher ist, dass "KI uns nicht ersetzen, sondern uns helfen wird".
Es scheint immer unwahrscheinlicher, dass dieser Gipfel zu den internationalen Regulierungen führen wird, die wir fordern.