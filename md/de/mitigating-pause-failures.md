

---
title: Ein Stopp der KI-Entwicklung könnte schiefgehen. Wie können wir die Risiken minimieren?
description: Dieser Artikel behandelt einige der Risiken eines Stopp der KI-Entwicklung und wie man sie minimieren kann.
---

Wir setzen uns für einen Stopp der Entwicklung großer, allgemeiner KI-Modelle ein.
Siehe unseren [Vorschlag](/proposal) für weitere Details.

Diese Maßnahme ist nicht ohne Risiken.
In diesem Artikel werden wir einige dieser Risiken ansprechen und diskutieren, wie man sie minimieren kann.

## Zu frühes Anhalten {#pausing-too-early}

Wenn ein KI-Stopp erfolgt, bevor die Risiken groß genug sind, könnten wir die Vorteile der KI verpassen.
Letztendlich müssen wir die Risiken mit den Kosten eines Stopp abwägen.

Unserer Meinung nach ist die Wahrscheinlichkeit, dass KI katastrophale Risiken verursacht, bereits groß genug, um einen Stopp zu rechtfertigen.
Wie Stuart Russell sagte, sollte man bei einem ungewissen Zeitplan die Aktion wählen, die optimal wäre, wenn die Zeit knapp wäre.

Je länger wir warten, desto mehr Menschen werden denken, dass ein Stopp nicht möglich ist, und desto mehr Menschen werden fantasieren und in theoretisch mögliche KI-Anwendungen investieren.
Also wird mehr Geld in Lobbyarbeit gegen Menschen wie uns investiert.

Außerdem kann es viele Jahre dauern, von Protesten und Lobbyarbeit bis hin zu Überzeugungsarbeit an Menschen mit Macht, um einen Vertrag in Kraft zu setzen. Abgesehen davon, dass selbst wenn dies nicht der Fall wäre, ein früher Stopp uns Luft verschafft, damit böse Akteure und algorithmische Durchbrüche uns nicht in den Abgrund stürzen.

## Zu kurzer Stopp / nur 6 Monate {#pausing-for-too-short--only-for-6-months}

Der Stopp, den wir vorschlagen, ist von unbestimmter Länge. Wir sollten nicht wieder aufhören, bis es einen breiten Konsens gibt, dass wir wissen, wie man Frontier-KIs sicher entwickeln kann. Technisch und demokratisch gesehen.
Unser Vorschlag ist NICHT "mindestens 6 Monate" wie der [offene Brief des Future of Life Institute](https://futureoflife.org/open-letter/pause-giant-ai-experiments/).

## Zu langer Stopp {#pausing-for-too-long}

Nick Bostrom, einer der frühen Stimmen, die Bedenken über KI äußerten, befürchtet, dass [wir uns zu sehr um KI-Risiken sorgen könnten](https://twitter.com/jachaseyoung/status/1723325057056010680), obwohl dieser Moment noch nicht gekommen ist.
Wenn die Sorge um KI weiter steigt und wir einen Stopp erreichen, könnte es zu einer Situation kommen, in der die gesamte KI-Entwicklung tabuisiert oder illegal wird.
Wenn das passiert, werden wir nie die Vorteile der KI ernten, und in der Zwischenzeit könnten wir andere existenzielle Risiken begegnen, die wir mit Hilfe der KI hätten vermeiden können.

Wir können dieses Risiko minimieren, indem wir klar darlegen, unter welchen Bedingungen die KI-Entwicklung wieder aufgenommen werden sollte.
Wie wir sagten, schlagen wir vor, dass die KI-Entwicklung wieder aufgenommen werden sollte, wenn es möglich wird, sicher KI zu bauen.
Außerdem schlagen wir nur vor, die Entwicklung von sehr spezifischen Arten von Modellen zu verbieten: die größten, allgemeinen Modelle.

## Was, wenn sichere KI-Entwicklung unbeweisbar oder unmöglich ist? {#what-if-safe-ai-development-is-unprovable-or-impossible}

Glücklicherweise gibt es andere Möglichkeiten, technologischen Fortschritt zu erzielen, der die Vorteile der KI verspricht, ohne die meisten ihrer Risiken.

Einige Menschen glauben, dass wir, um diese gefährliche Periode zu überwinden, andere Wege finden müssen, unsere Intelligenz zu steigern.
Sie können einen aktuellen Überblick über plausible Methoden dazu in [diesem Beitrag](https://www.lesswrong.com/posts/jTiSWHKAtnyA723LE/overview-of-strong-human-intelligence-amplification-methods) finden.
Einige davon und andere wurden in Nick Bostroms klassischem Buch "Superintelligence: Paths, Dangers, Strategies" analysiert.
Transparente KI-Paradigmen, Brain-Computer-Schnittstellen, ganze Gehirn-Emulationen, neuronale Verbesserungen, Wachstum in kollektiver Intelligenz, genetische Bearbeitung und Selektion und mehr.
Wie das Sprichwort sagt: "Wenn du sie nicht besiegen kannst, schließe dich ihnen an".

Viele Menschen sympathisieren mit dieser Strategie, wie der KI-Forscher Eliezer Yudkowsky[¹](https://x.com/ESYudkowsky/status/1648766287819026432)[²](https://x.com/ESYudkowsky/status/1676981883266301952) und der Mitbegründer von Ethereum, Vitalik Buterin[¹](https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#a-happy-path-merge-with-the-ais).

## Zentralisierung der KI könnte die Übernahmerisiken verschlimmern {#centralization-of-ai-might-make-takeover-risks-worse}

Wir schlagen keine Zentralisierung der KI-Entwicklung in einer einzigen Organisation vor. Das würde die KI-Entwicklung kontrollierbarer machen, aber es würde auch einen einzelnen Punkt des Versagens schaffen, den menschliche Gier und Dummheit ausnutzen könnten.
Die Entscheidung, ob ein CERN-/Apollo-/Manhattan-Projekt gut oder schlecht wäre, sollte multilateral diskutiert werden, sobald wir bereits bei einem Stopp zusammengearbeitet haben und außerhalb eines Wettrennens sind.

## Dezentralisierung wird dazu führen, dass weniger sicherheitsbewusste Akteure die Entwicklung anführen {#descentralization-will-cause-less-safety-minded-actors-to-lead-the-race}

Wenn Sie in die Geschichte von OpenAI, DeepMind und Anthropic eintauchen, werden Sie feststellen, dass alle von ihnen von Menschen gegründet wurden, die sich sehr um KI-Risiken sorgen.
In gewisser Weise sind wir froh, dass die größten KI-Unternehmen derzeit KI-Sicherheit als Teil ihrer Kultur haben.
Vielleicht gibt ein Stopp einer großen Anzahl von Unternehmen die Zeit, aufzuholen, was zu einer großen Gruppe von Unternehmen führen könnte, die weniger sicherheitsbewusst sind.

Wenn wir um einen zeitbasierten Stopp bitten würden, wäre dies eine berechtigte Sorge.
Aber was wir fordern, ist ein Stopp, bis wir beweisen können, dass KI sicher gebaut werden kann, also sollten wir nicht mit Organisationen enden, die unsichere KI nach dem Stopp bauen.

## Nationale/lokale Stopps könnten scheitern {#national-local-pauses-might-fail}

Wenn ein Land die KI-Entwicklung stoppt, werden andere Länder die KI-Entwicklung fortsetzen.
Wir könnten in einer Welt enden, in der die erste AGI von einem nicht kooperativen Akteur entwickelt wird, was wahrscheinlich ein schlechtes Ergebnis wäre.
Die Anreize für einen individuellen Stopp sind schwach, weil die Vorteile der KI-Entwicklung groß sind und die Risiken der KI-Entwicklung global sind.
Dies ist eine klassische [Gefangenendilemma](https://de.wikipedia.org/wiki/Gefangenendilemma)-Situation.

Die Lösung dafür ist, den Stopp international durch einen Vertrag zu machen, was wir vorschlagen.
Dies erfordert auch einen starken Durchsetzungsmechanismus.
Länder, die den Vertrag nicht einhalten, sollten bestraft werden.
Wirtschaftliche Sanktionen könnten ausreichen, aber militärische Interventionen könnten in extremen Fällen notwendig sein.

Ein bestimmter Akteur, von dem einige Menschen glauben, dass er nicht stoppen wird, ist China.
Wir stimmen dieser Einschätzung nicht zu und Sie können hier [mehr darüber lesen](/faq#if-we-pause-what-about-china).

## KI-Entwicklung könnte in den Untergrund gehen {#ai-development-might-go-underground}

Wenn die KI-Entwicklung (über einen bestimmten Schwellenwert hinaus) verboten wird, könnte sie in den Untergrund gehen.
Die potenziellen Vorteile sind so groß, dass ein Rogue-Akteur (Staat oder Nicht-Staat) beschließen könnte, KI in Geheimheit zu entwickeln.
Das bedeutet, dass der erste, der Superintelligenz erreicht, ein nicht kooperativer Akteur wäre, was wahrscheinlich ein schlechtes Ergebnis wäre.

Indem wir den Verkauf von GPUs verfolgen, können wir große KI-Entwicklungen erkennen.
Da die physische Infrastruktur, die zum Trainieren eines großen Modells erforderlich ist, schwer zu verbergen ist, können wir die Entwicklung von KI in großem Umfang verhindern.

Westliche Mächte (USA, Niederlande und Taiwan) kontrollieren die GPU-Lieferkette stark genug, um zu verhindern, dass nicht kooperative Staaten GPUs erhalten.
Nicht-Staatsakteure sind unwahrscheinlich, dass sie in Geheimheit ausreichende Ressourcen sammeln können, um ein AGI zu trainieren, zumindest ein Jahrzehnt nachdem AGI durch große Tech-Unternehmen möglich wird.
Außerdem würde die Tatsache, dass es kein Geschäftsanreiz mehr gibt, dazu beitragen, die Menge an Untergrund-KI-Entwicklung zu reduzieren.

## Hardware-Überhang könnte zu einem schnellen Takeoff führen {#hardware-overhang-could-cause-a-fast-takeoff}

> Wenn wir die Hardware-Forschung und -Entwicklung nicht in den Stopp einschließen, wird die Preis-Leistung von GPUs weiterhin alle 2,5 Jahre verdoppelt, wie es zwischen 2006 und 2021 der Fall war.
> Das bedeutet, dass KI-Systeme nach zehn Jahren mindestens 16-mal schneller und nach zwanzig Jahren 256-mal schneller werden, einfach aufgrund besserer Hardware.
> Wenn der Stopp aufgehoben wird, würden diese Hardware-Verbesserungen sofort für das Training von leistungsfähigeren Modellen zur Verfügung stehen - ein Hardware-Überhang.
> Dies würde zu einem schnellen und ziemlich diskontinuierlichen Anstieg der KI-Fähigkeiten führen, was möglicherweise zu einem schnellen Takeoff-Szenario und all den damit verbundenen Risiken führen könnte.

[_Von Nora Belrose_](https://bounded-regret.ghost.io/ai-pause-will-likely-backfire-by-nora/)

Dies ist ein ernstes Problem, obwohl es starke Argumente dafür gibt, dass [ein Überhang unwahrscheinlich ist](https://blog.aiimpacts.org/p/are-there-examples-of-overhang-for).

PauseAI unterstützt einen Stopp von relevanten Rechenleistungsverbesserungen.
Außerdem, wie wir sagten, sollte die "Play"-Taste nicht gedrückt werden, wenn wir immer noch nicht wissen, wie man sichere KI baut.
Und das schließt das Training und die Bereitstellung von Modellen mit fortschrittlicher Hardware ein.

## KI-Entwicklung ist notwendig, um zu lernen, wie man KI sicher macht {#ai-development-is-necessary-for-learning-how-to-make-ais-safe}

Die meisten Menschen glauben, dass ein gewisses Maß an prosaischer/inkrementeller Ausrichtung notwendig ist, also wenn ein vollständiger Stopp ohne Ausnahmen implementiert wird, würde nicht genug Fortschritt bei der Ausrichtung erzielt werden und letztendlich würden Akteure, die sich nicht um Sicherheit und den Stopp kümmern, eine nicht ausgerichtete leistungsfähige KI entwickeln.

Das ist ein Grund, warum wir vorschlagen, bestimmte Trainingsläufe zu genehmigen. Das würde es uns ermöglichen, von größeren Systemen zu lernen, wenn wir ihre Sicherheit gewährleisten können.
Allerdings haben wir in dem schlimmsten Fall, in dem wir ihre Sicherheit nicht gewährleisten können und nicht genug Fortschritte bei der Ausrichtung erzielen, immer noch die Option, unsere Intelligenz durch andere Technologien zu steigern.

## Algorithmische oder Laufzeitverbesserungen könnten kleinere Modelle gefährlich machen {#algorithmic-or-runtime-improvements-may-make-smaller-models-dangerous-too}

Wir haben gesehen, dass Änderungen in den Trainingsdaten, Trainingsalgorithmen oder Laufzeitanwendungen zu großen Verbesserungen der Modellleistung führen können.
Deswegen konzentrieren wir uns nicht nur auf die Modellgröße.
Wir [schlagen vor](/proposal), die Entwicklung von großen, allgemeinen KI-Modellen zu stoppen, die entweder 1) größer als 10^12 Parameter sind, 2) mehr als 10^25 FLOPs für das Training verwenden oder 3) Fähigkeiten haben, die GPT-4 überschreiten.
Diese dritte Bedingung wird hinzugefügt, um auch kleinere Modelle einzuschließen, die gefährlich sein können.
Die Durchsetzung einer Obergrenze für Fähigkeiten ist schwierig, da es schwer ist, die Fähigkeiten eines Modells vorherzusagen, bevor es trainiert wird.

Da die Einsätze so hoch sind, sollten wir vorsichtig sein, also unterstützen wir auch einen Stopp von relevanten algorithmischen und Laufzeitverbesserungen.
Allerdings wird die Durchsetzung hiervon schwieriger sein als die Durchsetzung von Rechenleistungsregulierungen, da Hardware leichter zu verfolgen ist als Software.

## Wenn wir nur allgemeine KI-Modelle verbieten, könnten wir immer noch AGI durch schmale Modelle erhalten {#if-we-only-ban-general-ai-models-we-might-still-get-agi-through-narrow-models}

Wir möchten gefährliche Modelle einschränken, die [gefährliche Fähigkeiten](/dangerous-capabilities) wie die Manipulation von Menschen, strategisches Planen und das Schreiben von Code haben.
Wir möchten keine sehr schmalen KI-Modelle einschränken, wie Bildklassifizierer in selbstfahrenden Autos oder medizinischer Diagnose.
Glücklicherweise fallen fast alle diese schmalen Modelle außerhalb unserer [vorgeschlagenen](/proposal) Einschränkungen, da diese Modelle tendenziell relativ klein sind.

Ein ausreichend leistungsfähiges schmales Modell (trainiert auf realen Daten) könnte wahrscheinlich zu gefährlichen Fähigkeiten verallgemeinern.
Ein sehr leistungsfähiges Bildgenerator-Modell könnte beispielsweise Bilder von funktionsfähigem Code erstellen oder ein sehr leistungsfähiges Video-Modell könnte einen Film über eine KI erstellen, die eine erfolgreiche Übernahme plant.
Schmale Modelle werden oft besser in ihrer schmalen Aufgabe, indem sie verallgemeinern.
In gewissem Umfang ist dies, was LLMs wie ChatGPT so erfolgreich macht: Sie werden nur trainiert, um "das nächste Wort vorherzusagen", aber um wirklich gut darin zu sein, müssen sie viel über die Welt lernen.

Daher haben wir in unserem Vorschlag "schmal" oder "allgemein" KI nicht definiert, sondern stattdessen drei Bedingungen verwendet, die sich auf Modellgröße, Rechenleistung und Fähigkeiten beziehen.

## Wenn ein Stopp implementiert wird, sollten wir einen politischen Kompromiss erwarten {#if-a-pause-is-implemented-we-should-expect-a-political-compromise}

Wir haben einen [spezifischen Vorschlag](/proposal), den wir für optimal halten.
Allerdings sollten wir nicht erwarten, dass unser Vorschlag genau so implementiert wird, wie wir es wollen.
Politik ist chaotisch und unvorhersehbar, also sollten wir erwarten, dass unsere Lobbyarbeit eher vage richtungsweisende Effekte hat als präzise Effekte.
Wenn wir eine Form eines Stopp erhalten, aber es ist nicht genau das, was wir wollen, könnte dies schlimmer sein als gar keinen Stopp zu haben.
Beispielsweise:

- Ein nationaler Stopp, der es potenziell schlimmeren Akteuren ermöglichen würde, als erste AGI zu erreichen
- Ein internationaler Stopp, der nicht ordnungsgemäß durchgesetzt wird, was zu einem ähnlichen Ergebnis führen würde

Wir können dies minimieren, indem wir in unseren Kommunikationen konsequent und klar sind, was wir wollen.

## Zu spätes Anhalten {#pausing-too-late}

Dies ist das offensichtlichste und wahrscheinlichste Risiko eines Scheiterns: Wenn wir zu spät anhalten, werden wir wahrscheinlich katastrophale Risiken begegnen.
Und das könnte bald passieren, wie wir auf unserer [Dringlichkeitsseite](/urgency) erklären.

Deswegen brauchen wir Ihre Hilfe, um jetzt für einen [Stopp](/action) zu sorgen.