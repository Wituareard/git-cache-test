

---
title: Ein Stillstand in der KI-Entwicklung könnte schiefgehen. Wie können wir die Risiken minimieren?
description: Dieser Artikel behandelt einige der Risiken eines Stillstands in der KI-Entwicklung und wie man sie minimieren kann.
---

Wir setzen uns für einen Stillstand in der Entwicklung großer, allgemeiner KI-Modelle ein.
Siehe unseren [Vorschlag](/proposal) für weitere Details.

Diese Maßnahme ist nicht ohne Risiken.
In diesem Artikel werden wir einige dieser Risiken ansprechen und wie man sie minimieren kann.

## Zu frühes Anhalten

Wenn ein KI-Stillstand eintritt, bevor die Risiken groß genug sind, könnten wir die Vorteile von KI verpassen.
Letztendlich müssen wir die Risiken mit den Kosten eines Stillstands abwägen.

Unserer Meinung nach ist die Wahrscheinlichkeit, dass KI katastrophale Risiken [bald](/urgency) verursacht, bereits groß genug, um einen Stillstand in diesem Moment zu rechtfertigen.
Wie Stuart Russell sagte, sollte man bei einem ungewissen Zeitlimit die Aktion wählen, die bei dem kürzesten Zeitlimit optimal wäre.

Je länger wir warten, desto mehr Menschen werden denken, dass ein Stillstand nicht möglich ist, und desto mehr Menschen werden fantasieren und in theoretisch mögliche KI-Anwendungen investieren.
Also wird mehr Geld in Lobbyarbeit gegen Menschen wie uns investiert.

Außerdem kann es viele Jahre dauern, von Protesten und Lobbyarbeit bis hin zu Überzeugung der Menschen an der Macht, um einen Vertrag in Kraft zu setzen. Nicht zu erwähnen, dass selbst wenn es nicht der Fall wäre, ein früher Stillstand uns Luft gibt, damit schlechte Akteure und algorithmische Durchbrüche uns nicht in den Abgrund stürzen.

## Zu kurzer Stillstand / nur 6 Monate

Der Stillstand, den wir vorschlagen, ist von unbestimmter Länge. Wir sollten nicht wieder aufhören, bis es einen ausreichenden Konsens gibt, dass wir wissen, wie man sichere KI entwickeln kann, egal wie leistungsfähig sie ist, und dass wir die Systeme haben, um es sorgfältig und demokratisch zu tun.
Es ist NICHT wie der [sechsmonatige Stillstand, der vom Future of Life Institute gefordert wurde](https://futureoflife.org/open-letter/pause-giant-ai-experiments/).

## Zu langer Stillstand

Nick Bostrom, einer der ersten KI-Alarmisten, sorgt sich, dass [irgendwann zu viel Angst vor KI-Risiken](https://twitter.com/jachaseyoung/status/1723325057056010680) besteht, obwohl dieser Moment noch nicht gekommen ist.
Wenn die Sorge um KI weiter steigt und wir einen Stillstand erreichen, könnte es zu einer Situation kommen, in der die gesamte KI-Entwicklung tabuisiert oder illegal wird.
Wenn das passiert, werden wir nie die Vorteile von KI ernten, und in der Zwischenzeit könnten wir andere existenzielle Risiken begegnen, die wir mit Hilfe von KI hätten vermeiden können.

Wir können dieses Risiko minimieren, indem wir klar darlegen, unter welchen Bedingungen die KI-Entwicklung wieder aufgenommen werden sollte.
Wie wir sagten, schlagen wir vor, dass die KI-Entwicklung wieder aufgenommen werden sollte, wenn es möglich wird, sichere KI zu bauen.
Außerdem schlagen wir nur vor, die Entwicklung von sehr spezifischen Arten von Modellen zu verbieten: die größten, allgemeinen Modelle.
In der Zwischenzeit gibt es andere Wege, um mehr Intelligenz zu erreichen: transparentere KI-Paradigmen, Hirn-Computer-Schnittstellen, ganze Hirn-Emulationen, neuronale Verbesserungen, Wachstum in kollektiver Intelligenz, genetische Bearbeitung und Selektion und vielleicht mehr.
Diese Wege zu einer größeren Intelligenz könnten uns die Vorteile bringen, die AGI verspricht, ohne so viele ihrer Risiken.

## Zentralisierung der KI könnte die Übernahmerisiken verschlimmern

Wir schlagen keine Zentralisierung der KI-Entwicklung in einer einzigen Organisation vor. Das würde die KI-Entwicklung kontrollierbarer machen, aber es würde auch einen einzelnen Punkt des Versagens schaffen, den menschliche Gier und Dummheit ausnutzen könnten.
Die Entscheidung, ob ein CERN-/Apollo-/Manhattan-Projekt gut oder schlecht wäre, sollte multilateral diskutiert werden, sobald wir bereits bei einem Stillstand zusammengearbeitet haben und außerhalb eines Wettrennens sind.

## Dezentralisierung wird dazu führen, dass weniger sicherheitsbewusste Akteure die Entwicklung anführen

Wenn Sie in die Geschichte von OpenAI, DeepMind und Anthropic eintauchen, werden Sie feststellen, dass alle von ihnen von Menschen gegründet wurden, die sich sehr um KI-Risiken sorgen.
In gewisser Weise sind wir froh, dass die größten KI-Unternehmen derzeit KI-Sicherheit als Teil ihrer Kultur haben.
Vielleicht gibt ein Stillstand einer großen Anzahl von Unternehmen die Zeit, aufzuholen, was zu einer großen Gruppe von Unternehmen führen könnte, die weniger sicherheitsbewusst sind.

Wenn wir einen zeitbasierten Stillstand fordern würden, wäre dies eine berechtigte Sorge.
Aber was wir fordern, ist ein Stillstand, bis wir beweisen können, dass KI sicher gebaut werden kann, also sollten wir nicht mit Organisationen enden, die unsichere KI nach dem Stillstand bauen.

## Nationale/lokale Stillstände könnten scheitern

Wenn ein Land die KI-Entwicklung anhält, werden andere Länder die KI-Entwicklung fortsetzen.
Wir könnten in einer Welt enden, in der die erste AGI von einem nicht kooperativen Akteur entwickelt wird, was wahrscheinlich ein schlechtes Ergebnis ist.
Die Anreize für einen individuellen Stillstand sind schwach, weil die Vorteile der KI-Entwicklung groß sind und die Risiken der KI-Entwicklung global sind.
Dies ist eine klassische [Gefangenendilemma](https://de.wikipedia.org/wiki/Gefangenendilemma)-Situation.

Die Lösung dafür ist, den Stillstand international durch einen Vertrag zu machen, was wir vorschlagen.
Dies erfordert auch einen starken Durchsetzungsmechanismus.
Länder, die den Vertrag nicht einhalten, sollten bestraft werden.
Wirtschaftliche Sanktionen könnten ausreichen, aber militärische Interventionen könnten in extremen Fällen notwendig sein.

Ein Akteur, von dem einige Menschen glauben, dass er nicht anhält, ist China.
Wir stimmen dieser Einschätzung nicht zu und Sie können hier [mehr darüber lesen](/faq#if-we-pause-what-about-china).

## KI-Entwicklung könnte in den Untergrund gehen

Wenn die KI-Entwicklung (über einen bestimmten Schwellenwert hinaus) verboten wird, könnte sie in den Untergrund gehen.
Die potenziellen Vorteile sind so groß, dass ein Rogue-Akteur (Staat) beschließen könnte, KI in Geheimheit zu entwickeln.
Das bedeutet, dass der erste, der Superintelligenz erreicht, ein nicht kooperativer Akteur wäre, was wahrscheinlich ein schlechtes Ergebnis ist.

Indem wir GPU-Verkäufe verfolgen, können wir große KI-Entwicklungen erkennen.
Da die Grenzmodell-GPU-Cluster immense Energiemengen und spezielle Gebäude erfordern, ist die physische Infrastruktur, die zum Trainieren eines großen Modells erforderlich ist, schwer zu verbergen.

Westliche Mächte (USA, Niederlande und Taiwan) kontrollieren die GPU-Lieferkette stark genug, um zu verhindern, dass nicht kooperative Staaten GPUs erhalten.
Nicht-Staatsakteure sind unwahrscheinlich, dass sie ausreichende Ressourcen in Geheimheit sammeln können, um ein AGI für mindestens ein Jahrzehnt nachdem AGI von großen Technologieunternehmen möglich wird, zu trainieren.
Außerdem würde die Tatsache, dass es kein Geschäftsanreiz mehr gibt, dazu beitragen, die Menge an Untergrund-KI-Entwicklung zu reduzieren.

## Hardware-Überhang könnte zu einem schnellen Takeoff führen

> Wenn wir die Hardware-Forschung und -Entwicklung nicht in den Stillstand einbeziehen, wird die Preis-Leistung von GPUs weiterhin alle 2,5 Jahre verdoppelt, wie es zwischen 2006 und 2021 der Fall war.
> Das bedeutet, dass KI-Systeme nach zehn Jahren mindestens 16-mal schneller und nach zwanzig Jahren 256-mal schneller werden, einfach aufgrund besserer Hardware.
> Wenn der Stillstand aufgehoben wird, würden diese Hardware-Verbesserungen sofort für das Training leistungsfähigerer Modelle zu geringeren Kosten verfügbar werden - ein Hardware-Überhang.
> Dies würde zu einem schnellen und ziemlich diskontinuierlichen Anstieg der KI-Fähigkeiten führen, was möglicherweise zu einem schnellen Takeoff-Szenario und all den damit verbundenen Risiken führen könnte.

[_Von Nora Belrose_](https://bounded-regret.ghost.io/ai-pause-will-likely-backfire-by-nora/)

Dies ist ein ernstes Problem, obwohl es starke Argumente dafür gibt, dass [ein Überhang unwahrscheinlich ist](https://blog.aiimpacts.org/p/are-there-examples-of-overhang-for).

PauseAI unterstützt einen Stillstand bei relevanten Rechenverbesserungen.
Außerdem, wie wir sagten, sollte die "Play"-Taste nicht gedrückt werden, wenn wir immer noch nicht wissen, wie man sichere KI baut.
Und das schließt das Training und die Bereitstellung von Modellen mit fortschrittlicherer Hardware ein.

## KI-Entwicklung ist notwendig, um zu lernen, wie man KI sicher macht

Die meisten Menschen glauben, dass ein gewisses Maß an prosaischer/inkrementeller Ausrichtung notwendig ist, also wenn ein vollständiger Stillstand ohne Ausnahmen implementiert wird, würde nicht genug Fortschritt bei der Ausrichtung gemacht werden und schließlich würden Akteure, die sich nicht um Sicherheit und Stillstand kümmern, eine nicht ausgerichtete leistungsfähige KI entwickeln.

Das ist ein Grund, warum wir vorschlagen, bestimmte Trainingsläufe zu genehmigen. Das würde uns ermöglichen, von größeren Systemen zu lernen, wenn wir ihre Sicherheit gewährleisten können.
Allerdings haben wir in dem schlimmsten Fall, in dem wir ihre Sicherheit nicht gewährleisten können und nicht genug in der Ausrichtung fortschreiten, immer noch die Option, unsere Intelligenz durch andere Technologien zu steigern.

<!-- 
## Politisches Kapital ist begrenzt, Stillstand könnte scheitern

KI-Sicherheitsleute werden weniger ernst genommen

## Algorithmische oder Laufzeitverbesserungen können kleinere Modelle gefährlich machen

Wir haben gesehen, dass Änderungen in Trainingsdaten, Trainingsalgorithmen oder Laufzeitnutzung zu großen Verbesserungen der Modellleistung führen können.
Deswegen konzentrieren wir uns nicht nur auf die Modellgröße.
Wir [schlagen vor](/proposal), die Entwicklung großer, allgemeiner KI-Modelle anzuhalten, die entweder 1) größer als 10^12 Parameter sind, 2) mehr als 10^25 FLOPs für das Training verwenden oder 3) Fähigkeiten haben, die GPT-4 überschreiten.
Diese dritte Bedingung wird hinzugefügt, um auch kleinere Modelle einzubeziehen, die gefährlich sein können.
Die Durchsetzung einer Obergrenze für Fähigkeiten ist schwierig, da es schwer ist, die Fähigkeiten eines Modells vorherzusagen, bevor es trainiert wird.

Da die Einsätze so hoch sind, sollten wir vorsichtig sein, also unterstützen wir auch einen Stillstand bei relevanten algorithmischen und Laufzeitverbesserungen.
Allerdings wird die Durchsetzung schwieriger sein als die Durchsetzung von Rechenregulierungen, weil Hardware leichter zu verfolgen ist als Software.

## Wenn wir nur allgemeine KI-Modelle verbieten, könnten wir immer noch AGI durch schmale Modelle erhalten

Wir möchten gefährliche Modelle einschränken, die [gefährliche Fähigkeiten](/dangerous-capabilities) wie die Manipulation von Menschen, strategisches Planen und das Schreiben von Code haben.
Wir möchten keine sehr schmalen KI-Modelle einschränken, wie Bildklassifizierer, die in selbstfahrenden Autos oder medizinischen Diagnosen verwendet werden.
Glücklicherweise fallen fast alle diese schmalen Modelle außerhalb unserer [vorgeschlagenen](/proposal) Einschränkungen, weil diese Modelle tendenziell relativ klein sind.

Ein ausreichend leistungsfähiges schmales Modell (das auf realen Daten trainiert wurde) könnte wahrscheinlich auf gefährliche Fähigkeiten verallgemeinern.
Ein sehr leistungsfähiges Bildgenerator-Modell könnte beispielsweise Bilder von funktionsfähigem Code erstellen oder ein sehr leistungsfähiges Video-Modell könnte einen Film über eine KI erstellen, die einen erfolgreichen Übernahmeplan plant.
Schmale Modelle werden oft besser in ihrer schmalen Aufgabe, indem sie verallgemeinern.
In gewissem Maße ist dies, was LLMs wie ChatGPT so erfolgreich macht: Sie werden nur trainiert, um "das nächste Wort vorherzusagen", aber um wirklich gut darin zu sein, müssen sie viel über die Welt lernen.

Daher haben wir in unserem Vorschlag "schmal" oder "allgemein" KI nicht definiert, sondern stattdessen drei Bedingungen verwendet, die sich auf Modellgröße, Rechenleistung und Fähigkeiten beziehen.

## Wenn ein Stillstand implementiert wird, sollten wir einen politischen Kompromiss erwarten

Wir haben einen [spezifischen Vorschlag](/proposal), den wir für optimal halten.
Allerdings sollten wir nicht erwarten, dass unser Vorschlag genau so implementiert wird, wie wir es wollen.
Politik ist chaotisch und unvorhersehbar, also sollten wir erwarten, dass unsere Lobbyarbeit vage richtungsweisende Auswirkungen hat, anstatt präzise Auswirkungen.
Wenn wir eine Form eines Stillstands erhalten, aber es ist nicht genau das, was wir wollen, könnte dies schlimmer sein als gar keinen Stillstand zu haben.
Beispielsweise:

- Ein nationaler Stillstand, der potenziell schlechtere Akteure dazu bringen könnte, als erste AGI zu erreichen
- Ein internationaler Stillstand, der nicht richtig durchgesetzt wird, was zu einem ähnlichen Ergebnis führen könnte

Wir können dies minimieren, indem wir in unseren Kommunikationen konsistent und klar sind, was wir wollen.

## Zu spätes Anhalten

Dies ist das offensichtlichste und wahrscheinlichste Risiko eines Scheiterns: Wenn wir zu spät anhalten, werden wir wahrscheinlich katastrophale Risiken begegnen.
Und das könnte bald passieren, wie wir auf unserer [Dringlichkeitsseite](/urgency) erklären.

Deswegen brauchen wir Ihre Hilfe, um jetzt [einen Stillstand zu fordern](/action).