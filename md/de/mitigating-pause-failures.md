---
title: Ein Stillstand in der KI-Entwicklung könnte schiefgehen. Wie können wir die Risiken minimieren?
description: Dieser Artikel behandelt einige der Risiken eines Stillstands in der KI-Entwicklung und wie man sie minimieren kann.
---

Wir setzen uns für einen Stillstand in der Entwicklung großer, allgemeiner KI-Modelle ein.
Siehe unseren [Vorschlag](/proposal) für weitere Details.

Diese Maßnahme ist nicht ohne Risiken.
In diesem Artikel werden wir einige dieser Risiken ansprechen und wie man sie minimieren kann.

## Zu früher Stillstand

Wenn ein KI-Stillstand eintritt, bevor die Risiken groß genug sind, könnten wir die Vorteile von KI verpassen.
Letztendlich müssen wir die Risiken mit den Kosten eines Stillstands abwägen.

Unserer Meinung nach ist die Wahrscheinlichkeit, dass KI katastrophale Risiken [bald](/urgency) verursacht, bereits groß genug, um einen Stillstand in diesem Moment zu rechtfertigen.
Wie Stuart Russell sagte, sollte man bei einem ungewissen Zeitlimit die Aktion wählen, die bei dem kürzesten Zeitlimit optimal wäre.

Je länger wir warten, desto mehr Menschen werden denken, dass ein Stillstand nicht möglich ist, und desto mehr Menschen werden fantasieren und in theoretisch mögliche KI-Anwendungen investieren.
Also wird mehr Geld in Lobbyarbeit gegen Menschen wie uns investiert.

Außerdem kann es viele Jahre dauern, von Protesten und Lobbyarbeit bis hin zu Überzeugung von Menschen an der Macht, um einen Vertrag in Kraft zu setzen. Nicht zu erwähnen, dass selbst wenn es nicht der Fall wäre, ein früher Stillstand uns Luft gibt, damit schlechte Akteure und algorithmische Durchbrüche uns nicht in den Abgrund stürzen.

## Zu kurzer Stillstand / nur 6 Monate

Der Stillstand, den wir vorschlagen, ist von unbestimmter Länge. Wir sollten nicht wieder aufhören, bis es einen ausreichenden Konsens gibt, dass wir wissen, wie man ausgerichtete KIs entwickelt, egal wie leistungsfähig sie sind, und dass wir die Systeme haben, um es sorgfältig und demokratisch zu tun.
Es ist NICHT wie der [sechsmonatige Stillstand, der vom Future of Life Institute gefordert wird](https://futureoflife.org/open-letter/pause-giant-ai-experiments/).

## Zu langer Stillstand

Nick Bostrom, einer der ersten KI-Alarmisten, sorgt sich, dass [irgendwann zu viel Angst vor KI-Risiken](https://twitter.com/jachaseyoung/status/1723325057056010680) besteht, obwohl dieser Moment noch nicht gekommen ist.
Wenn die Sorge um KI weiter steigt und wir einen Stillstand erreichen, könnte es zu einer Situation kommen, in der die gesamte KI-Entwicklung tabuisiert oder illegal wird.
Wenn das passiert, werden wir nie die Vorteile von KI ernten, und in der Zwischenzeit könnten wir andere existenzielle Risiken begegnen, die wir mit Hilfe von KI hätten vermeiden können.

Wir können dieses Risiko minimieren, indem wir klar darlegen, unter welchen Bedingungen die KI-Entwicklung wieder aufgenommen werden sollte.
Wie wir sagten, schlagen wir vor, dass die KI-Entwicklung wieder aufgenommen werden sollte, wenn es möglich wird, sichere KI zu bauen.
Außerdem schlagen wir nur vor, die Entwicklung von sehr spezifischen Arten von Modellen zu verbieten: die größten, allgemeinen Modelle.
In der Zwischenzeit gibt es andere Wege, um mehr Intelligenz zu erlangen: transparentere KI-Paradigmen, Hirn-Computer-Schnittstellen, ganze Hirn-Emulationen, neuronale Verbesserungen, Wachstum in kollektiver Intelligenz, genetische Bearbeitung und Selektion und vielleicht mehr.
Diese Wege zu einer größeren Intelligenz könnten uns die Vorteile bringen, die AGI verspricht, ohne so viele ihrer Risiken.

## Zentralisierung von KI könnte die Übernahmerisiken verschlimmern

Wir schlagen keine Zentralisierung der KI-Entwicklung in einer einzigen Organisation vor. Das würde die KI-Entwicklung kontrollierbarer machen, aber es würde auch einen einzelnen Punkt des Versagens schaffen, den menschliche Gier und Dummheit ausnutzen könnten.
Die Entscheidung, ob ein CERN-/Apollo-/Manhattan-Projekt gut oder schlecht wäre, sollte multilateral diskutiert werden, sobald wir bereits bei einem Stillstand zusammengearbeitet haben und außerhalb eines Wettbewerbs sind.

## Dezentralisierung wird dazu führen, dass weniger sicherheitsbewusste Akteure die Entwicklung anführen

Wenn man in die Geschichte von OpenAI, DeepMind und Anthropic eintaucht, wird man feststellen, dass alle von ihnen von Menschen gegründet wurden, die sich sehr um KI-Risiken sorgen.
In gewisser Weise sind wir froh, dass die größten KI-Unternehmen derzeit KI-Sicherheit als Teil ihrer Kultur haben.
Vielleicht gibt ein Stillstand einer großen Zahl von Unternehmen die Zeit, aufzuholen, was zu einer großen Gruppe von Unternehmen führen könnte, die weniger sicherheitsbewusst sind.

Wenn wir einen zeitbasierten Stillstand fordern würden, wäre dies eine berechtigte Sorge.
Aber was wir fordern, ist ein Stillstand, bis wir beweisen können, dass KI sicher gebaut werden kann, also sollten wir nicht mit Organisationen enden, die unsichere KI nach dem Stillstand bauen.

## Nationale/lokale Stillstände könnten scheitern

Wenn ein Land die KI-Entwicklung stillsetzt, werden andere Länder weiterhin KI entwickeln.
Wir könnten in einer Welt enden, in der die erste AGI von einem nicht kooperativen Akteur entwickelt wird, was wahrscheinlich ein schlechtes Ergebnis ist.
Die Anreize für einen individuellen Stillstand sind schwach, weil die Vorteile der KI-Entwicklung groß sind groß sind groß sind groß sind groß sind groß sind groß sind groß sind groß sind groß sind groß sind groß sind groß sind groß sind groß sind groß sind groß sind groß sind groß sind groß sind groß sind groß