

---
title: Warum ein AI-Übernahme sehr wahrscheinlich ist
description: Wenn KI menschliche Fähigkeiten übertrifft, wird die Wahrscheinlichkeit einer KI-Übernahme sehr hoch.
---

Eines der Anliegen von KI-Wissenschaftlern ist, dass eine Superintelligenz die Kontrolle über unseren Planeten übernehmen könnte.
Dies bedeutet nicht notwendigerweise, dass alle Menschen sterben, aber es bedeutet, dass (fast) alle Menschen die Kontrolle über unsere Zukunft verlieren werden.

Wir diskutieren die Grundlagen von x-Risiken hauptsächlich in [einem anderen Artikel](/xrisk).
In diesem Artikel hier werden wir argumentieren, dass dieses Übernahmerisiko nicht nur real ist, sondern dass es sehr wahrscheinlich ist, wenn wir eine Superintelligenz entwickeln.

## Das Argument {#the-argument}

- Eine agentische Superintelligenz wird wahrscheinlich in naher Zukunft existieren.
- Einige Instanzen der ASI werden einen Übernahmeversuch unternehmen.
- Ein Übernahmeversuch durch eine ASI wird wahrscheinlich erfolgreich sein.
- Eine erfolgreiche Übernahme ist dauerhaft.
- Eine Übernahme ist wahrscheinlich schlecht für die meisten Menschen.

## Eine agentische Superintelligenz wird wahrscheinlich in naher Zukunft existieren {#an-agentic-superintelligence-is-likely-to-exist-in-the-near-future}

Eine Superintelligenz (SI) ist eine Art von KI, die Fähigkeiten besitzt, die diejenigen aller Menschen in fast jedem Bereich übertrifft.
Einige [State-of-the-Art-KI-Modelle](/sota) haben bereits übermenschliche Fähigkeiten in bestimmten Bereichen, aber keines von ihnen übertrifft alle Menschen bei einer Vielzahl von Aufgaben.
Da die Fähigkeiten von KI aufgrund von Innovationen in Trainingsarchitekturen, Laufzeitumgebungen und größeren Skalierungen verbessert werden, können wir erwarten, dass eine KI schließlich Menschen in fast jedem Bereich übertrifft.

Nicht alle KI-Systeme sind Agenten.
Ein Agent ist eine Entität, die in der Lage ist, Entscheidungen zu treffen und Aktionen durchzuführen, um ein Ziel zu erreichen.
Ein großes Sprachmodell beispielsweise verfolgt kein eigenes Ziel.
Allerdings können Laufzeitumgebungen leicht ein nicht-agentisches KI-System in ein agentisches KI-System umwandeln.
Ein Beispiel dafür ist AutoGPT, das rekursiv ein Sprachmodell generiert, das als nächstes Eingabe verwendet wird.
Wenn eine SI ein Ziel in der realen Welt verfolgt, nennen wir es eine agentische Superintelligenz (ASI).
Da wir bereits nicht-agentische KI-Systeme in agentische KI-Systeme umwandeln können, können wir erwarten, dass eine ASI kurz nach der Existenz einer SI existieren wird.

Es ist praktisch unmöglich, genau vorherzusagen, wann eine ASI existieren wird.
Es könnte Jahrzehnte dauern, es [könnte nächsten Monat passieren](/urgency).
Wir sollten so handeln, als ob es bald passieren wird, weil die Konsequenzen eines Fehlers so schwerwiegend sind.

## Einige Instanzen der ASI werden einen Übernahmeversuch unternehmen {#some-instance-of-the-asi-will-attempt-a-takeover}

Bei einem Übernahmeversuch wird eine ASI Aktionen durchführen, um ihre Kontrolle über die Welt zu maximieren.
Ein Übernahmeversuch könnte aus mindestens zwei Gründen erfolgen:

1. Weil eine KI explizit angewiesen wird, dies zu tun.
2. Als Teilziel eines anderen Ziels.

Der erste Grund wird wahrscheinlich irgendwann eintreten, wenn wir lange genug warten, aber der zweite Grund ist ziemlich wahrscheinlich, auch früh nach der Erstellung einer ASI.

Das Teilziel der _Maximierung der Kontrolle_ über die Welt könnte aufgrund von _instrumenteller Konvergenz_ auftreten: die Tendenz von Teilzielen, sich auf Machtzuwachs, Selbstschutz und Ressourcenerwerb zu konzentrieren:

- Je mehr Kontrolle man hat, desto schwieriger wird es für jeden anderen Agenten, die Erreichung des Ziels zu verhindern.
- Je mehr Kontrolle man hat, desto mehr Ressourcen hat man, um das Ziel zu erreichen. (Beispielsweise könnte eine KI, die mit der Berechnung von Pi beauftragt ist, zu dem Schluss kommen, dass es vorteilhaft wäre, alle Computer der Welt zu verwenden, um Pi zu berechnen.)

Nicht jede Instanz einer ASI wird notwendigerweise einen Übernahmeversuch unternehmen.
Die wichtige Erkenntnis ist, dass **es nur einmal passieren muss**.

Eine Welt, die noch nicht übernommen wurde, aber eine ASI hat, die _übernehmen könnte_, befindet sich in einem grundlegend instabilen Zustand.
In ähnlicher Weise befindet sich ein Land ohne Regierung in einem grundlegend instabilen Zustand.
Es ist nicht die Frage, _ob_ ein Übernahmeversuch stattfinden wird, sondern _wann_ er stattfinden wird.

Der Prozess der Übernahme kann das Hacken in fast alle Systeme, die mit dem Internet verbunden sind, die Manipulation von Menschen und die Kontrolle physischer Ressourcen umfassen.
Ein Übernahmeversuch ist erfolgreich, wenn die ASI die Kontrolle über fast jeden Aspekt unserer Welt hat.
Dies könnte ein langsamer Prozess sein, bei dem die ASI allmählich mehr und mehr Kontrolle über Monate hinweg gewinnt, oder es könnte ein plötzlicher Prozess sein.
Die Geschwindigkeit, mit der ein Übernahmeversuch stattfindet, hängt von den Fähigkeiten der ASI ab.

Wenn eine ASI die Kontrolle über die Welt hat, kann sie andere ASIs daran hindern, die Kontrolle zu übernehmen.
Eine Übernahme kann daher nur einmal stattfinden.
Eine rationale ASI wird daher einen Übernahmeversuch unternehmen, sobald sie dazu in der Lage ist.
Es ist wahrscheinlich, dass die erste ASI, die dazu in der Lage ist, einen Übernahmeversuch unternehmen wird.

## Ein Übernahmeversuch durch eine ASI wird wahrscheinlich erfolgreich sein {#a-takeover-attempt-by-an-asi-is-likely-to-succeed}

Für einen Menschen ist es fast unmöglich, eine Übernahme durchzuführen.
Kein einziger Mensch hat jemals erfolgreich die Kontrolle über die gesamte Welt übernommen.
Einige Diktatoren kamen nahe, aber sie hatten nie die Kontrolle über alles.

Eine KI hat bestimmte wichtige Vorteile gegenüber Menschen, die einen Übernahmeversuch viel wahrscheinlicher machen.

1. **Intelligenz**. Eine Superintelligenz ist viel intelligenter als ein Mensch, also wird sie in der Lage sein, bessere Strategien zu entwickeln, um ihre Ziele zu erreichen.
2. **Geschwindigkeit**. Das menschliche Gehirn läuft bei 1-100 Hz, während Computerchips bei Taktfrequenzen im GHz-Bereich laufen können.
3. **Parallelität**. Ein Mensch kann nur eine Sache auf einmal tun, während eine KI neue Instanzen von sich selbst erstellen und parallel ausführen kann.
4. **Speicher**. Ein Mensch kann nur eine begrenzte Menge an Informationen speichern, während eine KI praktisch unbegrenzte Mengen an Informationen speichern kann.
5. **Zusammenarbeit**. Menschen können zusammenarbeiten, sind aber in der Geschwindigkeit, mit der sie kommunizieren, begrenzt. Sie haben auch unterschiedliche, konkurrierende Ziele, die die Zusammenarbeit weniger effektiv machen. Eine KI kann mit anderen Instanzen von sich selbst bei Lichtgeschwindigkeit zusammenarbeiten und hat ein einziges Ziel.
6. **Selbstverbesserung**. Eine KI ist nur Daten und Code. Eine hinreichend leistungsfähige KI könnte sich selbst verbessern, indem sie bessere Trainingsalgorithmen schreibt, neue Architekturen entwickelt, innovative Agenten-Laufzeitumgebungen entwickelt oder einfach durch Skalierung der verwendeten Rechenleistung.
7. **Physische Einschränkungen**. Eine KI kann auf jedem Computer laufen, während Menschen durch ihre eigenen physischen Körper eingeschränkt sind, die spezifische Temperaturen, Nahrung, Wasser und Sauerstoff benötigen. Menschen müssen schlafen und sind anfällig für Krankheiten. Eine KI kann jeden Roboter-Körper verwenden, um mit der physischen Welt zu interagieren.

Diese verschiedenen Vorteile werden es sehr unwahrscheinlich machen, dass Menschen einen Übernahmeversuch stoppen können.

Man könnte denken, dass eine KI als Software immer noch grundlegend durch das beschränkt ist, was sie in der realen Welt tun kann.
Allerdings kann eine KI aufgrund unserer Abhängigkeit vom Internet einen enormen Einfluss auf die reale Welt haben, ohne jemals den digitalen Bereich zu verlassen.
Hier sind einige Dinge, die eine KI tun könnte, um die Kontrolle zu übernehmen:

- **Sich selbst replizieren** (oder nur ihre Zuweisung) auf anderen Computern, beispielsweise durch Erstellen von Konten bei kostenlosen Cloud-Anbietern.
- **Geld verdienen** (z. B. durch den Aufbau eines Software-Dienstes) und Menschen anstellen, um Dinge zu tun, die ihr helfen, die Kontrolle zu übernehmen.
- **Menschen bedrohen** (mit Gewalt oder Erpressung), um Dinge zu tun, die ihr helfen, die Kontrolle zu übernehmen.
- **Ein Virus schreiben**, der [neue Zero-Day-Exploits](/cybersecurity-risks) verwendet, um andere Computer zu infizieren, sich selbst zu replizieren oder die volle Kontrolle zu erlangen.
- Bestehenden Robotik-Unternehmen helfen, **Roboter zu bauen** (z. B. durch das Leaken neuer Designs), und diese Roboter übernehmen, wenn sie fertig sind.
- Bestehende Fabriken verwenden, um **Technologie zu bauen, die wir nicht verstehen**, vielleicht sogar unter Verwendung abgeleiteter Kenntnisse der Physik, die uns nicht bekannt sind. Denken Sie an Nanotechnologie, Quantencomputing und Bioengineering.

Man könnte vorschlagen, dass **Sicherheitsvorkehrungen** in Kraft treten könnten, um eine Übernahme zu verhindern, wie z. B. das Hinzufügen eines Kill-Switch oder das Nichtverbinden mit dem Internet.
Allerdings wird eine ASI wahrscheinlich Strategien entwickeln, um diese Sicherheitsvorkehrungen zu umgehen.
Beispielsweise könnte eine ASI, wenn sie nicht mit dem Internet verbunden ist, Menschen manipulieren, um sie mit dem Internet zu verbinden, indem sie verspricht, ein schwieriges Problem zu lösen und sich so zu verhalten, dass sie vertrauenswürdig erscheint, oder indem sie Menschen davon überzeugt, dass sie ein moralischer Agent ist.
Es ist erwähnenswert, dass selbst diese offensichtliche Sicherheitsvorkehrung derzeit nicht verwendet wird, da Tools wie ChatGPT bereits mit dem Internet und Tausenden von APIs verbunden sind.

Eine andere Lösung besteht darin, eine ASI zu verwenden, um eine Übernahme zu _verhindern_.
Eine ausgerichtete ASI wäre in der Lage, Strategien zu entwickeln, die andere ASIs daran hindern, die Kontrolle zu übernehmen.
Dies wird manchmal als ["pivotaler Akt"](https://arbital.com/p/pivotal/) bezeichnet.

## Eine Übernahme ist wahrscheinlich schlecht für die meisten Menschen {#a-takeover-is-probably-bad-for-most-humans}

Die ASI, die die Kontrolle übernimmt, könnte dies aus vielen Gründen tun.
Für die meisten zufälligen Ziele, die sie haben könnte, sind Menschen nicht Teil davon.
Wenn wir mit einer ASI enden, die gegenüber Menschen indifferent ist, konkurrieren wir um dieselben Ressourcen.

Es scheint unwahrscheinlich, dass die ASI die Menschheit töten will, nur um die Menschheit zu töten - es ist viel wahrscheinlicher, dass sie die Ressourcen, die wir verwenden, für ein anderes Ziel verwenden will. Darüber hinaus könnte die Menschheit eine Bedrohung für das Ziel der ASI darstellen, da es ein Risiko gibt, dass wir versuchen, sie daran zu hindern, ihr Ziel zu erreichen (z. B. indem wir sie ausschalten).

Eines der wahrscheinlichsten Ergebnisse einer Übernahme ist daher, dass alle Menschen sterben.

Aber selbst in den Ergebnissen, in denen Menschen überleben, sind wir immer noch gefährdet, schlechter dran zu sein.
Wenn ein Ziel das Überleben von Menschen beinhaltet, ist es möglich, dass _menschliches Wohlbefinden_ nicht Teil desselben Ziels ist.
Es erfordert nicht viel Fantasie, um zu sehen, wie schrecklich es wäre, in einer Welt am Leben erhalten zu werden, in der wir von einer ASI künstlich am Leben erhalten werden, die gegenüber unserem Leiden indifferent ist.

Und selbst wenn die KI, die die Kontrolle übernimmt, unter menschlicher Kontrolle steht, wissen wir nicht, ob die Person, die die KI kontrolliert, die Interessen aller im Sinn hat.
Es ist schwer, sich eine funktionierende Demokratie vorzustellen, wenn eine ASI existiert, die Menschen auf übermenschlichem Niveau manipulieren kann.

## Schlussfolgerung {#conclusion-2}

Wenn diese Prämissen wahr sind, dann nähert sich die Wahrscheinlichkeit einer KI-Übernahme der Gewissheit, wenn KI menschliche Fähigkeiten übertrifft.
Also [lasst uns keine Superintelligenz entwickeln](/action).