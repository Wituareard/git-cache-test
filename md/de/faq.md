---
title: FAQ
description: H√§ufig gestellte Fragen zu PauseAI und den Risiken von superintelligenter KI.
---

<script>
    import SimpleToc from '$lib/components/simple-toc/SimpleToc.svelte'
</script>

<style>
    h2 {
        font-size: 1.2rem;
    }
</style>

<SimpleToc />

## Wer seid ihr?

Wir sind eine Gemeinschaft von [Freiwilligen](/people) und [lokale Gemeinschaften](/communities), die von einer [gemeinn√ºtzigen Organisation](/legal) koordiniert wird, um die [Risiken von KI](/risks) (einschlie√ülich des [Risikos des menschlichen Aussterbens](/xrisk)) zu mindern.
Unser Ziel ist es, unsere Regierungen davon zu √ºberzeugen, einzugreifen und die [Entwicklung von √ºbermenschlicher KI](/proposal) zu stoppen.
Wir tun dies, indem wir die √ñffentlichkeit informieren, mit Entscheidungstr√§gern sprechen und Proteste organisieren.

Sie k√∂nnen uns auf [Discord](https://discord.gg/2XXWXvErfA) (hier findet die meisten Koordination statt!), [Twitter](https://twitter.com/PauseAI), [Substack](https://substack.com/@pauseai), [Facebook](https://www.facebook.com/PauseAI), [TikTok](https://www.tiktok.com/@pauseai), [LinkedIn](https://www.linkedin.com/uas/login?session_redirect=/company/97035448/), [YouTube](https://www.youtube.com/@PauseAI), [Instagram](https://www.instagram.com/pause_ai), [Telegram](https://t.me/+UeTsIsNkmt82ZmQ8), [Whatsapp](https://chat.whatsapp.com/JgcAbjqRr8X3tvrXdeQvfj) und [Reddit](https://www.reddit.com/r/PauseAI/) finden.
Sie k√∂nnen uns auch per E-Mail kontaktieren: [joep@pauseai.info](mailto:joep@pauseai.info).

## Habt ihr nicht einfach Angst vor Ver√§nderungen und neuer Technologie?

Sie k√∂nnten √ºberrascht sein, dass die meisten Menschen bei PauseAI sich selbst als Techno-Optimisten betrachten.
Viele von ihnen sind in die KI-Entwicklung involviert, sind Gadget-Liebhaber und waren bisher sehr aufgeregt √ºber die Zukunft.
Insbesondere viele von ihnen waren aufgeregt √ºber das Potenzial von KI, um der Menschheit zu helfen.
Deshalb war f√ºr viele von ihnen die traurige Erkenntnis, dass KI ein existenzielles Risiko darstellen k√∂nnte, eine sehr [schwierige zu internalisieren](/psychology-of-x-risk).

## Wollt ihr alle KI verbieten?

Nein, nur die Entwicklung der gr√∂√üten allgemeinen KI-Systeme, oft "Frontier-Modelle" genannt.
Fast alle derzeit existierenden KI-Systeme w√§ren unter [unserem Vorschlag](/proposal) legal, und die meisten zuk√ºnftigen KI-Modelle werden auch legal bleiben.
Wir fordern ein Verbot von KI-Systemen, die leistungsf√§higer sind als GPT-4, bis wir wissen, wie wir provable sichere KI bauen k√∂nnen und sie unter demokratischer Kontrolle haben.

## Glaubt ihr, dass GPT-4 uns umbringen wird?

Nein, wir denken nicht, dass [aktuelle KI-Modelle](/sota) eine existenzielle Bedrohung darstellen.
Es scheint wahrscheinlich, dass die meisten n√§chsten KI-Modelle auch keine sein werden.
Aber wenn wir weiterhin immer leistungsf√§higere KI-Systeme bauen, werden wir schlie√ülich einen Punkt erreichen, an dem eines eine [existenzielle Bedrohung](/xrisk) darstellen wird.

## Kann ein Pause zur√ºckfeuern und die Dinge schlimmer machen?

Wir haben diese Bedenken in [diesem Artikel](/mitigating-pause-failures) angesprochen.

## Ist ein Pause √ºberhaupt m√∂glich?

AGI ist nicht unvermeidlich.
Es erfordert Horden von Ingenieuren mit Millionen-Dollar-Geh√§ltern.
Es erfordert eine voll funktionsf√§hige und unbeschr√§nkte Lieferkette der komplexesten Hardware.
Es erfordert, dass wir alle es diesen Unternehmen erlauben, mit unserer Zukunft zu spielen.

[Lesen Sie mehr √ºber die Machbarkeit eines Pause](/feasibility).

## Wer zahlt euch?

Virtuell alle unsere Aktionen bisher wurden von Freiwilligen durchgef√ºhrt.
Seit Februar 2024 ist PauseAI jedoch eine [registrierte gemeinn√ºtzige Stiftung](/legal), und wir haben mehrere Spenden von Einzelpersonen erhalten.
Wir haben auch 20.000 Dollar F√∂rderung vom LightSpeed-Netzwerk erhalten.

Sie k√∂nnen auch [spenden](/donate) an PauseAI, wenn Sie unsere Sache unterst√ºtzen!
Wir verwenden das meiste Geld, um lokalen Gemeinschaften zu erm√∂glichen, Veranstaltungen zu organisieren.

## Was sind eure Pl√§ne?

Fokus auf [die Bewegung zu vergr√∂√üern](/growth-strategy), Proteste zu organisieren, Politiker zu lobbyieren und die √ñffentlichkeit zu informieren.

√úberpr√ºfen Sie unseren [Zeitplan](/roadmap) f√ºr eine detaillierte √úbersicht √ºber unsere Pl√§ne und was wir mit mehr F√∂rderung tun k√∂nnten.

## Wie denkt ihr, dass ihr die Regierungen davon √ºberzeugen k√∂nnt, KI zu stoppen?

√úberpr√ºfen Sie unsere [Theorie des Wandels](/theory-of-change) f√ºr eine detaillierte √úbersicht √ºber unsere Strategie.

## Warum protestiert ihr?

- Protestieren zeigt der Welt, dass wir uns um diese Angelegenheit k√ºmmern. Indem wir protestieren, zeigen wir, dass wir bereit sind, unsere Zeit und Energie zu investieren, um die Menschen dazu zu bringen, zuzuh√∂ren.
- Proteste k√∂nnen und werden oft [die √∂ffentliche Meinung, das Wahlverhalten, das Unternehmensverhalten und die Politik positiv beeinflussen](https://www.socialchangelab.org/_files/ugd/503ba4_052959e2ee8d4924934b7efe3916981e.pdf).
- Die meisten Menschen unterst√ºtzen [friedliche/nicht-gewaltt√§tige Proteste](https://today.yougov.com/politics/articles/31718-do-protesters-want-help-or-hurt-america).
- Es gibt [keinen "Backfire"-Effekt](https://journals.sagepub.com/doi/full/10.1177/2378023120925949) [es sei denn, der Protest ist gewaltt√§tig](https://news.stanford.edu/2018/10/12/how-violent-protest-can-backfire/). Unsere Proteste sind friedlich und nicht-gewaltt√§tig.
- Es ist eine soziale Bindungserfahrung. Sie treffen andere Menschen, die Ihre Bedenken und Ihre Bereitschaft teilen, Ma√ünahmen zu ergreifen.
- √úberpr√ºfen Sie [diesen gro√üartigen Artikel](https://forum.effectivealtruism.org/posts/4ez3nvEmozwPwARr9/a-case-for-the-effectiveness-of-protest) f√ºr weitere Einblicke in die Wirksamkeit von Protesten.

Wenn Sie einen [Protest organisieren](/organizing-a-protest) m√∂chten, k√∂nnen wir Ihnen mit Ratschl√§gen und Ressourcen helfen.

## Wie wahrscheinlich ist es, dass superintelligente KI sehr schlechte Ergebnisse verursacht, wie das menschliche Aussterben?

Wir haben eine [Liste von 'p(doom)'-Werten](/pdoom) (Wahrscheinlichkeit von schlechten Ergebnissen) von verschiedenen bekannten Experten auf dem Gebiet zusammengestellt.

KI-Sicherheitsforscher (die die Experten auf diesem Gebiet sind) sind geteilt in dieser Frage, und Sch√§tzungen [reichen von 2% bis 97% mit einem Durchschnitt von 30%](https://web.archive.org/web/20221013014859/https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results).
Beachten Sie, dass keine (befragten) KI-Sicherheitsforscher glauben, dass es eine 0%-Chance gibt.
Es k√∂nnte jedoch eine Selektionsverzerrung geben: Menschen, die im Bereich der KI-Sicherheit arbeiten, tun dies wahrscheinlich, weil sie glauben, dass die Verhinderung von schlechten KI-Ergebnissen wichtig ist.

Wenn Sie KI-Forscher im Allgemeinen (nicht Sicherheitsspezialisten) fragen, sinkt diese Zahl auf einen [Mittelwert von etwa 14%](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/), mit einem Median von 5%.
Die √ºberwiegende Mehrheit, 86% von ihnen, glaubt, dass das Alignment-Problem sowohl ein reales als auch ein wichtiges Problem ist.
Beachten Sie, dass es auch hier eine Selektionsverzerrung in die entgegengesetzte Richtung geben k√∂nnte: Menschen, die im Bereich der KI arbeiten, tun dies wahrscheinlich, weil sie glauben, dass KI vorteilhaft sein wird.

_Stellen Sie sich vor, Sie werden zu einem Testflug in einem neuen Flugzeug eingeladen._
Die Flugzeugingenieure denken, dass es eine 14%-Chance gibt, dass es abst√ºrzt.
W√ºrden Sie in dieses Flugzeug einsteigen? Denn im Moment steigen wir alle in das KI-Flugzeug ein.

## Wie lange haben wir noch, bis superintelligente KI entsteht?

Es k√∂nnte Monate dauern, es k√∂nnte Jahrzehnte dauern, niemand wei√ü es genau.
Wir wissen jedoch, dass der Fortschritt in der KI oft stark untersch√§tzt wird.
Vor gerade drei Jahren dachten wir, dass wir KI-Systeme, die den SAT-Test bestehen, erst 2055 haben w√ºrden.
Wir haben es im April 2023 geschafft.
Wir sollten so handeln, als h√§tten wir sehr wenig Zeit, weil wir nicht √ºberrascht werden wollen.

[Lesen Sie mehr √ºber die Dringlichkeit](/urgency).

## Wenn wir Pause einlegen, was ist mit China?

Zun√§chst hat China strengere KI-Regulierungen als fast jedes andere Land.
Sie [erlaubten nicht einmal Chatbots](https://www.reuters.com/technology/chinas-slow-ai-roll-out-points-its-tech-sectors-new-regulatory-reality-2023-07-12/) und [verboten das Training auf Internet-Daten](https://cointelegraph.com/news/china-sets-stricter-rules-training-generative-ai-models) bis [September 2023](https://asia.nikkei.com/Business/Technology/China-approves-AI-chatbot-releases-but-will-it-unleash-innovation).
China hat eine kontrollierendere Regierung und hat daher noch mehr Grund, die unkontrollierbaren und unvorhersehbaren Auswirkungen von KI zu f√ºrchten.
W√§hrend des UNSC-Treffens zu KI-Sicherheit war China das einzige Land, das die M√∂glichkeit erw√§hnte, einen Pause einzulegen.

Beachten Sie auch, dass wir in erster Linie einen _internationalen_ Pause fordern, der durch einen Vertrag durchgesetzt wird.
Ein solcher Vertrag muss auch von China unterzeichnet werden.
Wenn der Vertrag garantiert, dass andere Nationen auch stoppen, und es ausreichende Durchsetzungsmechanismen gibt,
sollte dies etwas sein, das China auch sehen will.

## OpenAI und Google sagen, dass sie reguliert werden wollen. Warum protestiert ihr gegen sie?

Wir begr√º√üen [OpenAI](https://openai.com/blog/governance-of-superintelligence) und [Google](https://www.ft.com/content/8be1a975-e5e0-417d-af51-78af17ef4b79) f√ºr ihre Forderung nach internationaler Regulierung von KI.
Wir glauben jedoch, dass die aktuellen Vorschl√§ge nicht ausreichen, um eine KI-Katastrophe zu verhindern.
Google und Microsoft haben noch nicht √∂ffentlich etwas √ºber das existenzielle Risiko von KI gesagt.
Nur OpenAI [erw√§hnt explizit das Risiko des Aussterbens](https://openai.com/blog/governance-of-superintelligence), und wir begr√º√üen sie daf√ºr, dass sie dieses Risiko ernst nehmen.
Ihre Strategie ist jedoch ziemlich explizit: ein Pause ist unm√∂glich, wir m√ºssen zuerst superintelligent werden.
Das Problem dabei ist jedoch, dass sie [nicht glauben, dass sie das Alignment-Problem gel√∂st haben](https://youtu.be/L_Guz73e6fw?t=1478).
Die KI-Unternehmen sind in einem Wettlauf nach unten gefangen, bei dem die KI-Sicherheit f√ºr einen Wettbewerbsvorteil geopfert wird.
Dies ist einfach das Ergebnis von Marktdynamiken.
Wir brauchen Regierungen, die eingreifen und Politiken (auf internationaler Ebene) umsetzen, die [die schlimmsten Ergebnisse verhindern](/proposal).

## Dr√§ngen KI-Unternehmen die existenzielle Risiko-Erz√§hlung, um uns zu manipulieren?

Wir k√∂nnen nicht mit Sicherheit wissen, welche Motivationen diese Unternehmen haben, aber wir wissen, dass **das existenzielle Risiko nicht urspr√ºnglich von KI-Unternehmen vorangetrieben wurde - es waren Wissenschaftler, Aktivisten und NGOs**.
Lassen Sie uns die Zeitachse ansehen.

Es gab viele Menschen, die seit den fr√ºhen 2000er Jahren vor existenziellem Risiko gewarnt haben.
Eliezer Yudkowsky, Nick Bostrom, Stuart Russell, Max Tegmark und viele andere.
Sie hatten keine KI-Technologie zu pushen - sie waren einfach besorgt √ºber die Zukunft der Menschheit.

Die KI-Unternehmen erw√§hnten das existenzielle Risiko erst sehr k√ºrzlich.

Sam Altman ist eine interessante Ausnahme.
Er schrieb √ºber existenzielles KI-Risiko [bereits 2015 auf seinem privaten Blog](https://blog.samaltman.com/machine-intelligence-part-1), bevor er OpenAI gr√ºndete.
In den Jahren seitdem erw√§hnte er das existenzielle Risiko fast nicht mehr.
W√§hrend der Senatsanh√∂rung am 16. Mai 2023, als er nach seinem Blog-Beitrag zum existenziellen Risiko gefragt wurde, antwortete er nur, indem er √ºber Jobs und die Wirtschaft sprach.
Er dr√§ngte die existenzielle Risiko-Erz√§hlung nicht voran, er vermied sie aktiv.

Im Mai 2023 √§nderte sich alles:

- Am 1. Mai k√ºndigte der 'Gottvater der KI' Geoffrey Hinton [seinen Job bei Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/), um vor existenziellem Risiko zu warnen.
- Am 5. Mai wurde der [erste PauseAI-Protest angek√ºndigt](https://twitter.com/Radlib4/status/1654262421794717696), direkt vor OpenAIs Haust√ºr.
- Am 22. Mai ver√∂ffentlichte OpenAI [einen Blog-Beitrag √ºber die Regulierung von Superintelligenz](https://openai.com/blog/governance-of-superintelligence) und erw√§hnte das existenzielle Risiko zum ersten Mal.
- Am 24. Mai best√§tigte der ehemalige Google-CEO Eric Schmidt das existenzielle Risiko.
- Am 30. Mai wurde die [Safe.ai-Erkl√§rung (zum existenziellen Risiko)](https://www.safe.ai/statement-on-ai-risk) ver√∂ffentlicht. Diesmal einschlie√ülich Personen von OpenAI, Google und Microsoft.

Diese Unternehmen waren sehr langsam darin, das existenzielle Risiko anzuerkennen, wenn man bedenkt, dass viele ihrer Mitarbeiter sich dessen seit Jahren bewusst sind.
Also sehen wir es so, dass die KI-Unternehmen die existenzielle Risiko-Erz√§hlung nicht vorantreiben, sondern reagieren, wenn andere sie vorantreiben, und mit ihrer Reaktion warten, bis es absolut notwendig ist.

Die Gesch√§ftsanreize deuten in die entgegengesetzte Richtung: Unternehmen w√ºrden lieber nicht die Risiken ihrer Produkte betonen, um Kunden und Investitionen anzuziehen, anstatt sie zu √ºbertreiben.
Wie viel strenge Regulierung und negative Aufmerksamkeit laden sich die Unternehmen durch die Anerkennung dieser Gefahren ein?
Und w√ºrde ein Unternehmen wie OpenAI [20% seiner Rechenressourcen](https://openai.com/blog/introducing-superalignment) f√ºr KI-Sicherheit einsetzen, wenn es nicht an die Risiken glauben w√ºrde?

Hier ist unsere Interpretation: die KI-Unternehmen unterzeichneten die Erkl√§rung, weil _sie wissen, dass das existenzielle Risiko ein Problem ist, das sehr ernst genommen werden muss_.

Ein gro√üer Grund, warum viele andere Menschen nicht glauben wollen, dass das existenzielle Risiko ein reales Anliegen ist?
Weil die Anerkennung, dass _wir tats√§chlich in Gefahr sind_, eine sehr, sehr be√§ngstigende Sache ist.

[Lesen Sie mehr √ºber die Psychologie des existenziellen Risikos](/psychology-of-x-risk).

## Okay, ich will helfen! Was kann ich tun?

Es gibt viele [Dinge, die Sie tun k√∂nnen](/action).
Sie k√∂nnen auf eigene Faust einen [Brief](/writing-a-letter) schreiben, [Flyer](/flyering) verteilen, [lernen](/learn) und andere informieren, sich einem [Protest](/protests) anschlie√üen oder [spenden](/donate).
Aber noch wichtiger: Sie k√∂nnen [PauseAI beitreten](/join) und sich mit anderen koordinieren, die Ma√ünahmen ergreifen.
√úberpr√ºfen Sie, ob es [lokale Gemeinschaften](/communities) in Ihrer N√§he gibt.
Wenn Sie mehr beitragen m√∂chten, k√∂nnen Sie ein Freiwilliger werden und sich einem unserer [Teams](/teams) anschlie√üen oder [eine lokale Gemeinschaft gr√ºnden](/local-organizing).

Selbst wenn wir dem Ende der Welt gegen√ºberstehen, kann es noch Hoffnung und sehr lohnende Arbeit geben. üí™