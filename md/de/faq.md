

---
title: H√§ufig gestellte Fragen
description: H√§ufig gestellte Fragen √ºber PauseAI und die Risiken von superintelligenter KI.
---

<script>
    import SimpleToc from '$lib/components/simple-toc/SimpleToc.svelte'
</script>

<style>
    h2 {
        font-size: 1.2rem;
    }
</style>

<SimpleToc />

## Wer sind Sie? {#who-are-you}

Wir sind eine Gemeinschaft von [Freiwilligen](/people) und [lokale Gemeinschaften](/communities), die von einer [gemeinn√ºtzigen Organisation](/legal) koordiniert wird, um die [Risiken von KI](/risks) (einschlie√ülich des [Risikos des menschlichen Aussterbens](/xrisk)) zu mildern.
Wir m√∂chten unsere Regierungen davon √ºberzeugen, einzugreifen und die [Entwicklung von √ºbermenschlicher KI zu pausieren](/proposal).
Wir tun dies, indem wir die √ñffentlichkeit informieren, mit Entscheidungstr√§gern sprechen und Proteste organisieren.

Sie k√∂nnen uns auf [Discord](https://discord.gg/2XXWXvErfA) (hier findet die meisten Koordination statt!), [Twitter](https://twitter.com/PauseAI), [Substack](https://substack.com/@pauseai), [Facebook](https://www.facebook.com/PauseAI), [TikTok](https://www.tiktok.com/@pauseai), [LinkedIn](https://www.linkedin.com/uas/login?session_redirect=/company/97035448/), [YouTube](https://www.youtube.com/@PauseAI), [Instagram](https://www.instagram.com/pause_ai), [Telegram](https://t.me/+UeTsIsNkmt82ZmQ8), [Whatsapp](https://chat.whatsapp.com/JgcAbjqRr8X3tvrXdeQvfj) und [Reddit](https://www.reddit.com/r/PauseAI/) finden.
Sie k√∂nnen uns auch per E-Mail kontaktieren: [joep@pauseai.info](mailto:joep@pauseai.info).

## Haben Sie nicht einfach Angst vor Ver√§nderungen und neuer Technologie? {#arent-you-just-scared-of-changes-and-new-technology}

Sie k√∂nnten √ºberrascht sein, dass die meisten Menschen bei PauseAI sich selbst als Techno-Optimisten betrachten.
Viele von ihnen sind in die KI-Entwicklung involviert, sind Gadget-Liebhaber und waren bisher sehr aufgeregt √ºber die Zukunft.
Insbesondere viele von ihnen waren aufgeregt √ºber das Potenzial von KI, um der Menschheit zu helfen.
Deshalb war f√ºr viele von ihnen die traurige Erkenntnis, dass KI ein existenzielles Risiko darstellen k√∂nnte, eine sehr [schwierige zu internalisieren](/psychology-of-x-risk).

## Wollen Sie alle KI verbieten? {#do-you-want-to-ban-all-ai}

Nein, nur die Entwicklung der gr√∂√üten allgemeinen KI-Systeme, oft "Frontier-Modelle" genannt.
Fast alle derzeit existierenden KI-Systeme w√§ren unter [unserem Vorschlag](/proposal) legal, und die meisten zuk√ºnftigen KI-Modelle werden auch legal bleiben.
Wir fordern ein Verbot von KI-Systemen, die leistungsf√§higer sind als GPT-4, bis wir wissen, wie wir provable sichere KI bauen k√∂nnen und sie unter demokratischer Kontrolle haben.

## Glauben Sie, dass GPT-4 uns umbringen wird? {#do-you-believe-gpt-4-is-going-to-kill-us}

Nein, wir denken nicht, dass [aktuelle KI-Modelle](/sota) eine existenzielle Bedrohung darstellen.
Es scheint wahrscheinlich, dass die meisten n√§chsten KI-Modelle auch keine sein werden.
Aber wenn wir weiterhin immer leistungsf√§higere KI-Systeme bauen, werden wir schlie√ülich einen Punkt erreichen, an dem eines eine [existenzielle Bedrohung](/xrisk) darstellen wird.

## Kann eine Pause nach hinten losgehen und die Dinge verschlimmern? {#can-a-pause-backfire-and-make-things-worse}

Wir haben diese Bedenken in [diesem Artikel](/mitigating-pause-failures) angesprochen.

## Ist eine Pause √ºberhaupt m√∂glich? {#is-a-pause-even-possible}

AGI ist nicht unvermeidlich.
Es erfordert Horden von Ingenieuren mit Millionen-Dollar-Geh√§ltern.
Es erfordert eine voll funktionsf√§hige und unbeschr√§nkte Lieferkette der komplexesten Hardware.
Es erfordert, dass wir alle es diesen Unternehmen erlauben, mit unserer Zukunft zu spielen.

[Lesen Sie mehr √ºber die Machbarkeit einer Pause](/feasibility).

## Wer zahlt Ihnen? {#who-is-paying-you}

Praktisch alle unsere Aktionen bisher wurden von Freiwilligen durchgef√ºhrt.
Seit Februar 2024 ist PauseAI jedoch eine [registrierte gemeinn√ºtzige Stiftung](/legal), und wir haben mehrere Spenden von Einzelpersonen erhalten.
Wir haben auch 20.000 Dollar F√∂rderung vom LightSpeed-Netzwerk erhalten.

Sie k√∂nnen auch [spenden](/donate) an PauseAI, wenn Sie unsere Sache unterst√ºtzen!
Wir verwenden das meiste Geld, um lokalen Gemeinschaften die Organisation von Veranstaltungen zu erm√∂glichen.

## Was sind Ihre Pl√§ne? {#what-are-your-plans}

Fokussieren Sie sich auf das [Wachstum der Bewegung](/growth-strategy), die Organisation von Protesten, Lobbyarbeit bei Politikern und die Information der √ñffentlichkeit.

√úberpr√ºfen Sie unsere [Roadmap](/roadmap) f√ºr eine detaillierte √úbersicht √ºber unsere Pl√§ne und was wir mit mehr F√∂rderung tun k√∂nnten.

## Wie denken Sie, dass Sie die Regierungen davon √ºberzeugen k√∂nnen, KI zu pausieren? {#how-do-you-think-you-can-convince-governments-to-pause-ai}

√úberpr√ºfen Sie unsere [Theorie des Wandels](/theory-of-change) f√ºr eine detaillierte √úbersicht √ºber unsere Strategie.

## Warum protestieren Sie? {#why-do-you-protest}

- Protestieren zeigt der Welt, dass wir uns um diese Angelegenheit k√ºmmern. Indem wir protestieren, zeigen wir, dass wir bereit sind, unsere Zeit und Energie zu investieren, um die Menschen zum Zuh√∂ren zu bringen.
- Proteste k√∂nnen und werden oft [positiv beeinflussen](https://www.socialchangelab.org/_files/ugd/503ba4_052959e2ee8d4924934b7efe3916981e.pdf) die √∂ffentliche Meinung, das Wahlverhalten, das Verhalten von Unternehmen und die Politik.
- Die meisten Menschen unterst√ºtzen [friedliche/nicht-gewaltt√§tige Proteste](https://today.yougov.com/politics/articles/31718-do-protesters-want-help-or-hurt-america)
- Es gibt [keinen "Backfire"-Effekt](https://journals.sagepub.com/doi/full/10.1177/2378023120925949) [es sei denn, der Protest ist gewaltt√§tig](https://news.stanford.edu/2018/10/12/how-violent-protest-can-backfire/). Unsere Proteste sind friedlich und nicht-gewaltt√§tig.
- Es ist eine soziale Bindungserfahrung. Sie treffen andere Menschen, die Ihre Bedenken und Ihre Bereitschaft, Ma√ünahmen zu ergreifen, teilen.
- Lesen Sie [diesen gro√üartigen Artikel](https://forum.effectivealtruism.org/posts/4ez3nvEmozwPwARr9/a-case-for-the-effectiveness-of-protest) f√ºr weitere Einblicke in die Wirksamkeit von Protesten

Wenn Sie einen [Protest organisieren](/organizing-a-protest) m√∂chten, k√∂nnen wir Ihnen mit Ratschl√§gen und Ressourcen helfen.

## Wie wahrscheinlich ist es, dass superintelligente KI sehr schlechte Ergebnisse verursacht, wie das menschliche Aussterben? {#how-likely-is-it-that-superintelligent-ai-will-cause-very-bad-outcomes-like-human-extinction}

Wir haben eine [Liste von 'p(doom)'-Werten](/pdoom) (Wahrscheinlichkeit von schlechten Ergebnissen) von verschiedenen bekannten Experten auf dem Gebiet zusammengestellt.

KI-Sicherheitsforscher (die die Experten auf diesem Gebiet sind) sind geteilter Meinung √ºber diese Frage, und Sch√§tzungen [reichen von 2% bis 97% mit einem Durchschnitt von 30%](https://web.archive.org/web/20221013014859/https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results).
Beachten Sie, dass kein (befragter) KI-Sicherheitsforscher glaubt, dass es eine 0%-Chance gibt.
Es k√∂nnte jedoch eine Selektionsverzerrung geben: Menschen, die im Bereich der KI-Sicherheit arbeiten, tun dies wahrscheinlich, weil sie glauben, dass die Verhinderung von schlechten KI-Ergebnissen wichtig ist.

Wenn Sie KI-Forscher im Allgemeinen (nicht Sicherheitsspezialisten) fragen, sinkt diese Zahl auf einen [Mittelwert von etwa 14%](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/), mit einem Median von 5%.
Die √ºberwiegende Mehrheit, 86% von ihnen, glaubt, dass das Alignment-Problem sowohl ein reales als auch ein wichtiges Problem ist.
Beachten Sie, dass es auch hier eine Selektionsverzerrung in die entgegengesetzte Richtung geben k√∂nnte: Menschen, die in der KI arbeiten, tun dies wahrscheinlich, weil sie glauben, dass KI vorteilhaft sein wird.

_Stellen Sie sich vor, Sie werden zu einem Testflug in einem neuen Flugzeug eingeladen._
Die Flugzeugingenieure denken, dass es eine 14%-Chance gibt, dass es abst√ºrzt.
W√ºrden Sie in dieses Flugzeug einsteigen? Denn im Moment steigen wir alle in das KI-Flugzeug ein.

## Wie lange haben wir noch, bis superintelligente KI entsteht? {#how-long-do-we-have-until-superintelligent-ai}

Es k√∂nnte Monate dauern, es k√∂nnte Jahrzehnte dauern, niemand wei√ü es genau.
Wir wissen jedoch, dass der Fortschritt in der KI oft stark untersch√§tzt wird.
Vor gerade drei Jahren dachten wir, dass wir KI-Systeme, die den SAT-Test bestehen, im Jahr 2055 haben w√ºrden.
Wir haben es im April 2023 geschafft.
Wir sollten so handeln, als h√§tten wir sehr wenig Zeit, weil wir nicht √ºberrascht werden wollen.

[Lesen Sie mehr √ºber die Dringlichkeit](/urgency).

## Wenn wir pausieren, was ist mit China? {#if-we-pause-what-about-china}

Zun√§chst hat China strengere KI-Regulierungen als fast jedes andere Land.
Sie [erlaubten nicht einmal Chatbots](https://www.reuters.com/technology/chinas-slow-ai-roll-out-points-its-tech-sectors-new-regulatory-reality-2023-07-12/) und [verboten das Training auf Internet-Daten](https://cointelegraph.com/news/china-sets-stricter-rules-training-generative-ai-models) bis [September 2023](https://asia.nikkei.com/Business/Technology/China-approves-AI-chatbot-releases-but-will-it-unleash-innovation).
China hat eine kontrollierendere Regierung und hat daher noch mehr Grund, die unkontrollierbaren und unvorhersehbaren Auswirkungen von KI zu f√ºrchten.
W√§hrend des UNSC-Treffens zu KI-Sicherheit war China das einzige Land, das die M√∂glichkeit erw√§hnte, eine Pause umzusetzen.

Beachten Sie auch, dass wir in erster Linie eine _internationale_ Pause fordern, die durch einen Vertrag durchgesetzt wird.
Ein solcher Vertrag muss auch von China unterzeichnet werden.
Wenn der Vertrag garantiert, dass andere Nationen auch stoppen, und es ausreichende Durchsetzungsmechanismen gibt,
sollte dies etwas sein, das China auch sehen will.

## OpenAI und Google sagen, dass sie reguliert werden wollen. Warum protestieren Sie gegen sie? {#openai-and-google-are-saying-they-want-to-be-regulated-why-are-you-protesting-them}

Wir begr√º√üen [OpenAI](https://openai.com/blog/governance-of-superintelligence) und [Google](https://www.ft.com/content/8be1a975-e5e0-417d-af51-78af17ef4b79) f√ºr ihre Forderung nach internationaler Regulierung von KI.
Wir glauben jedoch, dass die aktuellen Vorschl√§ge nicht ausreichen, um eine KI-Katastrophe zu verhindern.
Google und Microsoft haben noch nicht √∂ffentlich etwas √ºber das existenzielle Risiko von KI gesagt.
Nur OpenAI [erw√§hnt explizit das Risiko des Aussterbens](https://openai.com/blog/governance-of-superintelligence), und wir begr√º√üen sie daf√ºr, dass sie dieses Risiko ernst nehmen.
Ihre Strategie ist jedoch ziemlich explizit: Eine Pause ist unm√∂glich, wir m√ºssen zuerst zu Superintelligenz gelangen.
Das Problem dabei ist jedoch, dass sie [nicht glauben, dass sie das Alignment-Problem gel√∂st haben](https://youtu.be/L_Guz73e6fw?t=1478).
Die KI-Unternehmen sind in einem Wettlauf nach unten gefangen, bei dem die KI-Sicherheit f√ºr einen Wettbewerbsvorteil geopfert wird.
Dies ist einfach das Ergebnis von Marktdynamiken.
Wir brauchen Regierungen, die eingreifen und Politiken umsetzen (auf internationaler Ebene), die [die schlimmsten Ergebnisse verhindern](/proposal).

## Dr√§ngen KI-Unternehmen die existenzielle Risiko-Erz√§hlung, um uns zu manipulieren? {#are-ai-companies-pushing-the-existential-risk-narrative-to-manipulate-us}

Wir k√∂nnen nicht mit Sicherheit wissen, welche Motivationen diese Unternehmen haben, aber wir wissen, dass **das existenzielle Risiko nicht urspr√ºnglich von KI-Unternehmen vorangetrieben wurde - es waren Wissenschaftler, Aktivisten und NGOs**.
Lassen Sie uns auf die Zeitachse schauen.

Es gab viele Menschen, die seit den fr√ºhen 2000er Jahren vor existenziellem Risiko gewarnt haben.
Eliezer Yudkowsky, Nick Bostrom, Stuart Russell, Max Tegmark und viele andere.
Sie hatten keine KI-Technologie zu pushen - sie waren einfach besorgt √ºber die Zukunft der Menschheit.

Die KI-Unternehmen erw√§hnten das existenzielle Risiko erst sehr k√ºrzlich.

Sam Altman ist eine interessante Ausnahme.
Er schrieb √ºber existenzielles KI-Risiko [im Jahr 2015 auf seinem privaten Blog](https://blog.samaltman.com/machine-intelligence-part-1), bevor er OpenAI gr√ºndete.
In den Jahren seitdem erw√§hnte er das existenzielle Risiko fast nicht mehr.
W√§hrend der Senatsanh√∂rung am 16. Mai 2023, als er nach seinem Blog-Beitrag zum existenziellen Risiko gefragt wurde, antwortete er nur, indem er √ºber Jobs und die Wirtschaft sprach.
Er dr√§ngte die existenzielle Risiko-Erz√§hlung nicht voran, er vermied sie aktiv.

Im Mai 2023 √§nderte sich alles:

- Am 1. Mai k√ºndigte der "Gottvater der KI" Geoffrey Hinton [seinen Job bei Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/), um vor existenziellem Risiko zu warnen.
- Am 5. Mai wurde der [erste PauseAI-Protest angek√ºndigt](https://twitter.com/Radlib4/status/1654262421794717696), direkt vor OpenAIs Haust√ºr.
- Am 22. Mai ver√∂ffentlichte OpenAI [einen Blog-Beitrag √ºber die Regulierung von Superintelligenz](https://openai.com/blog/governance-of-superintelligence) und erw√§hnte das existenzielle Risiko zum ersten Mal.
- Am 24. Mai best√§tigte der ehemalige Google-CEO Eric Schmidt das existenzielle Risiko.
- Am 30. Mai wurde die [Safe.ai-Erkl√§rung (zum existenziellen Risiko)](https://www.safe.ai/statement-on-ai-risk) ver√∂ffentlicht. Diesmal einschlie√ülich Menschen von OpenAI, Google und Microsoft.

Diese Unternehmen waren sehr langsam darin, das existenzielle Risiko anzuerkennen, wenn man bedenkt, dass viele ihrer Mitarbeiter sich seit Jahren dessen bewusst sind.
Also sehen wir es so, dass die KI-Unternehmen die existenzielle Risiko-Erz√§hlung nicht vorantreiben, sondern reagieren, wenn andere sie vorantreiben, und mit ihrer Reaktion warten, bis es absolut notwendig ist.

Die Gesch√§ftsanreize deuten in die andere Richtung: Unternehmen w√ºrden lieber nicht, dass die Menschen sich Sorgen √ºber die Risiken ihrer Produkte machen.
Fast alle Unternehmen bagatellisieren Risiken, um Kunden und Investitionen anzuziehen, anstatt sie zu √ºbertreiben.
Wie viel strenge Regulierung und negative Aufmerksamkeit laden sich die Unternehmen durch das Eingest√§ndnis dieser Gefahren auf?
Und w√ºrde ein Unternehmen wie OpenAI [20% seiner Rechenressourcen](https://openai.com/blog/introducing-superalignment) f√ºr KI-Sicherheit einsetzen, wenn es nicht an die Risiken glauben w√ºrde?

Hier ist unsere Interpretation: Die KI-Unternehmen unterzeichneten die Erkl√§rung, weil _sie wissen, dass das existenzielle Risiko ein Problem ist, das sehr ernst genommen werden muss_.

Ein gro√üer Grund, warum viele andere Menschen immer noch nicht glauben wollen, dass das existenzielle Risiko ein reales Anliegen ist?
Weil die Anerkennung, dass _wir tats√§chlich in Gefahr sind_, eine sehr, sehr be√§ngstigende Sache ist.

[Lesen Sie mehr √ºber die Psychologie des existenziellen Risikos](/psychology-of-x-risk).

## Okay, ich will helfen! Was kann ich tun? {#ok-i-want-to-help-what-can-i-do}

Es gibt viele [Dinge, die Sie tun k√∂nnen](/action).
Sie k√∂nnen auf eigene Faust einen [Brief schreiben](/writing-a-letter), [Flyer verteilen](/flyering), [lernen](/learn) und andere informieren, sich einem [Protest anschlie√üen](/protests) oder [Geld spenden](/donate)!
Aber noch wichtiger: Sie k√∂nnen [PauseAI beitreten](/join) und sich mit anderen koordinieren, die Ma√ünahmen ergreifen.
√úberpr√ºfen Sie, ob es [lokale Gemeinschaften](/communities) in Ihrer N√§he gibt.
Wenn Sie mehr beitragen m√∂chten, k√∂nnen Sie ein Freiwilliger werden und sich einem unserer [Teams anschlie√üen](/teams) oder [eine lokale Gemeinschaft gr√ºnden](/local-organizing)!

Selbst wenn wir dem Ende der Welt gegen√ºberstehen, kann es immer noch Hoffnung und sehr lohnende Arbeit geben. üí™