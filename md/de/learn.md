---
title: Warum KI-Sicherheit wichtig ist
description: Bildungsressourcen (Videos, Artikel, Bücher) zu KI-Risiken und KI-Alignment
---

Eines der wichtigsten Dinge, die Sie tun können, um bei der KI-Alignment und dem existenziellen Risiko (x-Risiko) zu helfen, das Superintelligenz darstellt, ist, sich darüber zu informieren.
Hier sind einige Ressourcen, um Ihnen den Einstieg zu erleichtern.

## Websites

- [AISafety.com](https://www.aisafety.com) & [AISafety.info](https://aisafety.info). Die Landingpages für KI-Sicherheit. Erfahren Sie mehr über die Risiken, Gemeinschaften, Veranstaltungen, Jobs, Kurse, Ideen zur Minderung der Risiken und vieles mehr!
- [AISafety.dance](https://aisafety.dance). Eine unterhaltsame, freundliche und interaktive Einführung in die katastrophalen Risiken von KI!
- [AISafety.world](https://aisafety.world/tiles/). Die gesamte KI-Sicherheitslandschaft mit allen Organisationen, Medien, Foren, Blogs und anderen Akteuren und Ressourcen.
- [IncidentDatabase.ai](https://incidentdatabase.ai/). Datenbank von Vorfällen, bei denen KI-Systeme Schaden verursacht haben.
<!-- [NavigatingAIRisks.ai](https://www.navigatingrisks.ai/). Ein Blog mit verschiedenen interessanten Artikeln. - [PauseAI.info](https://pauseai.info). Besuchen Sie den Rest der PauseAI-Seite für viele weitere Informationen und [Ressourcen](/learn), nützliche [Aktionen](/action), Experten[ Zitate](/quotes), kurze einseitige [Flyer](PauseAI_flyer.pdf), verwandte [FAQs](/faq) usw.

## Newsletter

- [PauseAI Substack](https://pauseai.substack.com/): Unser Newsletter.
- [TransformerNews](https://www.transformernews.ai/) Umfassender wöchentlicher Newsletter zu KI-Sicherheit und -Governance.
- [Don't Worry About The Vase](https://thezvi.substack.com/): Ein Newsletter über KI-Sicherheit, Rationalität und andere Themen.

## Videos

- [Kurzgesagt - A.I. ‐ Die letzte Erfindung der Menschheit?](https://www.youtube.com/watch?v=fa8k8IQ1_X0) (20 Minuten). Die Geschichte von KI und eine Einführung in das Konzept der Superintelligenz.
- [80k hours - Könnte KI die Menschheit auslöschen?](https://youtu.be/qzyEgZwfkKY?si=ief1l2PpkZ7_s6sq) (10 Minuten). Eine großartige Einführung in das Problem aus einer bodenständigen Perspektive.
- [Superintelligente KI sollte Sie beunruhigen...](https://www.youtube.com/watch?v=xBqU1QxCao8) (1 Minute). Die beste superkurze Einführung.
- [Don't look up - Die Dokumentation: Der Fall für KI als existenzielle Bedrohung](https://www.youtube.com/watch?v=U1eyUjVRir4) (17 Minuten). Eine eindrucksvolle und schön bearbeitete Dokumentation über die Gefahren von KI mit vielen Expertenzitaten aus Interviews.
- [Länder entwickeln KI aus Gründen](https://youtu.be/-9V9cIixPbM?si=L9q6PF2D6h_EBEwF) (10 Minuten). Karikatur des Rennens zu einer Superintelligenz und ihrer Gefahren.
- [Max Tegmark | Ted Talk (2023)](https://www.youtube.com/watch?v=xUNx_PxNHrY) (15 Minuten). KI-Fähigkeiten verbessern sich schneller als erwartet.
- [Tristan Harris | Nobelpreis-Gipfel 2023](https://www.youtube.com/watch?v=6lVBp2XjWsg) (15 Minuten). Vortrag darüber, warum wir unsere "paleolithischen Gehirne akzeptieren, unsere mittelalterlichen Institutionen aufwerten und die göttliche Technologie binden" müssen.
- [Sam Harris | Können wir KI entwickeln, ohne die Kontrolle darüber zu verlieren?](https://www.youtube.com/watch?v=8nt3edWLgIg) (15 Minuten). Ted-Talk über die verrückte Situation, in der wir uns befinden.
- [Ilya