---
title: Warum AI-Sicherheit wichtig ist
description: Bildungsressourcen (Videos, Artikel, Bücher) über AI-Risiken und AI-Alignment
---

Eines der wichtigsten Dinge, die Sie tun können, um bei der AI-Alignment und dem existenziellen Risiko (x-Risiko) zu helfen, das Superintelligenz darstellt, ist, sich darüber zu informieren.
Hier sind einige Ressourcen, um Sie auf den Weg zu bringen.

## Websites {#websites}

- [AISafety.com](https://www.aisafety.com) & [AISafety.info](https://aisafety.info). Die Landingpages für AI-Sicherheit. Erfahren Sie mehr über die Risiken, Gemeinschaften, Veranstaltungen, Jobs, Kurse, Ideen, um die Risiken zu mildern, und vieles mehr!
- [AISafety.dance](https://aisafety.dance). Eine unterhaltsamere, freundlichere und interaktivere Einführung in die katastrophalen AI-Risiken!
- [AISafety.world](https://aisafety.world/tiles/). Die gesamte AI-Sicherheitslandschaft mit allen Organisationen, Medien, Foren, Blogs und anderen Akteuren und Ressourcen.
- [IncidentDatabase.ai](https://incidentdatabase.ai/). Datenbank von Vorfällen, bei denen AI-Systeme Schaden verursacht haben.
<!-- [NavigatingAIRisks.ai](https://www.navigatingrisks.ai/). Ein Blog mit verschiedenen interessanten Artikeln. - [PauseAI.info](https://pauseai.info). Überprüfen Sie den Rest der PauseAI-Seite hier für viele verwandte Informationen und [Ressourcen](/learn), nützliche [Aktionen](/action), Experten-[Zitate](/quotes), kurze einseitige [Flyer](PauseAI_flyer.pdf), verwandte [FAQs](/faq) usw.

## Newsletter {#newsletters}

- [PauseAI Substack](https://pauseai.substack.com/): Unser Newsletter.
- [TransformerNews](https://www.transformernews.ai/) Umfassender wöchentlicher Newsletter über AI-Sicherheit und -Governance.
- [Don't Worry About The Vase](https://thezvi.substack.com/): Ein Newsletter über AI-Sicherheit, Rationalität und andere Themen.

## Videos {#videos}

- [Kurzgesagt - A.I. ‐ Die letzte Erfindung der Menschheit?](https://www.youtube.com/watch?v=fa8k8IQ1_X0) (20 Min.). Die Geschichte der AI und eine Einführung in das Konzept der Superintelligenz.
- [80k hours - Kann AI die Menschheit auslöschen?](https://youtu.be/qzyEgZwfkKY?si=ief1l2PpkZ7_s6sq) (10 Min.). Eine großartige Einführung in das Problem aus einer bodenständigen Perspektive.
- [Superintelligente AI sollte Sie beunruhigen...](https://www.youtube.com/watch?v=xBqU1QxCao8) (1 Min.). Die beste superkurze Einführung.
- [Don't look up - Die Dokumentation: Der Fall für AI als existenzielle Bedrohung](https://www.youtube.com/watch?v=U1eyUjVRir4) (17 Min.). Eine leistungsstarke und gut bearbeitete Dokumentation über die Gefahren von AI, mit vielen Expertenzitaten aus Interviews.
- [Länder entwickeln AI aus Gründen](https://youtu.be/-9V9cIixPbM?si=L9q6PF2D6h_EBEwF) (10 Min.). Karikatur des Rennens zu einer Superintelligenz und ihrer Gefahren.
- [Max Tegmark | Ted Talk (2023)](https://www.youtube.com/watch?v=xUNx_PxNHrY) (15 Min.). AI-Fähigkeiten verbessern sich schneller als erwartet.
- [Tristan Harris | Nobel-Preis-Gipfel 2023](https://www.youtube.com/watch?v=6lVBp2XjWsg) (15 Min.). Vortrag darüber, warum wir unsere "paleolithischen Gehirne aufwerten, unsere mittelalterlichen Institutionen verbessern und die gottgleiche Technologie binden" müssen.
- [Sam Harris | Können wir AI entwickeln, ohne die Kontrolle darüber zu verlieren?](https://www.youtube.com/watch?v=8nt3edWLgIg) (15 Min.). Ted-Talk über die verrückte Situation, in der wir uns befinden.
- [Ilya: Der AI-Wissenschaftler, der die Welt prägt](https://youtu.be/9iqn1HhFJ6c?si=WnzvpdsPtgCvqAZg) (12 Min.). Co-Gründer und ehemaliger Chef-Wissenschaftler bei OpenAI erklärt, wie AGI die Kontrolle über alles übernehmen wird und warum wir ihnen beibringen müssen, sich um Menschen zu kümmern.
- [Exploring the dangers from Artificial Intelligence](https://www.youtube.com/watch?v=sPyu_dTSma0&t=1328s) (25 Min.). Zusammenfassung der Cybersicherheits-, Biohazard- und Macht-suchenden AI-Risiken.
- [Warum dieser Top-AI-Guru denkt, dass wir in Schwierigkeiten stecken könnten | The InnerView](https://youtu.be/YZjmZFDx-pA?si=Y7QUxTaJcuC6LVji) (26 Min.). Interview mit Connor Leahy über AI-X-Risiken im Fernsehen.
- [Das AI-Dilemma](https://www.youtube.com/watch?v=xoVJKj8lcNQ&t=1903s) (1 Std.). Präsentation über die Gefahren von AI und das Rennen, in dem AI-Unternehmen stecken.
- [Robert Miles' YouTube-Videos](https://www.youtube.com/watch?v=tlS5Y2vm02c&list=PLfHsskCxi_g-c62a_dmsNuHynaXsRQm40) sind ein großartiger Ort, um die meisten Grundlagen der AI-Alignment zu verstehen.

## Podcasts {#podcasts}

- [Future of Life Institute | Connor Leahy über AI-Sicherheit und warum die Welt fragil ist](https://youtu.be/cSL3Zau1X8g?si=0X3EKoxZ80_HN9Rl&t=1803). Interview mit Connor über die AI-Sicherheitsstrategien.
- [Lex Fridman | Max Tegmark: Der Fall für die Einstellung der AI-Entwicklung](https://youtu.be/VcVfceTsD0A?t=1547). Interview, das in die Details unserer aktuellen gefährlichen Situation eintaucht.
- [Sam Harris | Eliezer Yudkowsky: AI, Racing Toward the Brink](https://samharris.org/episode/SE60B0CF4B8). Gespräch über die Natur der Intelligenz, verschiedene Arten von AI, das Alignment-Problem, Ist vs. Soll und vieles mehr. Eine von vielen Episoden, die Making Sense über AI-Sicherheit hat.
- [Connor Leahy, AI-Feueralarm](https://youtu.be/pGjyiqJZPJo?t=2510). Vortrag über die Intelligenzexplosion und warum sie das wichtigste Ereignis sein könnte, das jemals passieren könnte.
- [Die empfohlenen Episoden des 80.000-Stunden-Podcasts über AI](https://80000hours.org/podcast/on-artificial-intelligence/). Nicht 80.000 Stunden lang, sondern eine Zusammenstellung von Episoden des 80.000-Stunden-Podcasts über AI-Sicherheit.
- [Episoden des Future of Life Institute-Podcasts über AI](https://futureoflife.org/podcast/?_category_browser=ai). Alle Episoden des FLI-Podcasts über die Zukunft der künstlichen Intelligenz.

Podcasts mit PauseAI-Mitgliedern finden Sie in der [Medienberichterstattung](/press)-Liste.

## Artikel {#articles}

- [Das 'Don't Look Up'-Denken, das uns mit AI zum Untergang bringen könnte](https://time.com/6273743/thinking-that-could-doom-us-with-ai/) (von Max Tegmark)
- [Die Pause der AI-Entwicklungen reicht nicht aus. Wir müssen alles abschalten](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/) (von Eliezer Yudkowsky)
- [Der Fall für die Verlangsamung von AI](https://www.vox.com/the-highlight/23621198/artificial-intelligence-chatgpt-openai-existential-risk-china-ai-safety-technology) (von Sigal Samuel)
- [Die AI-Revolution: Der Weg zur Superintelligenz](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) (von WaitButWhy)
- [Wie sich rebellische AIs entwickeln könnten](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) (von Yoshua Bengio)
<!-- - [Eine lockere Einführung in AI-Doom und -Alignment](https://carado.moe/ai-doom.html)
Ich mag es und die Tatsache, dass es eine leichtere Lektüre ist, aber ich bin nicht sicher, ob ich es einfügen möchte, weil es Alignment als nur technische Arbeit definiert und die Leute auffordert, nur technische Arbeit zu leisten -->
- [Argumente gegen die Ernsthaftigkeit von AI-Sicherheit durchdenken](https://yoshuabengio.org/2024/07/09/reasoning-through-arguments-against-taking-ai-safety-seriously/) (von Yoshua Bengio)

Wenn Sie lesen möchten, was Journalisten über PauseAI geschrieben haben, überprüfen Sie die Liste der [Medienberichterstattung](/press).

## Bücher {#books}

<!-- - [AI: Unexplainable, Unpredictable, Uncontrollable](https://www.goodreads.com/book/show/197554072-ai) (Roman Yampolskiy, 2024)
Ich habe noch nichts Gutes darüber gehört -->
- [Unkontrollierbar: Die Bedrohung durch künstliche Superintelligenz und das Rennen, um die Welt zu retten](https://www.goodreads.com/book/show/202416160-uncontrollable) (Darren McKee, 2023). Holen Sie es sich kostenlos!
- [Der Abgrund: Existenzrisiko und die Zukunft der Menschheit](https://www.goodreads.com/en/book/show/50963653) (Toby Ord, 2020)
- [Das Alignment-Problem](https://www.goodreads.com/book/show/50489349-the-alignment-problem) (Brian Christian, 2020)
- [Menschlich kompatibel: Künstliche Intelligenz und das Problem der Kontrolle](https://www.goodreads.com/en/book/show/44767248) (Stuart Russell, 2019)
- [Leben 3.0: Mensch sein im Zeitalter der künstlichen Intelligenz](https://www.goodreads.com/en/book/show/34272565) (Max Tegmark, 2017)
- [Superintelligenz: Wege, Gefahren, Strategien](https://www.goodreads.com/en/book/show/20527133) (Nick Bostrom, 2014)
- [Unsere letzte Erfindung: Künstliche Intelligenz und das Ende der menschlichen Ära](https://www.goodreads.com/en/book/show/17286699) (James Barrat, 2013)

## Kurse {#courses}

- [AGI-Sicherheitsgrundlagen](https://www.agisafetyfundamentals.com/) (30 Stunden)
- [CHAI-Bibliographie empfohlener Materialien](https://humancompatible.ai/bibliography) (50 Stunden+)
- [AISafety.training](https://aisafety.training/): Überblick über Trainingsprogramme, Konferenzen und andere Veranstaltungen

## Organisationen {#organizations}

- [Future of Life Institute](https://futureoflife.org/cause-area/artificial-intelligence/) hat den [offenen Brief](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) gestartet, der von Max Tegmark geleitet wird.
- [FutureSociety](https://thefuturesociety.org/about-us/)
- [Conjecture](https://www.conjecture.dev/). Start-up, das an AI-Alignment und -Politik arbeitet, geleitet von Connor Leahy.
- [Existenzrisiko-Observatorium](https://existentialriskobservatory.org/). Niederländische Organisation, die die Öffentlichkeit über x-Risiken informiert und Kommunikationsstrategien untersucht.
- [Zentrum für AI-Sicherheit](https://www.safe.ai/) (CAIS) ist ein Forschungszentrum an der Tschechischen Technischen Universität in Prag, geleitet von
- [Zentrum für menschenkompatible künstliche Intelligenz](https://humancompatible.ai/about/) (CHAI), geleitet von Stuart Russell.
- [Machine Intelligence Research Institute](https://intelligence.org/) (MIRI), das mathematische Forschung auf AI-Sicherheit durchführt, geleitet von Eliezer Yudkowsky.
- [Zentrum für die Regulierung von AI](https://www.governance.ai/)
- [Institut für AI-Politik und -Strategie](https://www.iaps.ai/) (IAPS)
- [The AI Policy Institute](https://theaipi.org/)
- [AI-Sicherheitskommunikationszentrum](https://aiscc.org/2023/11/01/yougov-poll-83-of-brits-demand-companies-prove-ai-systems-are-safe-before-release/)
- [Das Midas-Projekt](https://www.themidasproject.com/) Unternehmensdruckkampagnen für AI-Sicherheit.
- [Das Human Survival Project](https://thehumansurvivalproject.org/)
- [AI-Sicherheitswelt](https://aisafety.world/) Hier finden Sie einen Überblick über die AI-Sicherheitslandschaft.

## Wenn Sie überzeugt sind und handeln möchten {#if-you-are-convinced-and-want-to-take-action}

Es gibt viele [Dinge, die Sie tun können](/action).
Ein Brief schreiben, an einer Demonstration teilnehmen, Geld spenden oder einer Gemeinschaft beitreten ist nicht so schwer!
Und diese Aktionen haben einen echten Einfluss.
Selbst wenn wir uns am Ende der Welt befinden, kann es noch Hoffnung und sehr lohnende Arbeit geben.

## Oder wenn Sie sich immer noch nicht ganz sicher sind {#or-if-you-still-dont-feel-quite-sure-of-it}

Das Lernen über die [Psychologie von x-Risiken](/psychology-of-x-risk) könnte Ihnen helfen.