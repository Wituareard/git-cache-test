

---
title: Warum AI-Sicherheit wichtig ist
description: Bildungsressourcen (Videos, Artikel, Bücher) über AI-Risiken und AI-Alignment
---

Eines der wichtigsten Dinge, die Sie tun können, um bei der AI-Alignment und dem existenziellen Risiko (x-Risiko) zu helfen, das Superintelligenz darstellt, ist, sich darüber zu informieren.
Hier sind einige Ressourcen, um loszulegen.

## Websites {#websites}

- [AISafety.com](https://www.aisafety.com) & [AISafety.info](https://aisafety.info). Die Landingpages für AI-Sicherheit. Erfahren Sie mehr über die Risiken, Gemeinschaften, Veranstaltungen, Jobs, Kurse, Ideen, um die Risiken zu mindern, und vieles mehr!
- [AISafety.dance](https://aisafety.dance). Eine unterhaltsame, freundliche und interaktive Einführung in die katastrophalen Risiken von AI!
- [AISafety.world](https://aisafety.world/tiles/). Die gesamte AI-Sicherheitslandschaft mit allen Organisationen, Medien, Foren, Blogs und anderen Akteuren und Ressourcen.
- [IncidentDatabase.ai](https://incidentdatabase.ai/). Datenbank von Vorfällen, bei denen AI-Systeme Schaden verursacht haben.
<!-- [NavigatingAIRisks.ai](https://www.navigatingrisks.ai/). Ein Blog mit verschiedenen interessanten Artikeln. - [PauseAI.info](https://pauseai.info). Besuchen Sie den Rest der PauseAI-Seite für viele weitere Informationen und [Ressourcen](/learn), nützliche [Aktionen](/action), Experten[ Zitate](/quotes), kurze einseitige [Flyer](PauseAI_flyer.pdf), verwandte [FAQs](/faq) usw.

## Newsletter {#newsletters}

- [PauseAI Substack](https://pauseai.substack.com/): Unser Newsletter.
- [TransformerNews](https://www.transformernews.ai/) Umfassender wöchentlicher Newsletter über AI-Sicherheit und -Governance.
- [Don't Worry About The Vase](https://thezvi.substack.com/): Ein Newsletter über AI-Sicherheit, Rationalität und andere Themen.

## Videos {#videos}

- [Kurzgesagt - A.I. ‐ Die letzte Erfindung der Menschheit?](https://www.youtube.com/watch?v=fa8k8IQ1_X0) (20 Minuten). Die Geschichte von AI und eine Einführung in das Konzept der Superintelligenz.
- [80k hours - Könnte AI die Menschheit auslöschen?](https://youtu.be/qzyEgZwfkKY?si=ief1l2PpkZ7_s6sq) (10 Minuten). Eine großartige Einführung in das Problem aus einer bodenständigen Perspektive.
- [Superintelligente AI sollte Sie beunruhigen...](https://www.youtube.com/watch?v=xBqU1QxCao8) (1 Minute). Die beste superkurze Einführung.
- [Don't look up - Die Dokumentation: Der Fall für AI als existenzielle Bedrohung](https://www.youtube.com/watch?v=U1eyUjVRir4) (17 Minuten). Eine leistungsstarke und schön bearbeitete Dokumentation über die Gefahren von AI, mit vielen Expertenzitaten aus Interviews.
- [Länder entwickeln AI aus Gründen](https://youtu.be/-9V9cIixPbM?si=L9q6PF2D6h_EBEwF) (10 Minuten). Karikatur des Rennens zu einer Superintelligenz und ihrer Gefahren.
- [Max Tegmark | Ted Talk (2023)](https://www.youtube.com/watch?v=xUNx_PxNHrY) (15 Minuten). AI-Fähigkeiten verbessern sich schneller als erwartet.
- [Tristan Harris | Nobelpreis-Gipfel 2023](https://www.youtube.com/watch?v=6lVBp2XjWsg) (15 Minuten). Vortrag darüber, warum wir unsere "paleolithischen Gehirne akzeptieren, unsere mittelalterlichen Institutionen aufwerten und die göttliche Technologie binden" müssen.
- [Sam Harris | Können wir AI entwickeln, ohne die Kontrolle darüber zu verlieren?](https://www.youtube.com/watch?v=8nt3edWLgIg) (15 Minuten). Ted-Talk über die verrückte Situation, in der wir uns befinden.
- [Ilya: Der AI-Wissenschaftler, der die Welt gestaltet](https://youtu.be/9iqn1HhFJ6c?si=WnzvpdsPtgCvqAZg) (12 Minuten). Mitbegründer und ehemaliger Chef-Wissenschaftler bei OpenAI erklärt, wie AGI die Kontrolle über alles übernehmen wird und warum wir sie lehren müssen, sich um Menschen zu kümmern.
- [Die Gefahren von künstlicher Intelligenz erkunden](https://www.youtube.com/watch?v=sPyu_dTSma0&t=1328s) (25 Minuten). Zusammenfassung von Cybersicherheits-, Biohazard- und Macht-suchenden AI-Risiken.
- [Warum dieser Top-AI-Guru denkt, dass wir möglicherweise in Schwierigkeiten auf Artniveau stecken | The InnerView](https://youtu.be/YZjmZFDx-pA?si=Y7QUxTaJcuC6LVji) (26 Minuten). Interview mit Connor Leahy über AI-X-Risiken im Fernsehen.
- [Das AI-Dilemma](https://www.youtube.com/watch?v=xoVJKj8lcNQ&t=1903s) (1 Stunde). Präsentation über die Gefahren von AI und das Rennen, in dem AI-Unternehmen stecken.
- [Robert Miles' YouTube-Videos](https://www.youtube.com/watch?v=tlS5Y2vm02c&list=PLfHsskCxi_g-c62a_dmsNuHynaXsRQm40) sind ein großartiger Ort, um die meisten Grundlagen von AI-Alignment zu verstehen.

## Podcasts {#podcasts}

- [Future of Life Institute | Connor Leahy über AI-Sicherheit und warum die Welt fragil ist](https://youtu.be/cSL3Zau1X8g?si=0X3EKoxZ80_HN9Rl&t=1803). Interview mit Connor über die AI-Sicherheitsstrategien.
- [Lex Fridman | Max Tegmark: Der Fall für die Einstellung der AI-Entwicklung](https://youtu.be/VcVfceTsD0A?t=1547). Interview, das in die Details unserer aktuellen gefährlichen Situation eintaucht.
- [Sam Harris | Eliezer Yudkowsky: AI, Racing Toward the Brink](https://samharris.org/episode/SE60B0CF4B8). Gespräch über die Natur der Intelligenz, verschiedene Arten von AI, das Alignment-Problem, Ist vs. Soll und vieles mehr. Eine von vielen Episoden, die Making Sense über AI-Sicherheit hat.
- [Connor Leahy, AI-Feueralarm](https://youtu.be/pGjyiqJZPJo?t=2510). Vortrag über die Intelligenzexplosion und warum sie das wichtigste Ereignis wäre, das jemals passieren könnte.
- [Die empfohlenen Episoden des 80.000-Stunden-Podcasts über AI](https://80000hours.org/podcast/on-artificial-intelligence/). Nicht 80.000 Stunden lang, sondern eine Zusammenstellung von Episoden des 80.000-Stunden-Podcasts über AI-Sicherheit.
- [Episoden des Future of Life Institute-Podcasts über AI](https://futureoflife.org/podcast/?_category_browser=ai). Alle Episoden des FLI-Podcasts über die Zukunft der künstlichen Intelligenz.

Podcasts mit PauseAI-Mitgliedern finden Sie in der [Medienberichterstattung](/press).

## Artikel {#articles}

- [Das 'Don't Look Up'-Denken, das uns mit AI zum Untergang bringen könnte](https://time.com/6273743/thinking-that-could-doom-us-with-ai/) (von Max Tegmark)
- [Das Anhalten der AI-Entwicklung reicht nicht aus. Wir müssen sie ganz abschalten](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/) (von Eliezer Yudkowsky)
- [Der Fall für die Verlangsamung von AI](https://www.vox.com/the-highlight/23621198/artificial-intelligence-chatgpt-openai-existential-risk-china-ai-safety-technology) (von Sigal Samuel)
- [Die AI-Revolution: Der Weg zur Superintelligenz](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) (von WaitButWhy)
- [Wie sich Rogue-AIs entwickeln können](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) (von Yoshua Bengio)
<!-- - [Eine lockere Einführung in AI-Doom und -Alignment](https://carado.moe/ai-doom.html)
Ich mag es und die Tatsache, dass es eine leichtere Lektüre ist, aber ich bin nicht sicher, ob ich es einfügen möchte, weil es Alignment als nur die technischen Aspekte definiert und die Leute auffordert, nur technische Arbeit zu leisten -->
- [Argumente gegen die Ernstnahme von AI-Sicherheit durchdenken](https://yoshuabengio.org/2024/07/09/reasoning-through-arguments-against-taking-ai-safety-seriously/) (von Yoshua Bengio)

Wenn Sie lesen möchten, was Journalisten über PauseAI geschrieben haben, besuchen Sie die Liste der [Medienberichterstattung](/press).

## Bücher {#books}

<!-- - [AI: Unexplainable, Unpredictable, Uncontrollable](https://www.goodreads.com/book/show/197554072-ai) (Roman Yampolskiy, 2024)
Ich habe noch nichts Gutes darüber gehört -->
- [Unkontrollierbar: Die Bedrohung durch künstliche Superintelligenz und das Rennen, um die Welt zu retten](https://www.goodreads.com/book/show/202416160-uncontrollable) (Darren McKee, 2023). Holen Sie es sich kostenlos!
- [Der Abgrund: Existenzrisiko und die Zukunft der Menschheit](https://www.goodreads.com/en/book/show/50963653) (Toby Ord, 2020)
- [Das Alignment-Problem](https://www.goodreads.com/book/show/50489349-the-alignment-problem) (Brian Christian, 2020)
- [Menschlich kompatibel: Künstliche Intelligenz und das Problem der Kontrolle](https://www.goodreads.com/en/book/show/44767248) (Stuart Russell, 2019)
- [Leben 3.0: Menschsein im Zeitalter der künstlichen Intelligenz](https://www.goodreads.com/en/book/show/34272565) (Max Tegmark, 2017)
- [Superintelligenz: Wege, Gefahren, Strategien](https://www.goodreads.com/en/book/show/20527133) (Nick Bostrom, 2014)
- [Unsere letzte Erfindung: Künstliche Intelligenz und das Ende der menschlichen Ära](https://www.goodreads.com/en/book/show/17286699) (James Barrat, 2013)

## Kurse {#courses}

- [AGI-Sicherheitsgrundlagen](https://www.agisafetyfundamentals.com/) (30 Stunden)
- [CHAI-Bibliographie empfohlener Materialien](https://humancompatible.ai/bibliography) (50 Stunden+)
- [AISafety.training](https://aisafety.training/): Überblick über Trainingsprogramme, Konferenzen und andere Veranstaltungen

## Organisationen {#organizations}

- [Future of Life Institute](https://futureoflife.org/cause-area/artificial-intelligence/) startete den [offenen Brief](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), geleitet von Max Tegmark.
- [FutureSociety](https://thefuturesociety.org/about-us/)
- [Conjecture](https://www.conjecture.dev/). Start-up, das an AI-Alignment und -Politik arbeitet, geleitet von Connor Leahy.
- [Existenzrisiko-Observatorium](https://existentialriskobservatory.org/). Niederländische Organisation, die die Öffentlichkeit über X-Risiken informiert und Kommunikationsstrategien untersucht.
- [Zentrum für AI-Sicherheit](https://www.safe.ai/) (CAIS) ist ein Forschungszentrum an der Tschechischen Technischen Universität in Prag, geleitet von
- [Zentrum für menschlich kompatible künstliche Intelligenz](https://humancompatible.ai/about/) (CHAI), geleitet von Stuart Russell.
- [Machine Intelligence Research Institute](https://intelligence.org/) (MIRI), das mathematische Forschung auf dem Gebiet der AI-Sicherheit betreibt, geleitet von Eliezer Yudkowsky.
- [Zentrum für die Regulierung von AI](https://www.governance.ai/)
- [Institut für AI-Politik und -Strategie](https://www.iaps.ai/) (IAPS)
- [Das AI-Politik-Institut](https://theaipi.org/)
- [AI-Sicherheitskommunikationszentrum](https://aiscc.org/2023/11/01/yougov-poll-83-of-brits-demand-companies-prove-ai-systems-are-safe-before-release/)
- [Das Midas-Projekt](https://www.themidasproject.com/) Unternehmensdruckkampagnen für AI-Sicherheit.
- [Das Human Survival Project](https://thehumansurvivalproject.org/)
- [AI-Sicherheitswelt](https://aisafety.world/) Hier finden Sie einen Überblick über die AI-Sicherheitslandschaft.

## Wenn Sie überzeugt sind und handeln möchten {#if-you-are-convinced-and-want-to-take-action}

Es gibt viele [Dinge, die Sie tun können](/action).
Ein Brief schreiben, an einer Demonstration teilnehmen, Geld spenden oder einer Gemeinschaft beitreten ist nicht so schwer!
Und diese Aktionen haben eine reale Auswirkung.
Selbst wenn wir dem Ende der Welt gegenüberstehen, kann es noch Hoffnung und sehr lohnende Arbeit geben.

## Oder wenn Sie sich immer noch nicht ganz sicher sind {#or-if-you-still-dont-feel-quite-sure-of-it}

Das Lernen über die [Psychologie von X-Risiken](/psychology-of-x-risk) könnte Ihnen helfen.