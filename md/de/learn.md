

---
title: Warum AI-Sicherheit wichtig ist
description: Bildungsressourcen (Videos, Artikel, Bücher) über AI-Risiken und AI-Alignment
---
Eines der wichtigsten Dinge, die Sie tun können, um bei der AI-Alignment und dem existenziellen Risiko (x-Risiko) zu helfen, das Superintelligenz darstellt, ist, sich darüber zu informieren.
Hier sind einige Ressourcen, um loszulegen.

## Websites {#websites}

- [AISafety.com](https://www.aisafety.com) & [AISafety.info](https://aisafety.info). Die Landingpages für AI-Sicherheit. Erfahren Sie mehr über die Risiken, Gemeinschaften, Veranstaltungen, Jobs, Kurse, Ideen, um die Risiken zu mildern, und vieles mehr!
- [AISafety.dance](https://aisafety.dance). Eine unterhaltsame, freundliche und interaktive Einführung in die katastrophalen Risiken von AI!
- [AISafety.world](https://aisafety.world/tiles/). Die gesamte AI-Sicherheitslandschaft mit allen Organisationen, Medien, Foren, Blogs und anderen Akteuren und Ressourcen.
- [IncidentDatabase.ai](https://incidentdatabase.ai/). Datenbank von Vorfällen, bei denen AI-Systeme Schaden verursacht haben.

## Newsletter {#newsletters}

- [PauseAI Substack](https://pauseai.substack.com/): Unser Newsletter.
- [TransformerNews](https://www.transformernews.ai/) Umfassender wöchentlicher Newsletter über AI-Sicherheit und -Governance.
- [Don't Worry About The Vase](https://thezvi.substack.com/): Ein Newsletter über AI-Sicherheit, Rationalität und andere Themen.

## Videos {#videos}

- [Einführung in AI-Risiken](https://www.youtube.com/playlist?list=PLI46NoubGtIJa0JVCBR-9CayxCOmU0EJt) ist eine von uns zusammengestellte YouTube-Playlist, die Videos von 1 Minute bis 1 Stunde in verschiedenen Formaten und von diversen Quellen enthält und keine Vorkenntnisse erfordert.
- [Robert Miles' YouTube-Videos](https://www.youtube.com/watch?v=tlS5Y2vm02c&list=PLfHsskCxi_g-c62a_dmsNuHynaXsRQm40) sind ein großartiger Ausgangspunkt, um die Grundlagen von AI-Alignment zu verstehen.

## Podcasts {#podcasts}

- [Future of Life Institute | Connor Leahy über AI-Sicherheit und warum die Welt fragil ist](https://youtu.be/cSL3Zau1X8g?si=0X3EKoxZ80_HN9Rl&t=1803). Interview mit Connor über AI-Sicherheitsstrategien.
- [Lex Fridman | Max Tegmark: Der Fall für die Einstellung der AI-Entwicklung](https://youtu.be/VcVfceTsD0A?t=1547). Interview, das in die Details unserer aktuellen gefährlichen Situation eintaucht.
- [Sam Harris | Eliezer Yudkowsky: AI, auf dem Weg zum Abgrund](https://samharris.org/episode/SE60B0CF4B8). Gespräch über die Natur der Intelligenz, verschiedene Arten von AI, das Alignment-Problem, Ist vs. Soll und vieles mehr. Eine von vielen Episoden, die Making Sense über AI-Sicherheit hat.
- [Connor Leahy, AI-Feueralarm](https://youtu.be/pGjyiqJZPJo?t=2510). Vortrag über die Intelligenzexplosion und warum sie das wichtigste Ereignis wäre, das jemals passieren könnte.
- [Die empfohlenen Episoden des 80,000 Hours Podcast über AI](https://80000hours.org/podcast/on-artificial-intelligence/). Nicht 80.000 Stunden lang, sondern eine Zusammenstellung von Episoden des 80,000 Hours Podcast über AI-Sicherheit.
- [Episoden des Future of Life Institute Podcast über AI](https://futureoflife.org/podcast/?_category_browser=ai). Alle Episoden des FLI-Podcasts über die Zukunft der künstlichen Intelligenz.

Podcasts mit PauseAI-Mitgliedern finden Sie in der [Medienberichterstattung](/press)-Liste.

## Artikel {#articles}

- [Das 'Don't Look Up'-Denken, das uns mit AI zum Verhängnis werden könnte](https://time.com/6273743/thinking-that-could-doom-us-with-ai/) (von Max Tegmark)
- [Die Einstellung der AI-Entwicklung reicht nicht aus. Wir müssen alles abschalten](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/) (von Eliezer Yudkowsky)
- [Der Fall für die Verlangsamung von AI](https://www.vox.com/the-highlight/23621198/artificial-intelligence-chatgpt-openai-existential-risk-china-ai-safety-technology) (von Sigal Samuel)
- [Die AI-Revolution: Der Weg zur Superintelligenz](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) (von WaitButWhy)
- [Wie sich bösartige AIs entwickeln könnten](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) (von Yoshua Bengio)

- [Argumente gegen die Ernstnahme von AI-Sicherheit durchdenken](https://yoshuabengio.org/2024/07/09/reasoning-through-arguments-against-taking-ai-safety-seriously/) (von Yoshua Bengio)

Wenn Sie lesen möchten, was Journalisten über PauseAI geschrieben haben, sehen Sie sich die Liste der [Medienberichterstattung](/press) an.

## Bücher {#books}



- [Unkontrollierbar: Die Bedrohung durch künstliche Superintelligenz und der Kampf, die Welt zu retten](https://www.goodreads.com/book/show/202416160-uncontrollable) (Darren McKee, 2023). Holen Sie es sich [kostenlos](https://impactbooks.store/cart/47288196366640:1?discount=UNCON-P3SFRS)!
- [Der Abgrund: Existenzrisiko und die Zukunft der Menschheit](https://www.goodreads.com/en/book/show/50963653) (Toby Ord, 2020)
- [Das Alignment-Problem](https://www.goodreads.com/book/show/50489349-the-alignment-problem) (Brian Christian, 2020)
- [Menschlich kompatibel: Künstliche Intelligenz und das Problem der Kontrolle](https://www.goodreads.com/en/book/show/44767248) (Stuart Russell, 2019)
- [Leben 3.0: Mensch sein im Zeitalter der künstlichen Intelligenz](https://www.goodreads.com/en/book/show/34272565) (Max Tegmark, 2017)
- [Superintelligenz: Wege, Gefahren, Strategien](https://www.goodreads.com/en/book/show/20527133) (Nick Bostrom, 2014)
- [Unsere letzte Erfindung: Künstliche Intelligenz und das Ende der menschlichen Ära](https://www.goodreads.com/en/book/show/17286699) (James Barrat, 2013)

## Kurse {#courses}

- [AGI-Sicherheitsgrundlagen](https://www.agisafetyfundamentals.com/) (30 Std.)
- [CHAI-Bibliographie empfohlener Materialien](https://humancompatible.ai/bibliography) (50 Std.+)
- [AISafety.training](https://aisafety.training/): Überblick über Trainingsprogramme, Konferenzen und andere Veranstaltungen

## Organisationen {#organizations}

- [Future of Life Institute](https://futureoflife.org/cause-area/artificial-intelligence/) startete den [offenen Brief](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), geleitet von Max Tegmark.
- [FutureSociety](https://thefuturesociety.org/about-us/)
- [Conjecture](https://www.conjecture.dev/). Start-up, das an AI-Alignment und AI-Politik arbeitet, geleitet von Connor Leahy.
- [Existential Risk Observatory](https://existentialriskobservatory.org/). Niederländische Organisation, die die Öffentlichkeit über x-Risiken informiert und Kommunikationsstrategien untersucht.
- [Center for AI Safety](https://www.safe.ai/) (CAIS) ist ein Forschungszentrum an der Tschechischen Technischen Universität in Prag, geleitet von
- [Center for Human-Compatible Artificial Intelligence](https://humancompatible.ai/about/) (CHAI), geleitet von Stuart Russell.
- [Machine Intelligence Research Institute](https://intelligence.org/) (MIRI), das mathematische Forschung zu AI-Sicherheit betreibt, geleitet von Eliezer Yudkowsky.
- [Centre for the Governance of AI](https://www.governance.ai/)
- [Institute for AI Policy and Strategy](https://www.iaps.ai/) (IAPS)
- [The AI Policy Institute](https://theaipi.org/)
- [AI Safety Communications Centre](https://aiscc.org/2023/11/01/yougov-poll-83-of-brits-demand-companies-prove-ai-systems-are-safe-before-release/)
- [The Midas Project](https://www.themidasproject.com/) Unternehmenskampagnen für AI-Sicherheit.
- [The Human Survival Project](https://thehumansurvivalproject.org/)
- [AI Safety World](https://aisafety.world/) Hier finden Sie einen Überblick über die AI-Sicherheitslandschaft.

## Wenn Sie überzeugt sind und handeln möchten {#if-you-are-convinced-and-want-to-take-action}

Es gibt viele [Dinge, die Sie tun können](/action).
Ein Brief schreiben, an einer Demonstration teilnehmen, Geld spenden oder einer Gemeinschaft beitreten ist nicht so schwer!
Und diese Aktionen haben einen realen Einfluss.
Selbst wenn wir dem Ende der Welt gegenüberstehen, kann es noch Hoffnung und sehr lohnende Arbeit geben.

## Oder wenn Sie sich noch nicht ganz sicher sind {#or-if-you-still-dont-feel-quite-sure-of-it}

Das Lernen über die [Psychologie von x-Risiken](/psychology-of-x-risk) könnte Ihnen helfen.