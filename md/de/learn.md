

---
title: Warum AI-Sicherheit wichtig ist
description: Bildungsressourcen (Videos, Artikel, Bücher) über AI-Risiken und AI-Alignment
---

## Auf dieser Website {#on-this-website}

- [Risiken](/risks). Eine Zusammenfassung der Risiken von KI.
- [X-Risiko](/xrisk). Warum KI ein existenzielles Risiko darstellt.
- [Übernahme](/ai-takeover). Wie KI die Welt übernehmen könnte.
- [Zitate](/quotes). Zitate zu KI-Risiken und -Governance.
- [Machbarkeit einer Pause](/feasibility). Die Machbarkeit einer Pause in der KI-Entwicklung.
- [Den Pause-Knopf bauen](/building-the-pause-button). Was es braucht, um KI zu pausieren.
- [FAQ](/faq). Häufig gestellte Fragen zu KI-Sicherheit und PauseAI.
- [Aktion](/action). Was Sie tun können, um zu helfen (mit Links zu vielen aktionsbezogenen Anleitungen)

## Websites {#websites}

- [Das Kompendium](https://www.thecompendium.ai/). Ein umfassendes Wissensbündel darüber, warum das aktuelle KI-Rennen so gefährlich ist und was wir dagegen tun können.
- [Ein schmaler Pfad](https://www.narrowpath.co/). Ein detaillierter Plan über die Schritte, die wir unternehmen müssen, um unsere Chancen zu erhöhen, die nächsten Jahrzehnte zu überleben.
- [AISafety.com](https://www.aisafety.com) & [AISafety.info](https://aisafety.info). Die Landingpages für KI-Sicherheit. Erfahren Sie mehr über die Risiken, Gemeinschaften, Veranstaltungen, Jobs, Kurse, Ideen, wie man die Risiken mildern kann, und vieles mehr!
- [AISafety.dance](https://aisafety.dance). Eine unterhaltsame, freundliche und interaktive Einführung in die katastrophalen KI-Risiken!
- [AISafety.world](https://aisafety.world/tiles/). Die gesamte KI-Sicherheitslandschaft mit allen Organisationen, Medien, Foren, Blogs und anderen Akteuren und Ressourcen.
- [IncidentDatabase.ai](https://incidentdatabase.ai/). Datenbank von Vorfällen, bei denen KI-Systeme Schaden verursacht haben.

## Newsletter {#newsletters}

- [PauseAI Substack](https://pauseai.substack.com/): Unser Newsletter.
- [TransformerNews](https://www.transformernews.ai/) Umfassender wöchentlicher Newsletter zu KI-Sicherheit und -Governance.
- [Don't Worry About The Vase](https://thezvi.substack.com/): Ein Newsletter über KI-Sicherheit, Rationalität und andere Themen.

## Videos {#videos}

- [Einführung in KI-Risiken](https://www.youtube.com/playlist?list=PLI46NoubGtIJa0JVCBR-9CayxCOmU0EJt) ist eine von uns zusammengestellte YouTube-Playlist mit Videos von 1 Minute bis 1 Stunde in verschiedenen Formaten und von diversen Quellen, die keine Vorkenntnisse erfordern.
- [Robert Miles' YouTube-Videos](https://www.youtube.com/watch?v=tlS5Y2vm02c&list=PLfHsskCxi_g-c62a_dmsNuHynaXsRQm40) sind ein großartiger Ausgangspunkt, um die Grundlagen des KI-Alignments zu verstehen.

## Podcasts {#podcasts}

- [Future of Life Institute | Connor Leahy über KI-Sicherheit und warum die Welt fragil ist](https://youtu.be/cSL3Zau1X8g?si=0X3EKoxZ80_HN9Rl&t=1803). Interview mit Connor über KI-Sicherheitsstrategien.
- [Lex Fridman | Max Tegmark: Der Fall für das Anhalten der KI-Entwicklung](https://youtu.be/VcVfceTsD0A?t=1547). Interview, das in die Details unserer aktuellen gefährlichen Situation eintaucht.
- [Sam Harris | Eliezer Yudkowsky: KI, auf dem Weg zum Abgrund](https://samharris.org/episode/SE60B0CF4B8). Gespräch über die Natur der Intelligenz, verschiedene Arten von KI, das Alignment-Problem, Ist vs. Soll und vieles mehr. Eine von vielen Episoden, die Making Sense über KI-Sicherheit hat.
- [Connor Leahy, KI-Feueralarm](https://youtu.be/pGjyiqJZPJo?t=2510). Vortrag über die Intelligenzexplosion und warum sie das wichtigste Ereignis wäre, das jemals passieren könnte.
- [Die empfohlenen Episoden des 80.000-Stunden-Podcasts zu KI](https://80000hours.org/podcast/on-artificial-intelligence/). Nicht 80.000 Stunden lang, sondern eine Zusammenstellung von Episoden des 80.000-Stunden-Podcasts über KI-Sicherheit.
- [Episoden des Future of Life Institute-Podcasts zu KI](https://futureoflife.org/podcast/?_category_browser=ai). Alle Episoden des FLI-Podcasts über die Zukunft der künstlichen Intelligenz.

Podcasts mit PauseAI-Mitgliedern finden Sie in der [Medienberichterstattung](/press).

## Artikel {#articles}

- [Das 'Don't Look Up'-Denken, das uns mit KI zum Untergang bringen könnte](https://time.com/6273743/thinking-that-could-doom-us-with-ai/) (von Max Tegmark)
- [Das Anhalten der KI-Entwicklung reicht nicht aus. Wir müssen sie ganz abschalten](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/) (von Eliezer Yudkowsky)
- [Der Fall für das Verlangsamen von KI](https://www.vox.com/the-highlight/23621198/artificial-intelligence-chatgpt-openai-existential-risk-china-ai-safety-technology) (von Sigal Samuel)
- [Die KI-Revolution: Der Weg zur Superintelligenz](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) (von WaitButWhy)
- [Wie sich böswillige KIs entwickeln könnten](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) (von Yoshua Bengio)

- [Argumente gegen die Ernstnahme von KI-Sicherheit durchdenken](https://yoshuabengio.org/2024/07/09/reasoning-through-arguments-against-taking-ai-safety-seriously/) (von Yoshua Bengio)

Wenn Sie lesen möchten, was Journalisten über PauseAI geschrieben haben, sehen Sie sich die Liste der [Medienberichterstattung](/press) an.

## Bücher {#books}

- [Unkontrollierbar: Die Bedrohung durch künstliche Superintelligenz und das Rennen, um die Welt zu retten](https://www.goodreads.com/book/show/202416160-uncontrollable) (Darren McKee, 2023). Holen Sie es sich [kostenlos](https://impactbooks.store/cart/47288196366640:1?discount=UNCON-P3SFRS)!
- [Der Abgrund: Existenzrisiko und die Zukunft der Menschheit](https://www.goodreads.com/en/book/show/50963653) (Toby Ord, 2020)
- [Das Alignment-Problem](https://www.goodreads.com/book/show/50489349-the-alignment-problem) (Brian Christian, 2020)
- [Menschlich kompatibel: Künstliche Intelligenz und das Problem der Kontrolle](https://www.goodreads.com/en/book/show/44767248) (Stuart Russell, 2019)
- [Leben 3.0: Mensch sein im Zeitalter der künstlichen Intelligenz](https://www.goodreads.com/en/book/show/34272565) (Max Tegmark, 2017)
- [Superintelligenz: Wege, Gefahren, Strategien](https://www.goodreads.com/en/book/show/20527133) (Nick Bostrom, 2014)
- [Unsere letzte Erfindung: Künstliche Intelligenz und das Ende der menschlichen Ära](https://www.goodreads.com/en/book/show/17286699) (James Barrat, 2013)

## Kurse {#courses}

- [AGI-Sicherheitsgrundlagen](https://www.agisafetyfundamentals.com/) (30 Stunden)
- [CHAI-Bibliographie empfohlener Materialien](https://humancompatible.ai/bibliography) (50 Stunden+)
- [AISafety.training](https://aisafety.training/): Überblick über Trainingsprogramme, Konferenzen und andere Veranstaltungen

## Organisationen {#organizations}

- [Future of Life Institute](https://futureoflife.org/cause-area/artificial-intelligence/) startete den [offenen Brief](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), geleitet von Max Tegmark.
- [FutureSociety](https://thefuturesociety.org/about-us/)
- [Conjecture](https://www.conjecture.dev/). Start-up, das an KI-Alignment und KI-Politik arbeitet, geleitet von Connor Leahy.
- [Existenzrisiko-Observatorium](https://existentialriskobservatory.org/). Niederländische Organisation, die die Öffentlichkeit über existenzielle Risiken informiert und Kommunikationsstrategien erforscht.
- [Zentrum für KI-Sicherheit](https://www.safe.ai/) (CAIS) ist ein Forschungszentrum an der Tschechischen Technischen Universität in Prag, geleitet von
- [Zentrum für menschlich kompatible künstliche Intelligenz](https://humancompatible.ai/about/) (CHAI), geleitet von Stuart Russell.
- [Machine Intelligence Research Institute](https://intelligence.org/) (MIRI), das mathematische Forschung zu KI-Sicherheit betreibt, geleitet von Eliezer Yudkowsky.
- [Zentrum für die Regulierung von KI](https://www.governance.ai/)
- [Institut für KI-Politik und -Strategie](https://www.iaps.ai/) (IAPS)
- [Das KI-Politik-Institut](https://theaipi.org/)
- [KI-Sicherheitskommunikationszentrum](https://aiscc.org/2023/11/01/yougov-poll-83-of-brits-demand-companies-prove-ai-systems-are-safe-before-release/)
- [Das Midas-Projekt](https://www.themidasproject.com/) Unternehmensdruckkampagnen für KI-Sicherheit.
- [Das Projekt für menschliches Überleben](https://thehumansurvivalproject.org/)
- [KI-Sicherheitswelt](https://aisafety.world/) Hier finden Sie einen Überblick über die KI-Sicherheitslandschaft.

## Wenn Sie überzeugt sind und handeln möchten {#if-you-are-convinced-and-want-to-take-action}

Es gibt viele [Dinge, die Sie tun können](/action).
Ein Brief schreiben, an einer Demonstration teilnehmen, Geld spenden oder einer Gemeinschaft beitreten ist nicht so schwer!
Und diese Aktionen haben einen realen Einfluss.
Selbst wenn wir dem Ende der Welt gegenüberstehen, kann es noch Hoffnung und sehr lohnende Arbeit geben.

## Oder wenn Sie sich noch nicht ganz sicher sind {#or-if-you-still-dont-feel-quite-sure-of-it}

Das Lernen über die [Psychologie existenzieller Risiken](/psychology-of-x-risk) könnte Ihnen helfen.